{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. Initialization\n",
    "\n",
    "> a. Read the excel/csv containing the entire query-output\n",
    "\n",
    "> b. Initialize the parameters of thresholds, directories, and fields to concat for generating individual as well as combined match-score\n",
    "\n",
    "> c. Get the unique list of countries in present dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nColumns:  ['SOURCE_IDENTIFIER' 'DATA_SOURCE_NAME' 'PROTOCOL_NUMBER' 'SITE_NUM'\n 'UNIQUE_SITE_ID' 'COUNTRY' 'SITE_NAME' 'STATE' 'CITY' 'ADDRESS_LINE_1'\n 'ADDRESS_LINE_2' 'ADDRESS_LINE_3' 'POSTAL_CODE' 'SITE_STATUS'] \n\n\nUnique Countries having counts greater than  5000 :  ['United_States']\n"
     ]
    }
   ],
   "source": [
    "import time, numpy as np, pandas as pd, re, string, subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "_STATIC_FILE_NAME=\"SM_Temp_Shortlist.xlsx\"\n",
    "_RAW_SCORES_DIRECTORY='Raw_Scores'\n",
    "_CLEANED_SCORES_DIRECTORY='Cleaned_Scores'\n",
    "_MASTER_DATA_DIRECTORY='Master_Data'\n",
    "_FIELDS_TO_CONCAT={ 'CONCAT_ADDRESS':   ['ADDRESS_LINE_1','ADDRESS_LINE_2','ADDRESS_LINE_3'] }\n",
    "\n",
    "_COLUMNS_TO_CLEAN=['ADDRESS_LINE_1','ADDRESS_LINE_2','ADDRESS_LINE_3','SITE_NAME','STATE','CITY','POSTAL_CODE']\n",
    "_BINARIES_NAME=\"levenshtein\"\n",
    "_BINARIES_EXTENSION=\".dll\"\n",
    "_MAXSIZE=5000\n",
    "\n",
    "_THRESHOLD_FOR_INDIVIDUAL=0.85\n",
    "_THRESHOLD_FOR_ADDRESS_COMBINED=0.75\n",
    "\n",
    "_THRESHOLDS_DICT={\n",
    "    'CONCAT_ADDRESS': _THRESHOLD_FOR_ADDRESS_COMBINED,\n",
    "    'SITE_NAME': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'STATE': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'CITY': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'POSTAL_CODE': _THRESHOLD_FOR_INDIVIDUAL\n",
    "    }\n",
    "_COLS_FOR_TOTAL_MATCH_CALC=[colname+'_COMPARISON_SCORE' for colname in _THRESHOLDS_DICT]\n",
    "\n",
    "_SCALING_FACTOR=3\n",
    "\n",
    "_TOTAL_MATCHES_THRESHOLD=4\n",
    "\n",
    "\n",
    "def write_df_to_csv(df, root_dir='', curr_country='', file_suffix='_temp.csv', index_flag=False):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Writes the dataframe to a csv file and throw error if it fails.\n",
    "        INPUT:      Dataframe, Target-Directory, Country-name, Suffix-of-csv-file, Index-Flag\n",
    "        OUTPUT:     Dataframe csv at target-directory, or error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        abs_path=os.path.join(root_dir, curr_country+file_suffix)\n",
    "        df.to_csv(abs_path, index=index_flag)\n",
    "        print(f'\\nSuccessfully created \\{abs_path}!')\n",
    "    except:\n",
    "        print(f'\\nSomething went wrong while writing the file. Please check if it is currently in use.')\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Imputes blank cells with '', replaces whitespace with underscore in country-name, and strips whitespace in cells.\n",
    "        INPUT:      Dataframe\n",
    "        OUTPUT:     Imputed and cleaned dataframe.\n",
    "    \"\"\"\n",
    "    df.replace(np.nan, '', inplace=True)\n",
    "    for colname in df.columns.values:\n",
    "        if colname=='COUNTRY':\n",
    "            df[colname]=df[colname].apply(lambda x: x.replace(' ','_'))\n",
    "        df[colname]=df[colname].astype(str).apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "def clean_dataframe(df, columns_to_clean=_COLUMNS_TO_CLEAN, fields_to_concat=_FIELDS_TO_CONCAT, replace_punctuations=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Replaces special-chars in lowercase-converted cells if replace_punctuation==True, for the columns relevant to computing match-scores.\n",
    "                    Generates the concatenated address fields, and drops the individual ones.\n",
    "                    Overall will be left with alphanumeric chars in UTF-8 encoding.\n",
    "        INPUT:      Dataframe, columns-to-clean, address-fields-to-concat, flag-to-replace-punctuations\n",
    "        OUTPUT:     Imputed and cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # todo: Replace another special character which was causing Italy CSV file read to fail in R\n",
    "    if replace_punctuations:\n",
    "        special_chars=re.escape(string.punctuation)+'\u001a'\n",
    "        print('\\nSpecial Character that will be replaced are:  ', special_chars)\n",
    "    for colname in df.columns.values:\n",
    "        if colname in columns_to_clean and replace_punctuations:\n",
    "            df[colname]=df[colname].replace(r'['+special_chars+']', '', regex=True).str.lower()\n",
    "    for colname, cols_to_concat in fields_to_concat.items():\n",
    "        df[colname]=df[cols_to_concat].apply(lambda single_row: ''.join(single_row.values), axis=1)\n",
    "    df.drop(labels=fields_to_concat['CONCAT_ADDRESS'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def deduplicate_dataset_R(rscript_command, script_name, args, current_directory):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Invokes the R-code from Python using 32-bit Rscript 3.4.4 command.\n",
    "                    Uses the Python subprocess module to create a new Pipe.\n",
    "        INPUT:      Abs-path-of-32bit-Rscript-command, Script-to-invoke, Args-for-script, Current-directory\n",
    "        OUTPUT:     Prints R-console output based on return-code. Rscript command generates a csv of the score_features, or errors out.\n",
    "    \"\"\"\n",
    "    cmd = [rscript_command, script_name, args]\n",
    "    pipe = Popen( cmd, cwd=current_directory, stdin=PIPE, stdout=PIPE, stderr=PIPE )\n",
    "    output, error = pipe.communicate()\n",
    "\n",
    "    if pipe.returncode==0:\n",
    "        print('R OUTPUT:\\n',output.decode())\n",
    "    else:\n",
    "        print('R OUTPUT:\\n',output.decode())\n",
    "        print('R ERROR:\\n',error.decode())\n",
    "\n",
    "\n",
    "\n",
    "def scale_up_comparison_score(df, colname='SITE_NAME_COMPARISON_SCORE', scaling_factor=_SCALING_FACTOR):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Scale-up a column's binary-valued score by a factor\n",
    "        INPUT:      Dataframe, score-colname, scaling-factor\n",
    "        OUTPUT:     Scaled up dataframe.\n",
    "    \"\"\"\n",
    "    print(f'\\nScaling up {colname} by {scaling_factor}')\n",
    "    df[colname]=df[colname].apply(lambda x: x*scaling_factor)\n",
    "\n",
    "\n",
    "\n",
    "def return_top_match(df, child_column, score_key_column):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Input Dataframe has SR_NUM_1 (child-col) matching against multiple SR_NUM_2.\n",
    "                    Orders by child-col asc, score-col desc, and chooses the first possible entry of child-col.\n",
    "        INPUT:      Dataframe-of-score-features-above-a-total-threshold, index-column (SR_NUM_1), total-score-column (NUM_OF_MATCHES_FOUND)\n",
    "        OUTPUT:     Dataframe of normalized-score-features.\n",
    "    \"\"\"\n",
    "    normalized_duplicates=df.sort_values(by=[child_column]).sort_values(by=[score_key_column],ascending=False)\n",
    "    normalized_duplicates=normalized_duplicates.groupby(child_column).head(1).sort_values(by=[child_column])\n",
    "    return normalized_duplicates\n",
    "\n",
    "\n",
    "\n",
    "def replace_cyclic_dependencies(df, child_indicator, master_indicator, verbose=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Input Dataframe has cases like-     Record45 matches with Record44, and Record67 matches with Record45.\n",
    "                    In this case we should maintain-    Record67 matches with Record44.\n",
    "                    Applies a for-loop and replaces values in master-column whenever such a cyclic-occurence observed.\n",
    "        INPUT:      Dataframe-of-score-features-with-cyclic-indexes, child-column, master-column\n",
    "        OUTPUT:     Dataframe of normalized-score-features.\n",
    "    \"\"\"\n",
    "    arr=set(df[child_indicator].array)\n",
    "    for val in df[master_indicator]:\n",
    "        if val in arr:\n",
    "            replace_val=df[df[child_indicator]==val][master_indicator].values[0]\n",
    "            if verbose:\n",
    "                print(val,' found in normalized_duplicates[',child_indicator,']. Replacement: ', replace_val)\n",
    "            df[master_indicator].replace(val, replace_val, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_score_features(curr_country, source_dir=_RAW_SCORES_DIRECTORY, target_dir=_CLEANED_SCORES_DIRECTORY, verbose=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Reads the output of the Rscript command that is a csv of score_features having total-score greater than a total-threshold.\n",
    "                    Invokes the top-match function, and the replace-cyclic-occurences function to get a set of clean-score-features.\n",
    "                    Writes the dataframe in the Cleaned-Scores directory.\n",
    "        INPUT:      country-name\n",
    "        OUTPUT:     Dataframe of cleaned-normalized-score-features.\n",
    "    \"\"\"\n",
    "    duplicates=pd.read_csv(os.path.join(source_dir, curr_country+'_Score_Features.csv'))\n",
    "    duplicates['COUNTRY']=curr_country\n",
    "    duplicates=return_top_match(df=duplicates, child_column='SR_NUM_1', score_key_column='NUM_OF_MATCHES_FOUND')\n",
    "    duplicates=replace_cyclic_dependencies(df=duplicates, child_indicator='SR_NUM_1', master_indicator='SR_NUM_2', verbose=verbose)\n",
    "    write_df_to_csv(df=duplicates, root_dir=target_dir, curr_country=curr_country, file_suffix='_Cleaned_Feature_Scores.csv')\n",
    "    print('\\n\"SR_NUM_2\" will be the master record')\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "def get_deduplicated_master_records(normalized_duplicates, country_df):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  From the list of cleaned-normalized-score-features, use set-theory to find the unique list of masters.\n",
    "                        a.  Think of 'SR_NUM_1' as the list of incoming Primary-keys, and 'SR_NUM_2' as the value to which it should be mapped based on match-score.\n",
    "                        b.  Hence, union of 'SR_NUM_1' & 'SR_NUM_2' will be entire set of duplicates.\n",
    "                        c.  Stand-alone records in the current country_batch_dataframe will not fall in this entire set of duplicates.\n",
    "                        d.  Master-records set wil be the sets of 'SR_NUM_2' & #c above.\n",
    "                        >   Universe                            = {SR_NUM}\n",
    "                        >   a1                                  = {SR_NUM_1}\n",
    "                        >   a2                                  = {SR_NUM_2}\n",
    "                        >   Falls into any duplication-scenario = anymatch  = {a1 U a2}\n",
    "                        >   Falls into no duplication-scenario  = nomatch   = {Universe - anymatch}\n",
    "                        >   Total masters                       = {nomatch U a2}\n",
    "        INPUT:      Dataframe-of-cleaned-normalized-score_features\n",
    "        OUTPUT:     Unique set of master-record-ids (SR_NUM)\n",
    "    \"\"\"\n",
    "    a1=set(normalized_duplicates['SR_NUM_1'].values.tolist())\n",
    "    a2=set(normalized_duplicates['SR_NUM_2'].values.tolist())\n",
    "    country_set=set(country_df.index.values.tolist())\n",
    "    entire_duplicates_set=a1.union(a2)\n",
    "    no_match_set=country_set.difference(entire_duplicates_set)\n",
    "    master_record_ids=no_match_set.union(a2)\n",
    "    return master_record_ids\n",
    "\n",
    "\n",
    "def generate_deduplicated_master(curr_country, site_master_df, master_record_ids, target_dir=_MASTER_DATA_DIRECTORY):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Use the original df to extract columns-info and generate the country-specific Master file.\n",
    "        INPUT:      Country-name, Original-Dataframe, Unique set of master-record-ids (SR_NUM)\n",
    "        OUTPUT:     Dataframe-for-country-with-original-info, Master-Dataframe\n",
    "    \"\"\"\n",
    "    country_df_copy=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "    preprocess_dataframe(df=country_df_copy)\n",
    "    clean_dataframe(df=country_df_copy, replace_punctuations=False)\n",
    "    country_master_df=country_df_copy.loc[master_record_ids]\n",
    "    write_df_to_csv(df=country_master_df, root_dir=target_dir, curr_country=curr_country, index_flag=True, file_suffix='_Master.csv')\n",
    "    print(f'{country_df_copy.shape[0]} records get merged into {len(master_record_ids)}')\n",
    "    return country_df_copy, country_master_df\n",
    "\n",
    "\n",
    "\n",
    "def generate_dummy_cross_refs_for_masters(master_record_ids):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Create a dummy cross-reference dataframe for master-records; Record45 matches with Record45 having a total match-score of maximum.\n",
    "        INPUT:      Unique set of master-record-ids (SR_NUM)\n",
    "        OUTPUT:     Dataframe-of-dummy-entries-for-master-cross-references.\n",
    "    \"\"\"\n",
    "    master_record_score_array=[1.0]*len(master_record_ids)\n",
    "    master_record_df_dict={\n",
    "        'SR_NUM_1': list(master_record_ids),\n",
    "        'SR_NUM_2': list(master_record_ids),\n",
    "        'SITE_NAME_COMPARISON_SCORE': master_record_score_array,\n",
    "        'STATE_COMPARISON_SCORE': master_record_score_array,\n",
    "        'CITY_COMPARISON_SCORE': master_record_score_array,\n",
    "        'CONCAT_ADDRESS_COMPARISON_SCORE': master_record_score_array,\n",
    "        'POSTAL_CODE_COMPARISON_SCORE': master_record_score_array }\n",
    "\n",
    "    cross_ref_df=pd.DataFrame(master_record_df_dict)\n",
    "    cross_ref_df['COUNTRY']=curr_country\n",
    "    scale_up_comparison_score(cross_ref_df,'CONCAT_ADDRESS_COMPARISON_SCORE',_SCALING_FACTOR)\n",
    "    cross_ref_df['NUM_OF_MATCHES_FOUND']=cross_ref_df[_COLS_FOR_TOTAL_MATCH_CALC].sum(axis=1)\n",
    "    return cross_ref_df\n",
    "\n",
    "\n",
    "def generate_final_cross_refs(cross_ref_df, normalized_duplicates, target_dir=_MASTER_DATA_DIRECTORY):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Merge the dummy cross-reference of masters, with the cleaned-normalized-feature-scores.\n",
    "        INPUT:      Dataframe-of-dummy-entries-for-master-cross-references, Dataframe-of-cleaned-normalized-score_features\n",
    "        OUTPUT:     Dataframe-of-cross-references.\n",
    "    \"\"\"\n",
    "    cross_ref_df=cross_ref_df.append(normalized_duplicates)\n",
    "    cross_ref_df.sort_values(by=['SR_NUM_1'], axis=0, inplace=True)\n",
    "    write_df_to_csv(df=cross_ref_df, root_dir=target_dir, curr_country=curr_country, file_suffix='_Raw_Cross_Ref.csv')\n",
    "    return cross_ref_df\n",
    "\n",
    "\n",
    "\n",
    "def generate_cross_ref_report(cross_ref_df, country_df, target_dir=_MASTER_DATA_DIRECTORY):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Creates cross-reference report by performing left-join of cross-reference-dataframe with the original-info in country-df.\n",
    "                        a. Merge the master_cross_reference_df with the country_batch_dataframe as a left-outer-join on Primary-key='SR_NUM_1'\n",
    "                        b. Merge this master_cross_reference_df with the country_batch_dataframe as a left-outer-join on Primary-key='SR_NUM_2'\n",
    "                    Writes the dataframe in the Master-Data directory.\n",
    "        INPUT:      Dataframe-of-cross-references, Dataframe-for-country-with-original-info\n",
    "        OUTPUT:     Dataframe-of-cross-references-with-original-info.\n",
    "    \"\"\"\n",
    "    country_df.reset_index(inplace=True)\n",
    "    country_df_colnames=country_df.columns.values\n",
    "\n",
    "    country_df.columns=[colname+'_1' for colname in country_df_colnames]\n",
    "    cross_ref_df=cross_ref_df.merge(country_df, how='left', on='SR_NUM_1')\n",
    "\n",
    "    country_df.columns=[colname+'_2' for colname in country_df_colnames]\n",
    "    cross_ref_df=cross_ref_df.merge(country_df, how='left', on='SR_NUM_2')\n",
    "\n",
    "    columns_in_report_format=['SR_NUM_1', 'SR_NUM_2', 'SITE_NAME_1','SITE_NAME_2','SITE_NAME_COMPARISON_SCORE','STATE_1','STATE_2','STATE_COMPARISON_SCORE', 'CITY_1', 'CITY_2','CITY_COMPARISON_SCORE','CONCAT_ADDRESS_1','CONCAT_ADDRESS_2','CONCAT_ADDRESS_COMPARISON_SCORE', 'POSTAL_CODE_1','POSTAL_CODE_2',   'POSTAL_CODE_COMPARISON_SCORE','NUM_OF_MATCHES_FOUND']\n",
    "    cross_ref_df=cross_ref_df[columns_in_report_format]\n",
    "    write_df_to_csv(df=cross_ref_df, root_dir=target_dir, curr_country=curr_country, file_suffix='_Cross_Ref_Full_Report.csv')\n",
    "\n",
    "\n",
    "\n",
    "site_master_df=pd.read_excel(_STATIC_FILE_NAME, index_col=0)\n",
    "preprocess_dataframe(site_master_df)\n",
    "print('\\nColumns: ', site_master_df.columns.values,'\\n')\n",
    "countries=list(site_master_df['COUNTRY'].unique())\n",
    "small_countries=list(site_master_df['COUNTRY'].value_counts()[site_master_df['COUNTRY'].value_counts() > _MAXSIZE].index)\n",
    "for c in small_countries:\n",
    "    countries.remove(c)\n",
    "print('\\nUnique Countries having counts greater than ', _MAXSIZE, ': ', countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nSpecial Character that will be replaced are:   !\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\u001a\n\nSuccessfully created \\United_States_country_df.csv!\n\nUnited_States has 1000 records\n"
     ]
    }
   ],
   "source": [
    "# todo: for loop- Research on multithreading to speed up country-wise batches. RAM might crash for incoming batch-size>4000.\n",
    "c=0\n",
    "curr_country=countries[c]\n",
    "country_df=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "clean_dataframe(country_df, columns_to_clean=_COLUMNS_TO_CLEAN, fields_to_concat=_FIELDS_TO_CONCAT, replace_punctuations=True)\n",
    "write_df_to_csv(df=country_df[_THRESHOLDS_DICT.keys()], curr_country=curr_country, file_suffix='_country_df.csv', index_flag=True)\n",
    "print(f'\\n{curr_country} has {country_df.shape[0]} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate_dataset_R(\n",
    "    rscript_command=\"C:/Program Files/R/R-3.4.4/bin/i386/Rscript\",  script_name=\"Site_Master_Record_Linkage.R\",\n",
    "    args=f\"{_BINARIES_NAME} {_BINARIES_EXTENSION} {_THRESHOLD_FOR_INDIVIDUAL} {_THRESHOLD_FOR_ADDRESS_COMBINED} {_SCALING_FACTOR} {curr_country} {_RAW_SCORES_DIRECTORY} {_TOTAL_MATCHES_THRESHOLD}\",\n",
    "    current_directory=\"C:/Users/vdeshpande/Desktop/Del_Project-Takeda/Site_Master_Repo/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and normalize the score features\n",
    "normalized_duplicates = clean_score_features(curr_country=curr_country, source_dir=_RAW_SCORES_DIRECTORY, target_dir=_CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "# Get the unique set of master-record-ids\n",
    "master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=country_df)\n",
    "# Get the country-master-df and a copy of original country-df\n",
    "country_df_copy, country_master_df = generate_deduplicated_master(curr_country, site_master_df=site_master_df, master_record_ids=master_record_ids, target_dir=_MASTER_DATA_DIRECTORY)\n",
    "# Create a dummy set of cross-refs for masters\n",
    "cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "# Create full set of cross-refs for country-df\n",
    "cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, target_dir=_MASTER_DATA_DIRECTORY)\n",
    "# Create the csv for the cross-ref report\n",
    "generate_cross_ref_report(cross_ref_df=cross_ref_df, country_df_copy=country_df_copy, target_dir=_MASTER_DATA_DIRECTORY)"
   ]
  }
 ]
}
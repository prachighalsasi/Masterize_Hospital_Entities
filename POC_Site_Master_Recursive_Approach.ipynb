{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381jvsc74a57bd0df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407",
   "display_name": "Python 3.8.1 32-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. Initialization\n",
    "\n",
    "> a. Read the excel/csv containing the entire query-output\n",
    "\n",
    "> b. Initialize the parameters of thresholds, directories, and fields to concat for generating individual as well as combined match-score\n",
    "\n",
    "> c. Get the unique list of countries in present dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read the Source-file Site_Master_Extract.xlsx\n",
      "\n",
      "Columns: ['SOURCE_IDENTIFIER' 'DATA_SOURCE_NAME' 'PROTOCOL_NUMBER' 'SITE_NUM'\n",
      " 'UNIQUE_SITE_ID' 'COUNTRY' 'SITE_NAME' 'STATE' 'CITY' 'ADDRESS_LINE_1'\n",
      " 'ADDRESS_LINE_2' 'ADDRESS_LINE_3' 'POSTAL_CODE' 'SITE_STATUS' 'rnum']\n",
      "\n",
      "\n",
      "Countries : ['Algeria', 'Argentina', 'Australia', 'Austria', 'Belarus', 'Belgium', 'Bosnia_and_Herzegovina', 'Brazil', 'Bulgaria', 'Canada', 'Chile', 'China', 'Colombia', 'Croatia', 'Czech_Republic', 'Denmark', 'Dominican_Republic', 'Egypt', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hong_Kong', 'Hungary', 'India', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kenya', 'Korea,_Republic_of', 'Latvia', 'Lithuania', 'Macedonia_Former_Yugoslav_Rep', 'Malaysia', 'Mexico', 'Moldova,_Republic_of', 'Netherlands', 'New_Zealand', 'Norway', 'Panama', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Romania', 'Russia', 'Russian_Federation', 'Serbia', 'Singapore', 'Slovakia', 'South_Africa', 'South_Korea', 'Spain', 'Sweden', 'Switzerland', 'Taiwan', 'Taiwan,_Province_of_China', 'Thailand', 'Tunisia', 'Turkey', 'Ukraine', 'United_Kingdom', 'United_States', 'Venezuela,_Bolivarian_Republic', 'Viet_Nam']\n"
     ]
    }
   ],
   "source": [
    "import time, numpy as np, pandas as pd, re, string, subprocess, os\n",
    "from subprocess import Popen, PIPE\n",
    "from config import Config\n",
    "from util_functions import *\n",
    "\n",
    "conf=Config()\n",
    "\n",
    "#site_master_df=pd.read_csv(conf._STATIC_FILE_NAME, index_col=0)\n",
    "site_master_df=pd.read_excel(conf._STATIC_FILE_NAME, index_col=0)\n",
    "print(\"Read the Source-file {}\".format(conf._STATIC_FILE_NAME))\n",
    "#site_master_df.set_index('SR_NUM', inplace=True)\n",
    "\n",
    "site_master_df=preprocess_dataframe(df=site_master_df)\n",
    "print('\\nColumns: {}\\n'.format(site_master_df.columns.values))\n",
    "countries=list(site_master_df['COUNTRY'].unique())\n",
    "print('\\nCountries : {}'.format(countries))"
   ]
  },
  {
   "source": [
    "# TESTING OUT THE RECURSIVE APPROACH WITH MINI-BATCHES OF MASTERS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Special Character that will be replaced are:  !\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n",
      "\n",
      "There will be 1 batches since incoming dataset-size=3 and minibatch-size=500\n",
      "\n",
      "\n",
      "Starting Batch[0]...\n",
      "\n",
      "Successfully created \\Belarus_country_df.csv!\n",
      "\n",
      "Belarus_0 has 3 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "R OUTPUT:\n",
      "\n",
      "b'[1] \"levenshtein .dll 0.85 0.75 3 Belarus Raw_Scores 4 Dedup NA NA\"\\r\\n[1] \"Loading levenshtein.dll !\"\\r\\n         used (Mb) gc trigger (Mb) max used (Mb)\\r\\nNcells 120133  3.3     350000  9.4   302969  8.1\\r\\nVcells 169833  1.3     786432  6.0   697517  5.4\\r\\n[1] \"NRows= 3 , Candidate-pairs= 3 , Columns are \"\\r\\n[1] \"SR_NUM\"         \"CONCAT_ADDRESS\" \"SITE_NAME\"      \"STATE\"         \\r\\n[5] \"CITY\"           \"POSTAL_CODE\"   \\r\\n[1] \"N_combinations= 3 , Columns are \"\\r\\n[1] \"id1\"            \"id2\"            \"CONCAT_ADDRESS\" \"SITE_NAME\"     \\r\\n[5] \"STATE\"          \"CITY\"           \"POSTAL_CODE\"    \"is_match\"      \\r\\n[1] \"Scaling up column scores if threshold crossed\"\\r\\n[1] \"SITE_NAME  :  0.85\"\\r\\n[1] \"STATE  :  0.85\"\\r\\n[1] \"CITY  :  0.85\"\\r\\n[1] \"POSTAL_CODE  :  0.85\"\\r\\n[1] \"CONCAT_ADDRESS  :  0.75\"\\r\\n[1] \"No potential matches found in the incoming dataset! Creating a dummy csv...\"\\r\\n[1] \"Raw_Scores//Belarus_Score_Features.csv\"\\r\\n[1] \"Successfully created //Raw_Scores//Belarus_Score_Features.csv !\"\\r\\n\\r\\nTime difference of 0.206959 secs\\r\\n'\n",
      "\n",
      "\n",
      "Get the unique set of all record-ids since there aren't any potential duplicates.\n",
      "\n",
      "3 records get merged into 3\n",
      "\n",
      "Successfully created \\Master_Data\\Recursive_Staging_Area\\Belarus_0_Master.csv!\n",
      "1 csvs generated are: ['Belarus_0_Master.csv']\n"
     ]
    }
   ],
   "source": [
    "# for c in range(len(countries)):\n",
    "c=4\n",
    "curr_country=countries[c]\n",
    "entire_country_df=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "entire_country_df=clean_dataframe(entire_country_df, columns_to_clean=conf._COLUMNS_TO_CLEAN, fields_to_concat=conf._FIELDS_TO_CONCAT, replace_punctuations=True)\n",
    "entire_country_df_copy=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "entire_country_df_copy=clean_dataframe(entire_country_df_copy, columns_to_clean=conf._COLUMNS_TO_CLEAN, fields_to_concat=conf._FIELDS_TO_CONCAT, replace_punctuations=False)\n",
    "nrows=entire_country_df.shape[0]\n",
    "m=(nrows//conf._MAXSIZE)+1\n",
    "print('\\nThere will be {} batches since incoming dataset-size={} and minibatch-size={}'.format(m, entire_country_df.shape[0], conf._MAXSIZE))\n",
    "entire_country_cross_ref_df=pd.DataFrame()\n",
    "queue_of_csvs=list()\n",
    "\n",
    "for i in range(m):\n",
    "    print(\"\\n\\nStarting Batch[{}]...\".format(i))\n",
    "    country_df=entire_country_df.iloc[i*conf._MAXSIZE : (i+1)*conf._MAXSIZE]\n",
    "    country_df_copy=entire_country_df_copy.iloc[i*conf._MAXSIZE : (i+1)*conf._MAXSIZE]\n",
    "    write_df_to_csv(df=country_df[conf._THRESHOLDS_DICT.keys()], curr_country=curr_country, file_suffix='_country_df.csv', index_flag=True)\n",
    "    _CREATE_MASTER_MINIBATCHES=(country_df.shape[0]>1)\n",
    "\t\n",
    "    if _CREATE_MASTER_MINIBATCHES==False:\n",
    "\n",
    "        print(\"\\n\\nGet the unique set of all record-ids, since Layer-zero cannot create mastered mini-batches.\\n\")\n",
    "        # Get the unique set of master-record-ids\n",
    "        master_record_ids = country_df.index.values.astype(list)\n",
    "        # Create the country-master-df\n",
    "        country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "        # Create a dummy set of cross-refs for masters\n",
    "        cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "\n",
    "        new_file_name='_{}_Master.csv'.format(i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Invoke the Rscript and generate the Raw_score_features csv file for each minibatch\n",
    "        args=\"{} {} {} {} {} {} {} {} {} NA NA\".format(conf._BINARIES_NAME, conf._BINARIES_EXTENSION, conf._THRESHOLD_FOR_INDIVIDUAL, conf._THRESHOLD_FOR_ADDRESS_COMBINED, conf._SCALING_FACTOR, curr_country, conf._RAW_SCORES_DIRECTORY, conf._TOTAL_MATCHES_THRESHOLD, conf._DEDUP_METHOD)\n",
    "        print('\\n{}_{} has {} records.\\n\\nInvoking the Rscript now...'.format(curr_country, i, country_df.shape[0]))\n",
    "        deduplicate_dataset_R( rscript_command=conf._RSCRIPT_CMD,  script_name=conf._SCRIPT_NAME, args=args )\n",
    "\n",
    "        normalized_duplicates=pd.DataFrame()\n",
    "        # Clean and normalize the score features\n",
    "        normalized_duplicates = clean_score_features(curr_country=curr_country, country_df=country_df, source_dir=conf._RAW_SCORES_DIRECTORY, target_dir=conf._CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "\n",
    "        if normalized_duplicates.shape[0]!=0:\n",
    "            \n",
    "            print(\"\\n\\nFound potential duplicates. Processing their master and cross-reference...\\n\")\n",
    "            # Get the unique set of master-record-ids\n",
    "            master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=country_df)\n",
    "            # Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "            # Create full set of cross-refs for country-df\n",
    "            cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create the csv for the cross-ref report\n",
    "            generate_cross_ref_report(cross_ref_df=cross_ref_df, curr_country=curr_country, country_df=country_df_copy, target_dir=conf._STAGING_AREA_DIRECTORY)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"\\n\\nGet the unique set of all record-ids since there aren't any potential duplicates.\\n\")\n",
    "\t\t\t# Get the unique set of all-record-ids since there aren't any potential duplicates\n",
    "            master_record_ids = country_df.index.values.astype(list)\n",
    "\t\t\t# Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "\t\t\t# Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "                \n",
    "        new_file_name='_{}_Master.csv'.format(i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\n",
    "\n",
    "        del country_df, country_df_copy, master_record_ids\n",
    "        if _CREATE_MASTER_MINIBATCHES:\n",
    "            del normalized_duplicates\n",
    "\n",
    "print(\"{} csvs generated are: {}\".format(len(queue_of_csvs), queue_of_csvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nMax-depth for Belarus will be 1\n1 csvs need to be processed: ['Belarus_0_Master.csv'] , length=2\n\n\nGet the unique set of all record-ids since there isn't a second file to compare.\n\n3 records get merged into 3\n\nSuccessfully created \\Master_Data\\Recursive_Staging_Area\\Belarus_d1_0_Master.csv!\n\nSuccessfully created \\Master_Data\\Recursive_Staging_Area\\Belarus_d1_Raw_Cross_Ref.csv!\n\n\nDepth[1] processed successfully.\n\n\n\n\nProcessed all 1 levels. Generating the master and cross-reference at the final-layer...\n\nSuccessfully created \\Master_Data\\Belarus_Master.csv!\n3 records get merged into 3\n\nSuccessfully created \\Master_Data\\Belarus_Raw_Cross_Ref.csv!\n\nSuccessfully created \\Master_Data\\Belarus_Cross_Ref_Full_Report.csv!\n"
     ]
    }
   ],
   "source": [
    "# Number of levels for the recursive computations\n",
    "d=(m+1)//2\n",
    "print(\"\\nMax-depth for {} will be {}\".format(curr_country, d))\n",
    "\n",
    "for j in range(1,d+1):\n",
    "    combined_crossref_at_depth=pd.DataFrame()\n",
    "    n_csvs_to_read=len(queue_of_csvs)\n",
    "    length=n_csvs_to_read if n_csvs_to_read%2==0 else n_csvs_to_read+1\n",
    "    print(\"{} csvs need to be processed: {} , length={}\".format(n_csvs_to_read, queue_of_csvs, length))\n",
    "    for i in range(0, length, 2):\n",
    "        master_csv_1=os.path.join(conf._STAGING_AREA_DIRECTORY, queue_of_csvs[i])\n",
    "        master_df_1=pd.read_csv(master_csv_1, index_col=0)\n",
    "        \n",
    "        if i+1<n_csvs_to_read:\n",
    "            master_csv_2=os.path.join(conf._STAGING_AREA_DIRECTORY, queue_of_csvs[i+1])\n",
    "            master_df_2=pd.read_csv(master_csv_2, index_col=0)\n",
    "            \n",
    "\t\t\t# Invoke the Rscript and generate the Raw_score_features csv file\n",
    "            print('\\n{} has {} records, and {} has {} records.\\n\\nInvoking the Rscript now...\\n'.format(master_csv_1, master_df_1.shape[0],master_csv_2, master_df_2.shape[0]))\n",
    "            args=\"{} {} {} {} {} {} {} {} {} {} {}\".format(conf._BINARIES_NAME, conf._BINARIES_EXTENSION, conf._THRESHOLD_FOR_INDIVIDUAL, conf._THRESHOLD_FOR_ADDRESS_COMBINED, conf._SCALING_FACTOR, curr_country, conf._RAW_SCORES_DIRECTORY, conf._TOTAL_MATCHES_THRESHOLD, conf._LINKAGE_METHOD, master_csv_1, master_csv_2)\n",
    "            deduplicate_dataset_R( rscript_command=conf._RSCRIPT_CMD,  script_name=conf._SCRIPT_NAME, args=args )\n",
    "            \n",
    "            \n",
    "            normalized_duplicates=pd.DataFrame()\n",
    "            # Clean and normalize the score features\n",
    "            normalized_duplicates = clean_score_features(curr_country=curr_country, country_df=master_df_1.append(master_df_2), source_dir=conf._RAW_SCORES_DIRECTORY, target_dir=conf._CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "            \n",
    "            if normalized_duplicates.shape[0]!=0:\n",
    "                \n",
    "                print(\"\\n\\nFound potential duplicates. Processing their master and cross-reference...\\n\")\n",
    "                # Get the unique set of master-record-ids\n",
    "                master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=master_df_1.append(master_df_2))\n",
    "                # Get the country-master-df\n",
    "                country_master_df = generate_deduplicated_master(country_df=master_df_1.append(master_df_2), master_record_ids=list(master_record_ids), curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create a dummy set of cross-refs for masters\n",
    "                cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "                # Create full set of cross-refs for country-df\n",
    "                cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create the csv for the cross-ref report\n",
    "                generate_cross_ref_report(cross_ref_df=cross_ref_df, country_df=master_df_1.append(master_df_2), curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                print(\"\\n\\nGet the unique set of all record-ids since there aren't any potential duplicates.\\n\")\n",
    "                # Get the unique set of all-record-ids since there aren't any potential duplicates\n",
    "                master_record_ids = master_df_1.append(master_df_2).index.values.astype(list)\n",
    "                # Get the country-master-df\n",
    "                country_master_df = generate_deduplicated_master(country_df=master_df_1.append(master_df_2), master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create a dummy set of cross-refs for masters\n",
    "                cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            print(\"\\n\\nGet the unique set of all record-ids since there isn't a second file to compare.\\n\")\n",
    "            # Get the unique set of master-record-ids\n",
    "            master_record_ids = master_df_1.index.values.astype(list)\n",
    "            # Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=master_df_1, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "            \n",
    "            \n",
    "        combined_crossref_at_depth = combined_crossref_at_depth.append(cross_ref_df)\n",
    "        new_file_name=\"_d{}_{}_Master.csv\".format(j,i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        del master_record_ids, master_df_1\n",
    "        if i+1<n_csvs_to_read:\n",
    "            del normalized_duplicates, master_df_2\n",
    "    \n",
    "    write_df_to_csv(df=combined_crossref_at_depth, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=\"_d{}_Raw_Cross_Ref.csv\".format(j), index_flag=False)\n",
    "    print(\"\\n\\nDepth[{}] processed successfully.\".format(j))\n",
    "    update_entire_country_cross_ref(new_depth_cross_ref_df=combined_crossref_at_depth, entire_country_cross_ref_df=entire_country_cross_ref_df)\n",
    "    queue_of_csvs=queue_of_csvs[n_csvs_to_read:]\n",
    "\n",
    "\n",
    "\n",
    "if len(queue_of_csvs)==1:\n",
    "    print(\"\\n\\n\\n\\nProcessed all {} levels. Generating the master and cross-reference at the final-layer...\".format(d))\n",
    "    master_csv_1=os.path.join(conf._STAGING_AREA_DIRECTORY, queue_of_csvs[i])\n",
    "    master_df_1=pd.read_csv(master_csv_1, index_col=0)\n",
    "    # Get the unique set of master-record-ids\n",
    "    master_record_ids = master_df_1.index.values.astype(list)\n",
    "    # Get the country-master-df\n",
    "    country_master_df = generate_deduplicated_master(country_df=entire_country_df_copy, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._MASTER_DATA_DIRECTORY, write_csv=True)\n",
    "\t# Write the final raw-cross-ref to a csv\n",
    "    write_df_to_csv(df=entire_country_cross_ref_df, root_dir=conf._MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix=\"_Raw_Cross_Ref.csv\", index_flag=False)\n",
    "    # Create the csv for the cross-ref report\n",
    "    generate_cross_ref_report(cross_ref_df=entire_country_cross_ref_df, country_df=entire_country_df_copy, curr_country=curr_country, target_dir=conf._MASTER_DATA_DIRECTORY)"
   ]
  }
 ]
}
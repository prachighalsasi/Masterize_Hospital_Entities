{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted the Data_Files\\hospital_account_info_raw.csv file into Data_Files\\hospital_account_info.csv using PySpark successfully.\n",
      "\n",
      "Finished reading the Source-file Data_Files\\hospital_account_info.csv\n",
      "\n",
      "Columns: ['COUNTRY' 'POSTAL_CODE' 'SITE_NAME' 'STATE' 'CITY' 'ADDRESS_LINE_1'\n",
      " 'ADDRESS_LINE_2' 'ADDRESS_LINE_3' 'COUNTY_NAME' 'PHONE_NUM' 'SITE_TYPE'\n",
      " 'SITE_OWNERSHIP' 'ACCOUNT_NUM']\n",
      "\n",
      "\n",
      "Countries : ['USA']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\r\n",
    "from config import *\r\n",
    "from utils.util_functions import *\r\n",
    "\r\n",
    "#preformat_input_using_sparksql()\r\n",
    "print('\\nFormatted the {} file into {} using PySpark successfully.'.format(config._RAW_STATIC_FILE_NAME, config._STATIC_FILE_NAME))\r\n",
    "\r\n",
    "if '.csv' in config._STATIC_FILE_NAME.lower():\r\n",
    "    site_master_df = pd.read_csv(config._STATIC_FILE_NAME, index_col=0)\r\n",
    "elif '.xlsx' in config._STATIC_FILE_NAME.lower():\r\n",
    "    site_master_df = pd.read_excel(config._STATIC_FILE_NAME, index_col=0)\r\n",
    "\r\n",
    "print('\\nFinished reading the Source-file {}'.format(config._STATIC_FILE_NAME))\r\n",
    "\r\n",
    "site_master_df=preprocess_dataframe(df=site_master_df)\r\n",
    "print('\\nColumns: {}\\n'.format(site_master_df.columns.values))\r\n",
    "\r\n",
    "countries=list(site_master_df['COUNTRY'].unique())\r\n",
    "print('\\nCountries : {}'.format(countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Special Character that will be replaced are:  !\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n",
      "\n",
      "There will be 1 batches since incoming dataset-size=64 and minibatch-size=2000\n",
      "\n",
      "\n",
      "Starting Batch[0]...\n",
      "\n",
      "Successfully created \\USA_country_df.csv!\n",
      "\n",
      "USA_0 has 64 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "R OUTPUT:\n",
      "\n",
      "[1] \"levenshtein .dll 0.75 0.6 3 USA Data_Files\\\\Raw_Scores 4 Dedup NA NA\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "[1] 4000\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 286488  8.8     643737 19.7   462378 14.2\n",
      "Vcells 285633  2.2    8388608 64.0  1450623 11.1\n",
      "[1] \"NRows= 64 , Candidate-pairs= 2016 , Columns are \"\n",
      "[1] \"SR_NUM\"         \"CONCAT_ADDRESS\" \"SITE_NAME\"      \"STATE\"         \n",
      "[5] \"CITY\"           \"POSTAL_CODE\"   \n",
      "[1] \"N_combinations= 2016 , Columns are \"\n",
      "[1] \"id1\"            \"id2\"            \"CONCAT_ADDRESS\" \"SITE_NAME\"     \n",
      "[5] \"STATE\"          \"CITY\"           \"POSTAL_CODE\"    \"is_match\"      \n",
      "[1] \"Scaling up column scores if threshold crossed\"\n",
      "[1] \"SITE_NAME  :  0.75\"\n",
      "[1] \"STATE  :  0.75\"\n",
      "[1] \"CITY  :  0.75\"\n",
      "[1] \"POSTAL_CODE  :  0.75\"\n",
      "[1] \"CONCAT_ADDRESS  :  0.6\"\n",
      "[1] \"Data_Files\\\\Raw_Scores//USA_Score_Features.csv\"\n",
      "[1] \"Successfully created //Data_Files\\\\Raw_Scores//USA_Score_Features.csv !\"\n",
      "\n",
      "Time difference of 0.5388639 secs\n",
      "\n",
      "\n",
      "\n",
      "3 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.\n",
      "\n",
      "Successfully created \\Data_Files\\Cleaned_Scores\\USA_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "64 records get merged into 53\n",
      "\n",
      "Successfully created \\Data_Files\\Master_Data\\Recursive_Staging_Area\\USA_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Data_Files\\Master_Data\\Recursive_Staging_Area\\USA_0_Master.csv!\n",
      "1 csvs generated are: ['USA_0_Master.csv']\n"
     ]
    }
   ],
   "source": [
    "# for c in range(len(countries)):\r\n",
    "c=0\r\n",
    "curr_country=countries[c]\r\n",
    "entire_country_df=site_master_df[site_master_df['COUNTRY']==curr_country]\r\n",
    "entire_country_df=clean_dataframe(entire_country_df, columns_to_clean=config._COLUMNS_TO_CLEAN, fields_to_concat=config._FIELDS_TO_CONCAT, replace_punctuations=True)\r\n",
    "entire_country_df_copy=site_master_df[site_master_df['COUNTRY']==curr_country]\r\n",
    "entire_country_df_copy=clean_dataframe(entire_country_df_copy, columns_to_clean=config._COLUMNS_TO_CLEAN, fields_to_concat=config._FIELDS_TO_CONCAT, replace_punctuations=False)\r\n",
    "nrows=entire_country_df.shape[0]\r\n",
    "m=int(np.ceil(np.divide(nrows, config._MAXSIZE)))\r\n",
    "print('\\nThere will be {} batches since incoming dataset-size={} and minibatch-size={}'.format(m, entire_country_df.shape[0], config._MAXSIZE))\r\n",
    "entire_country_cross_ref_df=pd.DataFrame()\r\n",
    "queue_of_csvs=list()\r\n",
    "\r\n",
    "for i in range(m):\r\n",
    "    print('\\n\\nStarting Batch[{}]...'.format(i))\r\n",
    "    country_df=entire_country_df.iloc[i*config._MAXSIZE : (i+1)*config._MAXSIZE]\r\n",
    "    country_df_copy=entire_country_df_copy.iloc[i*config._MAXSIZE : (i+1)*config._MAXSIZE]\r\n",
    "    write_df_to_csv(df=country_df[config._THRESHOLDS_DICT.keys()], curr_country=curr_country, file_suffix='_country_df.csv', index_flag=True)\r\n",
    "    _CREATE_MASTER_MINIBATCHES = (country_df.shape[0]>1)\r\n",
    "\t\r\n",
    "    if not _CREATE_MASTER_MINIBATCHES:\r\n",
    "\r\n",
    "        print('\\n\\nGet the unique set of all record-ids, since Layer-zero cannot create mastered mini-batches.\\n')\r\n",
    "        # Get the unique set of master-record-ids\r\n",
    "        master_record_ids = country_df.index.values.astype(list)\r\n",
    "        # Create the country-master-df\r\n",
    "        country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=config._STAGING_AREA_DIRECTORY, write_csv=False)\r\n",
    "        # Create a dummy set of cross-refs for masters\r\n",
    "        cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\r\n",
    "\r\n",
    "        # Write the current master dataset to a csv, add the filename to the queue of csvs, and append currently generated cross-ref to existing cross-ref\r\n",
    "        new_file_name='_{}_Master.csv'.format(i)\r\n",
    "        write_df_to_csv(df=country_master_df, root_dir=config._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\r\n",
    "        new_file_name=curr_country+new_file_name\r\n",
    "        queue_of_csvs.append(new_file_name)\r\n",
    "        entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\r\n",
    "\r\n",
    "\r\n",
    "    elif _CREATE_MASTER_MINIBATCHES:\r\n",
    "\r\n",
    "        # Invoke the Rscript and generate the Raw_score_features csv file for each minibatch\r\n",
    "        args='{} {} {} {} {} {} {} {} {} NA NA'.format(config._BINARIES_NAME, config._BINARIES_EXTENSION, config._THRESHOLD_FOR_INDIVIDUAL, config._THRESHOLD_FOR_ADDRESS_COMBINED, config._SCALING_FACTOR, curr_country, config._RAW_SCORES_DIRECTORY, config._TOTAL_MATCHES_THRESHOLD, config._DEDUP_METHOD)\r\n",
    "        print('\\n{}_{} has {} records.\\n\\nInvoking the Rscript now...'.format(curr_country, i, country_df.shape[0]))\r\n",
    "        deduplicate_dataset_R( rscript_command=config._RSCRIPT_CMD,  script_name=config._SCRIPT_NAME, args=args )\r\n",
    "\r\n",
    "        normalized_duplicates=pd.DataFrame()\r\n",
    "        # Clean and normalize the score features\r\n",
    "        normalized_duplicates = clean_score_features(curr_country=curr_country, country_df=country_df, source_dir=config._RAW_SCORES_DIRECTORY, target_dir=config._CLEANED_SCORES_DIRECTORY, verbose=False)\r\n",
    "\r\n",
    "        if normalized_duplicates.shape[0]!=0:\r\n",
    "            \r\n",
    "            print('\\n\\nFound potential duplicates. Processing their master and cross-reference...\\n')\r\n",
    "            # Get the unique set of master-record-ids\r\n",
    "            master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=country_df)\r\n",
    "            # Get the country-master-df\r\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=config._STAGING_AREA_DIRECTORY, write_csv=False)\r\n",
    "            # Create a dummy set of cross-refs for masters\r\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\r\n",
    "            # Create full set of cross-refs for country-df\r\n",
    "            cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, curr_country=curr_country, target_dir=config._STAGING_AREA_DIRECTORY, write_csv=False)\r\n",
    "            # Create the csv for the cross-ref report\r\n",
    "            generate_cross_ref_report(cross_ref_df=cross_ref_df, curr_country=curr_country, country_df=country_df_copy, target_dir=config._STAGING_AREA_DIRECTORY)\r\n",
    "\r\n",
    "\r\n",
    "        else:\r\n",
    "            print('\\n\\nGet the unique set of all record-ids since there aren\\'t any potential duplicates.\\n')\r\n",
    "\t\t\t# Get the unique set of all-record-ids since there aren't any potential duplicates\r\n",
    "            master_record_ids = country_df.index.values.astype(list)\r\n",
    "\t\t\t# Get the country-master-df\r\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=config._STAGING_AREA_DIRECTORY, write_csv=False)\r\n",
    "\t\t\t# Create a dummy set of cross-refs for masters\r\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\r\n",
    "                \r\n",
    "        # Write the current master dataset to a csv, add the filename to the queue of csvs, and append currently generated cross-ref to existing cross-ref\r\n",
    "        new_file_name='_{}_Master.csv'.format(i)\r\n",
    "        write_df_to_csv(df=country_master_df, root_dir=config._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\r\n",
    "        new_file_name=curr_country+new_file_name\r\n",
    "        queue_of_csvs.append(new_file_name)\r\n",
    "        entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\r\n",
    "\r\n",
    "        del country_df, country_df_copy, master_record_ids\r\n",
    "        if _CREATE_MASTER_MINIBATCHES:\r\n",
    "            del normalized_duplicates\r\n",
    "\r\n",
    "print('{} csvs generated are: {}'.format(len(queue_of_csvs), queue_of_csvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max-depth for USA will be 1\n",
      "1 csvs need to be processed: ['USA_0_Master.csv'] , length=2\n",
      "\n",
      "\n",
      "Get the unique set of all record-ids since there isn't a second file to compare.\n",
      "\n",
      "53 records get merged into 53\n",
      "\n",
      "Successfully created \\Data_Files\\Master_Data\\Recursive_Staging_Area\\USA_d1_0_Master.csv!\n",
      "\n",
      "Successfully created \\Data_Files\\Master_Data\\Recursive_Staging_Area\\USA_d1_Raw_Cross_Ref.csv!\n",
      "\n",
      "\n",
      "Depth[1] processed successfully.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processed all 1 levels. Generating the master and cross-reference at the final-layer...\n",
      "\n",
      "Successfully created \\Data_Files\\Master_Data\\USA_Master.csv!\n",
      "64 records get merged into 53\n",
      "\n",
      "Successfully created \\Data_Files\\Master_Data\\USA_Raw_Cross_Ref.csv!\n",
      "\n",
      "Successfully created \\Data_Files\\Master_Data\\USA_Cross_Ref_Full_Report.csv!\n"
     ]
    }
   ],
   "source": [
    "# Number of levels for the recursive computations\r\n",
    "d=(m+1)//2\r\n",
    "print('\\nMax-depth for {} will be {}'.format(curr_country, d))\r\n",
    "\r\n",
    "\r\n",
    "for j in range(1,d+1):\r\n",
    "    combined_crossref_at_depth=pd.DataFrame()\r\n",
    "    n_csvs_to_read=len(queue_of_csvs)\r\n",
    "    length=n_csvs_to_read if n_csvs_to_read%2==0 else n_csvs_to_read+1\r\n",
    "    print('{} csvs need to be processed: {} , length={}'.format(n_csvs_to_read, queue_of_csvs, length))\r\n",
    "    for i in range(0, length, 2):\r\n",
    "        master_csv_1=os.path.join(config._STAGING_AREA_DIRECTORY, queue_of_csvs[i])\r\n",
    "        master_df_1=pd.read_csv(master_csv_1, index_col=0)\r\n",
    "        \r\n",
    "        if i+1<n_csvs_to_read:\r\n",
    "            master_csv_2=os.path.join(config._STAGING_AREA_DIRECTORY, queue_of_csvs[i+1])\r\n",
    "            master_df_2=pd.read_csv(master_csv_2, index_col=0)\r\n",
    "            \r\n",
    "\t\t\t# Invoke the Rscript and generate the Raw_score_features csv file\r\n",
    "            print('\\n{} has {} records, and {} has {} records.\\n\\nInvoking the Rscript now...\\n'.format(master_csv_1, master_df_1.shape[0],master_csv_2, master_df_2.shape[0]))\r\n",
    "            args='{} {} {} {} {} {} {} {} {} {} {}'.format(config._BINARIES_NAME, config._BINARIES_EXTENSION, config._THRESHOLD_FOR_INDIVIDUAL, config._THRESHOLD_FOR_ADDRESS_COMBINED, config._SCALING_FACTOR, curr_country, config._RAW_SCORES_DIRECTORY, config._TOTAL_MATCHES_THRESHOLD, config._LINKAGE_METHOD, master_csv_1, master_csv_2)\r\n",
    "            deduplicate_dataset_R( rscript_command=config._RSCRIPT_CMD,  script_name=config._SCRIPT_NAME, args=args )\r\n",
    "            \r\n",
    "            \r\n",
    "            normalized_duplicates=pd.DataFrame()\r\n",
    "            # Clean and normalize the score features\r\n",
    "            normalized_duplicates = clean_score_features(curr_country=curr_country, country_df=master_df_1.append(master_df_2), source_dir=config._RAW_SCORES_DIRECTORY, target_dir=config._CLEANED_SCORES_DIRECTORY, verbose=False)\r\n",
    "            \r\n",
    "            if normalized_duplicates.shape[0]!=0:\r\n",
    "                \r\n",
    "                print('\\n\\nFound potential duplicates. Processing their master and cross-reference...\\n')\r\n",
    "                # Get the unique set of master-record-ids\r\n",
    "                master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=master_df_1.append(master_df_2))\r\n",
    "                # Get the country-master-df\r\n",
    "                country_master_df = generate_deduplicated_master(country_df=master_df_1.append(master_df_2), master_record_ids=list(master_record_ids), curr_country=curr_country, target_dir=config._STAGING_AREA_DIRECTORY, write_csv=False)\r\n",
    "                # Create a dummy set of cross-refs for masters\r\n",
    "                cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\r\n",
    "                # Create full set of cross-refs for country-df\r\n",
    "                cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, curr_country=curr_country, target_dir=config._STAGING_AREA_DIRECTORY, write_csv=False)\r\n",
    "                # Create the csv for the cross-ref report\r\n",
    "                generate_cross_ref_report(cross_ref_df=cross_ref_df, country_df=master_df_1.append(master_df_2), curr_country=curr_country, target_dir=config._STAGING_AREA_DIRECTORY)\r\n",
    "                \r\n",
    "            else:\r\n",
    "                \r\n",
    "                print('\\n\\nGet the unique set of all record-ids since there aren\\'t any potential duplicates.\\n')\r\n",
    "                # Get the unique set of all-record-ids since there aren't any potential duplicates\r\n",
    "                master_record_ids = master_df_1.append(master_df_2).index.values.astype(list)\r\n",
    "                # Get the country-master-df\r\n",
    "                country_master_df = generate_deduplicated_master(country_df=master_df_1.append(master_df_2), master_record_ids=master_record_ids, curr_country=curr_country, target_dir=config._STAGING_AREA_DIRECTORY, write_csv=False)\r\n",
    "                # Create a dummy set of cross-refs for masters\r\n",
    "                cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\r\n",
    "                \r\n",
    "        else:\r\n",
    "            \r\n",
    "            print('\\n\\nGet the unique set of all record-ids since there isn\\'t a second file to compare.\\n')\r\n",
    "            # Get the unique set of master-record-ids\r\n",
    "            master_record_ids = master_df_1.index.values.astype(list)\r\n",
    "            # Get the country-master-df\r\n",
    "            country_master_df = generate_deduplicated_master(country_df=master_df_1, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=config._STAGING_AREA_DIRECTORY, write_csv=False)\r\n",
    "            # Create a dummy set of cross-refs for masters\r\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\r\n",
    "            \r\n",
    "        \r\n",
    "        # Write the current master dataset to a csv, add the filename to the queue of csvs, and append currently generated cross-ref to existing cross-ref\r\n",
    "        combined_crossref_at_depth = combined_crossref_at_depth.append(cross_ref_df)\r\n",
    "        new_file_name='_d{}_{}_Master.csv'.format(j,i)\r\n",
    "        write_df_to_csv(df=country_master_df, root_dir=config._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\r\n",
    "        new_file_name=curr_country+new_file_name\r\n",
    "        queue_of_csvs.append(new_file_name)\r\n",
    "        del master_record_ids, master_df_1\r\n",
    "        if i+1<n_csvs_to_read:\r\n",
    "            del normalized_duplicates, master_df_2\r\n",
    "    \r\n",
    "    write_df_to_csv(df=combined_crossref_at_depth, root_dir=config._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix='_d{}_Raw_Cross_Ref.csv'.format(j), index_flag=False)\r\n",
    "    print('\\n\\nDepth[{}] processed successfully.'.format(j))\r\n",
    "    update_entire_country_cross_ref(new_depth_cross_ref_df=combined_crossref_at_depth, entire_country_cross_ref_df=entire_country_cross_ref_df)\r\n",
    "    queue_of_csvs=queue_of_csvs[n_csvs_to_read:]\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "if len(queue_of_csvs)==1:\r\n",
    "    print('\\n\\n\\n\\nProcessed all {} levels. Generating the master and cross-reference at the final-layer...'.format(d))\r\n",
    "    master_csv_1=os.path.join(config._STAGING_AREA_DIRECTORY, queue_of_csvs[i])\r\n",
    "    master_df_1=pd.read_csv(master_csv_1, index_col=0)\r\n",
    "    # Get the unique set of master-record-ids\r\n",
    "    master_record_ids = master_df_1.index.values.astype(list)\r\n",
    "    # Get the country-master-df\r\n",
    "    country_master_df = generate_deduplicated_master(country_df=entire_country_df_copy, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=config._MASTER_DATA_DIRECTORY, write_csv=True)\r\n",
    "\t# Write the final raw-cross-ref to a csv\r\n",
    "    write_df_to_csv(df=entire_country_cross_ref_df, root_dir=config._MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix='_Raw_Cross_Ref.csv', index_flag=False)\r\n",
    "    # Create the csv for the cross-ref report\r\n",
    "    generate_cross_ref_report(cross_ref_df=entire_country_cross_ref_df, country_df=entire_country_df_copy, curr_country=curr_country, target_dir=config._MASTER_DATA_DIRECTORY)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59a29d8085dce36b5ecb224ea0aa434baf6012d06ca2e52b5f3f330775644297"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
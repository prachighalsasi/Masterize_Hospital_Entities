{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 32-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407"
   }
  },
  "interpreter": {
   "hash": "df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nRead the Source-file hospital_account_info.csv\n\nColumns: ['COUNTRY' 'POSTAL_CODE' 'SITE_NAME' 'STATE' 'CITY' 'ADDRESS_LINE_1'\n 'ADDRESS_LINE_2' 'ADDRESS_LINE_3' 'COUNTY_NAME' 'PHONE_NUM' 'SITE_TYPE'\n 'SITE_OWNERSHIP' 'ACCOUNT_NUM']\n\n\nCountries : ['USA']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, config as conf\n",
    "from util_functions import *\n",
    "\n",
    "#preformat_input_using_sparksql()\n",
    "#print('\\nFormatted the {} file into {} using PySpark successfully.'.format(conf._RAW_STATIC_FILE_NAME, conf._STATIC_FILE_NAME))\n",
    "\n",
    "site_master_df=pd.read_csv(conf._STATIC_FILE_NAME, index_col=0)\n",
    "#site_master_df=pd.read_excel(conf._STATIC_FILE_NAME, index_col=0)\n",
    "print('\\nRead the Source-file {}'.format(conf._STATIC_FILE_NAME))\n",
    "\n",
    "site_master_df=preprocess_dataframe(df=site_master_df)\n",
    "print('\\nColumns: {}\\n'.format(site_master_df.columns.values))\n",
    "\n",
    "countries=list(site_master_df['COUNTRY'].unique())\n",
    "print('\\nCountries : {}'.format(countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_master_df.groupby(by='COUNTRY')['COUNTRY'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for c in range(len(countries)):\n",
    "c=1\n",
    "curr_country=countries[c]\n",
    "entire_country_df=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "entire_country_df=clean_dataframe(entire_country_df, columns_to_clean=conf._COLUMNS_TO_CLEAN, fields_to_concat=conf._FIELDS_TO_CONCAT, replace_punctuations=True)\n",
    "entire_country_df_copy=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "entire_country_df_copy=clean_dataframe(entire_country_df_copy, columns_to_clean=conf._COLUMNS_TO_CLEAN, fields_to_concat=conf._FIELDS_TO_CONCAT, replace_punctuations=False)\n",
    "nrows=entire_country_df.shape[0]\n",
    "m=int(np.ceil(np.divide(nrows, conf._MAXSIZE)))\n",
    "print('\\nThere will be {} batches since incoming dataset-size={} and minibatch-size={}'.format(m, entire_country_df.shape[0], conf._MAXSIZE))\n",
    "entire_country_cross_ref_df=pd.DataFrame()\n",
    "queue_of_csvs=list()\n",
    "\n",
    "for i in range(m):\n",
    "    print('\\n\\nStarting Batch[{}]...'.format(i))\n",
    "    country_df=entire_country_df.iloc[i*conf._MAXSIZE : (i+1)*conf._MAXSIZE]\n",
    "    country_df_copy=entire_country_df_copy.iloc[i*conf._MAXSIZE : (i+1)*conf._MAXSIZE]\n",
    "    write_df_to_csv(df=country_df[conf._THRESHOLDS_DICT.keys()], curr_country=curr_country, file_suffix='_country_df.csv', index_flag=True)\n",
    "    _CREATE_MASTER_MINIBATCHES = (country_df.shape[0]>1)\n",
    "\t\n",
    "    if not _CREATE_MASTER_MINIBATCHES:\n",
    "\n",
    "        print('\\n\\nGet the unique set of all record-ids, since Layer-zero cannot create mastered mini-batches.\\n')\n",
    "        # Get the unique set of master-record-ids\n",
    "        master_record_ids = country_df.index.values.astype(list)\n",
    "        # Create the country-master-df\n",
    "        country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "        # Create a dummy set of cross-refs for masters\n",
    "        cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "\n",
    "        # Write the current master dataset to a csv, add the filename to the queue of csvs, and append currently generated cross-ref to existing cross-ref\n",
    "        new_file_name='_{}_Master.csv'.format(i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\n",
    "\n",
    "\n",
    "    elif _CREATE_MASTER_MINIBATCHES:\n",
    "\n",
    "        # Invoke the Rscript and generate the Raw_score_features csv file for each minibatch\n",
    "        args='{} {} {} {} {} {} {} {} {} NA NA'.format(conf._BINARIES_NAME, conf._BINARIES_EXTENSION, conf._THRESHOLD_FOR_INDIVIDUAL, conf._THRESHOLD_FOR_ADDRESS_COMBINED, conf._SCALING_FACTOR, curr_country, conf._RAW_SCORES_DIRECTORY, conf._TOTAL_MATCHES_THRESHOLD, conf._DEDUP_METHOD)\n",
    "        print('\\n{}_{} has {} records.\\n\\nInvoking the Rscript now...'.format(curr_country, i, country_df.shape[0]))\n",
    "        deduplicate_dataset_R( rscript_command=conf._RSCRIPT_CMD,  script_name=conf._SCRIPT_NAME, args=args )\n",
    "\n",
    "        normalized_duplicates=pd.DataFrame()\n",
    "        # Clean and normalize the score features\n",
    "        normalized_duplicates = clean_score_features(curr_country=curr_country, country_df=country_df, source_dir=conf._RAW_SCORES_DIRECTORY, target_dir=conf._CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "\n",
    "        if normalized_duplicates.shape[0]!=0:\n",
    "            \n",
    "            print('\\n\\nFound potential duplicates. Processing their master and cross-reference...\\n')\n",
    "            # Get the unique set of master-record-ids\n",
    "            master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=country_df)\n",
    "            # Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "            # Create full set of cross-refs for country-df\n",
    "            cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create the csv for the cross-ref report\n",
    "            generate_cross_ref_report(cross_ref_df=cross_ref_df, curr_country=curr_country, country_df=country_df_copy, target_dir=conf._STAGING_AREA_DIRECTORY)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('\\n\\nGet the unique set of all record-ids since there aren\\'t any potential duplicates.\\n')\n",
    "\t\t\t# Get the unique set of all-record-ids since there aren't any potential duplicates\n",
    "            master_record_ids = country_df.index.values.astype(list)\n",
    "\t\t\t# Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "\t\t\t# Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "                \n",
    "        # Write the current master dataset to a csv, add the filename to the queue of csvs, and append currently generated cross-ref to existing cross-ref\n",
    "        new_file_name='_{}_Master.csv'.format(i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\n",
    "\n",
    "        del country_df, country_df_copy, master_record_ids\n",
    "        if _CREATE_MASTER_MINIBATCHES:\n",
    "            del normalized_duplicates\n",
    "\n",
    "print('{} csvs generated are: {}'.format(len(queue_of_csvs), queue_of_csvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of levels for the recursive computations\n",
    "d=(m+1)//2\n",
    "print('\\nMax-depth for {} will be {}'.format(curr_country, d))\n",
    "\n",
    "\n",
    "for j in range(1,d+1):\n",
    "    combined_crossref_at_depth=pd.DataFrame()\n",
    "    n_csvs_to_read=len(queue_of_csvs)\n",
    "    length=n_csvs_to_read if n_csvs_to_read%2==0 else n_csvs_to_read+1\n",
    "    print('{} csvs need to be processed: {} , length={}'.format(n_csvs_to_read, queue_of_csvs, length))\n",
    "    for i in range(0, length, 2):\n",
    "        master_csv_1=os.path.join(conf._STAGING_AREA_DIRECTORY, queue_of_csvs[i])\n",
    "        master_df_1=pd.read_csv(master_csv_1, index_col=0)\n",
    "        \n",
    "        if i+1<n_csvs_to_read:\n",
    "            master_csv_2=os.path.join(conf._STAGING_AREA_DIRECTORY, queue_of_csvs[i+1])\n",
    "            master_df_2=pd.read_csv(master_csv_2, index_col=0)\n",
    "            \n",
    "\t\t\t# Invoke the Rscript and generate the Raw_score_features csv file\n",
    "            print('\\n{} has {} records, and {} has {} records.\\n\\nInvoking the Rscript now...\\n'.format(master_csv_1, master_df_1.shape[0],master_csv_2, master_df_2.shape[0]))\n",
    "            args='{} {} {} {} {} {} {} {} {} {} {}'.format(conf._BINARIES_NAME, conf._BINARIES_EXTENSION, conf._THRESHOLD_FOR_INDIVIDUAL, conf._THRESHOLD_FOR_ADDRESS_COMBINED, conf._SCALING_FACTOR, curr_country, conf._RAW_SCORES_DIRECTORY, conf._TOTAL_MATCHES_THRESHOLD, conf._LINKAGE_METHOD, master_csv_1, master_csv_2)\n",
    "            deduplicate_dataset_R( rscript_command=conf._RSCRIPT_CMD,  script_name=conf._SCRIPT_NAME, args=args )\n",
    "            \n",
    "            \n",
    "            normalized_duplicates=pd.DataFrame()\n",
    "            # Clean and normalize the score features\n",
    "            normalized_duplicates = clean_score_features(curr_country=curr_country, country_df=master_df_1.append(master_df_2), source_dir=conf._RAW_SCORES_DIRECTORY, target_dir=conf._CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "            \n",
    "            if normalized_duplicates.shape[0]!=0:\n",
    "                \n",
    "                print('\\n\\nFound potential duplicates. Processing their master and cross-reference...\\n')\n",
    "                # Get the unique set of master-record-ids\n",
    "                master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=master_df_1.append(master_df_2))\n",
    "                # Get the country-master-df\n",
    "                country_master_df = generate_deduplicated_master(country_df=master_df_1.append(master_df_2), master_record_ids=list(master_record_ids), curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create a dummy set of cross-refs for masters\n",
    "                cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "                # Create full set of cross-refs for country-df\n",
    "                cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create the csv for the cross-ref report\n",
    "                generate_cross_ref_report(cross_ref_df=cross_ref_df, country_df=master_df_1.append(master_df_2), curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                print('\\n\\nGet the unique set of all record-ids since there aren\\'t any potential duplicates.\\n')\n",
    "                # Get the unique set of all-record-ids since there aren't any potential duplicates\n",
    "                master_record_ids = master_df_1.append(master_df_2).index.values.astype(list)\n",
    "                # Get the country-master-df\n",
    "                country_master_df = generate_deduplicated_master(country_df=master_df_1.append(master_df_2), master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create a dummy set of cross-refs for masters\n",
    "                cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            print('\\n\\nGet the unique set of all record-ids since there isn\\'t a second file to compare.\\n')\n",
    "            # Get the unique set of master-record-ids\n",
    "            master_record_ids = master_df_1.index.values.astype(list)\n",
    "            # Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=master_df_1, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "            \n",
    "        \n",
    "        # Write the current master dataset to a csv, add the filename to the queue of csvs, and append currently generated cross-ref to existing cross-ref\n",
    "        combined_crossref_at_depth = combined_crossref_at_depth.append(cross_ref_df)\n",
    "        new_file_name='_d{}_{}_Master.csv'.format(j,i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        del master_record_ids, master_df_1\n",
    "        if i+1<n_csvs_to_read:\n",
    "            del normalized_duplicates, master_df_2\n",
    "    \n",
    "    write_df_to_csv(df=combined_crossref_at_depth, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix='_d{}_Raw_Cross_Ref.csv'.format(j), index_flag=False)\n",
    "    print('\\n\\nDepth[{}] processed successfully.'.format(j))\n",
    "    update_entire_country_cross_ref(new_depth_cross_ref_df=combined_crossref_at_depth, entire_country_cross_ref_df=entire_country_cross_ref_df)\n",
    "    queue_of_csvs=queue_of_csvs[n_csvs_to_read:]\n",
    "\n",
    "\n",
    "\n",
    "if len(queue_of_csvs)==1:\n",
    "    print('\\n\\n\\n\\nProcessed all {} levels. Generating the master and cross-reference at the final-layer...'.format(d))\n",
    "    master_csv_1=os.path.join(conf._STAGING_AREA_DIRECTORY, queue_of_csvs[i])\n",
    "    master_df_1=pd.read_csv(master_csv_1, index_col=0)\n",
    "    # Get the unique set of master-record-ids\n",
    "    master_record_ids = master_df_1.index.values.astype(list)\n",
    "    # Get the country-master-df\n",
    "    country_master_df = generate_deduplicated_master(country_df=entire_country_df_copy, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._MASTER_DATA_DIRECTORY, write_csv=True)\n",
    "\t# Write the final raw-cross-ref to a csv\n",
    "    write_df_to_csv(df=entire_country_cross_ref_df, root_dir=conf._MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix='_Raw_Cross_Ref.csv', index_flag=False)\n",
    "    # Create the csv for the cross-ref report\n",
    "    generate_cross_ref_report(cross_ref_df=entire_country_cross_ref_df, country_df=entire_country_df_copy, curr_country=curr_country, target_dir=conf._MASTER_DATA_DIRECTORY)"
   ]
  }
 ]
}
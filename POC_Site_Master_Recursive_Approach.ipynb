{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. Initialization\n",
    "\n",
    "> a. Read the excel/csv containing the entire query-output\n",
    "\n",
    "> b. Initialize the parameters of thresholds, directories, and fields to concat for generating individual as well as combined match-score\n",
    "\n",
    "> c. Get the unique list of countries in present dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nColumns:  ['SOURCE_IDENTIFIER' 'DATA_SOURCE_NAME' 'PROTOCOL_NUMBER' 'SITE_NUM'\n 'UNIQUE_SITE_ID' 'COUNTRY' 'SITE_NAME' 'STATE' 'CITY' 'ADDRESS_LINE_1'\n 'ADDRESS_LINE_2' 'ADDRESS_LINE_3' 'POSTAL_CODE' 'SITE_STATUS'] \n\n\nUnique Countries having counts greater than  1000 :  ['United_States']\n"
     ]
    }
   ],
   "source": [
    "import time, numpy as np, pandas as pd, re, string, subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "_STATIC_FILE_NAME=\"SM_Temp_Shortlist.xlsx\"\n",
    "_RAW_SCORES_DIRECTORY='Raw_Scores'\n",
    "_CLEANED_SCORES_DIRECTORY='Cleaned_Scores'\n",
    "_MASTER_DATA_DIRECTORY='Master_Data'\n",
    "_FIELDS_TO_CONCAT={ 'CONCAT_ADDRESS':   ['ADDRESS_LINE_1','ADDRESS_LINE_2','ADDRESS_LINE_3'] }\n",
    "\n",
    "_COLUMNS_TO_CLEAN=['ADDRESS_LINE_1','ADDRESS_LINE_2','ADDRESS_LINE_3','SITE_NAME','STATE','CITY','POSTAL_CODE']\n",
    "_BINARIES_NAME=\"levenshtein\"\n",
    "_BINARIES_EXTENSION=\".dll\"\n",
    "_MAXSIZE=1000\n",
    "\n",
    "_THRESHOLD_FOR_INDIVIDUAL=0.85\n",
    "_THRESHOLD_FOR_ADDRESS_COMBINED=0.75\n",
    "\n",
    "_THRESHOLDS_DICT={\n",
    "    'CONCAT_ADDRESS': _THRESHOLD_FOR_ADDRESS_COMBINED,\n",
    "    'SITE_NAME': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'STATE': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'CITY': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'POSTAL_CODE': _THRESHOLD_FOR_INDIVIDUAL\n",
    "    }\n",
    "_COLS_FOR_TOTAL_MATCH_CALC=[colname+'_COMPARISON_SCORE' for colname in _THRESHOLDS_DICT]\n",
    "\n",
    "_SCALING_FACTOR=3\n",
    "\n",
    "_TOTAL_MATCHES_THRESHOLD=4\n",
    "\n",
    "_DEDUP_METHOD='Dedup'\n",
    "_LINKAGE_METHOD='Linkage'\n",
    "\n",
    "def write_df_to_csv(df, root_dir='', curr_country='', file_suffix='_temp.csv', index_flag=False):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Writes the dataframe to a csv file and throw error if it fails.\n",
    "        INPUT:      Dataframe, Target-Directory, Country-name, Suffix-of-csv-file, Index-Flag\n",
    "        OUTPUT:     Dataframe csv at target-directory, or error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        abs_path=os.path.join(root_dir, curr_country+file_suffix)\n",
    "        df.to_csv(abs_path, index=index_flag)\n",
    "        print(f'\\nSuccessfully created \\{abs_path}!')\n",
    "    except:\n",
    "        print(f'\\nSomething went wrong while writing the file. Please check if it is currently in use.')\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Imputes blank cells with '', replaces whitespace with underscore in country-name, and strips whitespace in cells.\n",
    "        INPUT:      Dataframe\n",
    "        OUTPUT:     Imputed and cleaned dataframe.\n",
    "    \"\"\"\n",
    "    df.replace(np.nan, '', inplace=True)\n",
    "    for colname in df.columns.values:\n",
    "        if colname=='COUNTRY':\n",
    "            df[colname]=df[colname].apply(lambda x: x.replace(' ','_'))\n",
    "        df[colname]=df[colname].astype(str).apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "def clean_dataframe(df, columns_to_clean=_COLUMNS_TO_CLEAN, fields_to_concat=_FIELDS_TO_CONCAT, replace_punctuations=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Replaces special-chars in lowercase-converted cells if replace_punctuation==True, for the columns relevant to computing match-scores.\n",
    "                    Generates the concatenated address fields, and drops the individual ones.\n",
    "                    Overall will be left with alphanumeric chars in UTF-8 encoding.\n",
    "        INPUT:      Dataframe, columns-to-clean, address-fields-to-concat, flag-to-replace-punctuations\n",
    "        OUTPUT:     Imputed and cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # Added another special character which was causing Italy CSV file read to fail in R\n",
    "    if replace_punctuations:\n",
    "        special_chars=re.escape(string.punctuation)+'\u001a'\n",
    "        print('\\nSpecial Character that will be replaced are:  ', special_chars)\n",
    "    for colname in df.columns.values:\n",
    "        if colname in columns_to_clean and replace_punctuations:\n",
    "            df[colname]=df[colname].replace(r'['+special_chars+']', '', regex=True).str.lower()\n",
    "    for colname, cols_to_concat in fields_to_concat.items():\n",
    "        df[colname]=df[cols_to_concat].apply(lambda single_row: ''.join(single_row.values), axis=1)\n",
    "    df.drop(labels=fields_to_concat['CONCAT_ADDRESS'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def deduplicate_dataset_R(rscript_command, script_name, args):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Invokes the R-code from Python using 32-bit Rscript 3.4.4 command.\n",
    "                    Uses the Python subprocess module to create a new Pipe.\n",
    "        INPUT:      Abs-path-of-32bit-Rscript-command, Script-to-invoke, Args-for-script\n",
    "        OUTPUT:     Prints R-console output based on return-code. Rscript command generates a csv of the score_features, or errors out.\n",
    "    \"\"\"\n",
    "    cmd = [rscript_command, script_name, args]\n",
    "    pipe = Popen( cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE )\n",
    "    output, error = pipe.communicate()\n",
    "\n",
    "    if pipe.returncode==0:\n",
    "        print('R OUTPUT:\\n',output.decode())\n",
    "    else:\n",
    "        print('R OUTPUT:\\n',output.decode())\n",
    "        print('R ERROR:\\n',error.decode())\n",
    "\n",
    "\n",
    "\n",
    "def scale_up_comparison_score(df, colname='SITE_NAME_COMPARISON_SCORE', scaling_factor=_SCALING_FACTOR):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Scale-up a column's binary-valued score by a factor\n",
    "        INPUT:      Dataframe, score-colname, scaling-factor\n",
    "        OUTPUT:     Scaled up dataframe.\n",
    "    \"\"\"\n",
    "    df[colname]=df[colname].apply(lambda x: x*scaling_factor)\n",
    "\n",
    "\n",
    "\n",
    "def return_top_match(df, child_column, score_key_column):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Input Dataframe has SR_NUM_1 (child-col) matching against multiple SR_NUM_2.\n",
    "                    Orders by child-col asc, score-col desc, and chooses the first possible entry of child-col.\n",
    "        INPUT:      Dataframe-of-score-features-above-a-total-threshold, index-column (SR_NUM_1), total-score-column (NUM_OF_MATCHES_FOUND)\n",
    "        OUTPUT:     Dataframe of normalized-score-features.\n",
    "    \"\"\"\n",
    "    normalized_duplicates=df.sort_values(by=[child_column]).sort_values(by=[score_key_column],ascending=False)\n",
    "    normalized_duplicates=normalized_duplicates.groupby(child_column).head(1).sort_values(by=[child_column])\n",
    "    return normalized_duplicates\n",
    "\n",
    "\n",
    "\n",
    "def replace_cyclic_dependencies(df, child_indicator, master_indicator, verbose=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Input Dataframe has cases like-     Record45 matches with Record44, and Record67 matches with Record45.\n",
    "                    In this case we should maintain-    Record67 matches with Record44.\n",
    "                    Applies a for-loop and replaces values in master-column whenever such a cyclic-occurence observed.\n",
    "        INPUT:      Dataframe-of-score-features-with-cyclic-indexes, child-column, master-column\n",
    "        OUTPUT:     Dataframe of normalized-score-features.\n",
    "    \"\"\"\n",
    "    arr=set(df[child_indicator].array)\n",
    "    for val in df[master_indicator]:\n",
    "        if val in arr:\n",
    "            replace_val=df[df[child_indicator]==val][master_indicator].values[0]\n",
    "            if verbose:\n",
    "                print(val,' found in normalized_duplicates[',child_indicator,']. Replacement: ', replace_val)\n",
    "            df[master_indicator].replace(val, replace_val, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_score_features(curr_country, source_dir=_RAW_SCORES_DIRECTORY, target_dir=_CLEANED_SCORES_DIRECTORY, verbose=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Reads the output of the Rscript command that is a csv of score_features having total-score greater than a total-threshold.\n",
    "                    Invokes the top-match function, and the replace-cyclic-occurences function to get a set of clean-score-features.\n",
    "                    Writes the dataframe in the Cleaned-Scores directory.\n",
    "        INPUT:      country-name\n",
    "        OUTPUT:     Dataframe of cleaned-normalized-score-features.\n",
    "    \"\"\"\n",
    "    duplicates=pd.read_csv(os.path.join(source_dir, curr_country+'_Score_Features.csv'))\n",
    "    # if no potential duplicates found, return an empty df\n",
    "    if duplicates.shape[0]==1 and duplicates['SR_NUM_1'][0]==0 and duplicates['SR_NUM_2'][0]==0:\n",
    "        return duplicates.head(0)\n",
    "    duplicates['COUNTRY']=curr_country\n",
    "    duplicates=return_top_match(df=duplicates, child_column='SR_NUM_1', score_key_column='NUM_OF_MATCHES_FOUND')\n",
    "    duplicates=replace_cyclic_dependencies(df=duplicates, child_indicator='SR_NUM_1', master_indicator='SR_NUM_2', verbose=verbose)\n",
    "    write_df_to_csv(df=duplicates, root_dir=target_dir, curr_country=curr_country, file_suffix='_Cleaned_Feature_Scores.csv')\n",
    "    print('\\n\"SR_NUM_2\" will be the master record')\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "def get_deduplicated_master_records(normalized_duplicates, country_df):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  From the list of cleaned-normalized-score-features, use set-theory to find the unique list of masters.\n",
    "                        a.  Think of 'SR_NUM_1' as the list of incoming Primary-keys, and 'SR_NUM_2' as the value to which it should be mapped based on match-score.\n",
    "                        b.  Hence, union of 'SR_NUM_1' & 'SR_NUM_2' will be entire set of duplicates.\n",
    "                        c.  Stand-alone records in the current country_batch_dataframe will not fall in this entire set of duplicates.\n",
    "                        d.  Master-records set wil be the sets of 'SR_NUM_2' & #c above.\n",
    "                        >   Universe                            = {SR_NUM}\n",
    "                        >   a1                                  = {SR_NUM_1}\n",
    "                        >   a2                                  = {SR_NUM_2}\n",
    "                        >   Falls into any duplication-scenario = anymatch  = {a1 U a2}\n",
    "                        >   Falls into no duplication-scenario  = nomatch   = {Universe - anymatch}\n",
    "                        >   Total masters                       = {nomatch U a2}\n",
    "        INPUT:      Dataframe-of-cleaned-normalized-score_features\n",
    "        OUTPUT:     Unique set of master-record-ids (SR_NUM)\n",
    "    \"\"\"\n",
    "    a1=set(normalized_duplicates['SR_NUM_1'].values.tolist())\n",
    "    a2=set(normalized_duplicates['SR_NUM_2'].values.tolist())\n",
    "    country_set=set(country_df.index.values.tolist())\n",
    "    entire_duplicates_set=a1.union(a2)\n",
    "    no_match_set=country_set.difference(entire_duplicates_set)\n",
    "    master_record_ids=no_match_set.union(a2)\n",
    "    return master_record_ids\n",
    "\n",
    "\n",
    "def generate_deduplicated_master(country_df, master_record_ids, target_dir=_MASTER_DATA_DIRECTORY, write_csv=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Use the original df to extract columns-info and generate the country-specific Master file.\n",
    "        INPUT:      Original-country-Dataframe, Unique set of master-record-ids (SR_NUM)\n",
    "        OUTPUT:     Dataframe-for-country-with-original-info, Master-Dataframe\n",
    "    \"\"\"\n",
    "    country_master_df=country_df.loc[master_record_ids]\n",
    "    if write_csv:\n",
    "        write_df_to_csv(df=country_master_df, root_dir=target_dir, curr_country=curr_country, index_flag=True, file_suffix='_Master.csv')\n",
    "    print(f'{country_df.shape[0]} records get merged into {len(master_record_ids)}')\n",
    "    return country_master_df\n",
    "\n",
    "\n",
    "\n",
    "def generate_dummy_cross_refs_for_masters(master_record_ids):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Create a dummy cross-reference dataframe for master-records; Record45 matches with Record45 having a total match-score of maximum.\n",
    "        INPUT:      Unique set of master-record-ids (SR_NUM)\n",
    "        OUTPUT:     Dataframe-of-dummy-entries-for-master-cross-references.\n",
    "    \"\"\"\n",
    "    master_record_score_array=[1.0]*len(master_record_ids)\n",
    "    master_record_df_dict={\n",
    "        'SR_NUM_1': list(master_record_ids),\n",
    "        'SR_NUM_2': list(master_record_ids),\n",
    "        'SITE_NAME_COMPARISON_SCORE': master_record_score_array,\n",
    "        'STATE_COMPARISON_SCORE': master_record_score_array,\n",
    "        'CITY_COMPARISON_SCORE': master_record_score_array,\n",
    "        'CONCAT_ADDRESS_COMPARISON_SCORE': master_record_score_array,\n",
    "        'POSTAL_CODE_COMPARISON_SCORE': master_record_score_array }\n",
    "\n",
    "    cross_ref_df=pd.DataFrame(master_record_df_dict)\n",
    "    cross_ref_df['COUNTRY']=curr_country\n",
    "    scale_up_comparison_score(cross_ref_df,'CONCAT_ADDRESS_COMPARISON_SCORE',_SCALING_FACTOR)\n",
    "    cross_ref_df['NUM_OF_MATCHES_FOUND']=cross_ref_df[_COLS_FOR_TOTAL_MATCH_CALC].sum(axis=1)\n",
    "    return cross_ref_df\n",
    "\n",
    "\n",
    "def generate_final_cross_refs(cross_ref_df, normalized_duplicates, target_dir=_MASTER_DATA_DIRECTORY, write_csv=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Merge the dummy cross-reference of masters, with the cleaned-normalized-feature-scores.\n",
    "        INPUT:      Dataframe-of-dummy-entries-for-master-cross-references, Dataframe-of-cleaned-normalized-score_features\n",
    "        OUTPUT:     Dataframe-of-cross-references.\n",
    "    \"\"\"\n",
    "    cross_ref_df=cross_ref_df.append(normalized_duplicates)\n",
    "    cross_ref_df.sort_values(by=['SR_NUM_1'], axis=0, inplace=True)\n",
    "    if write_csv:\n",
    "        write_df_to_csv(df=cross_ref_df, root_dir=target_dir, curr_country=curr_country, file_suffix='_Raw_Cross_Ref.csv')\n",
    "    return cross_ref_df\n",
    "\n",
    "\n",
    "\n",
    "def generate_cross_ref_report(cross_ref_df, country_df, target_dir=_MASTER_DATA_DIRECTORY):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Creates cross-reference report by performing left-join of cross-reference-dataframe with the original-info in country-df.\n",
    "                        a. Merge the master_cross_reference_df with the country_batch_dataframe as a left-outer-join on Primary-key='SR_NUM_1'\n",
    "                        b. Merge this master_cross_reference_df with the country_batch_dataframe as a left-outer-join on Primary-key='SR_NUM_2'\n",
    "                    Writes the dataframe in the Master-Data directory.\n",
    "        INPUT:      Dataframe-of-cross-references, Dataframe-for-country-with-original-info\n",
    "        OUTPUT:     Dataframe-of-cross-references-with-original-info.\n",
    "    \"\"\"\n",
    "    country_df.reset_index(inplace=True)\n",
    "    country_df_colnames=country_df.columns.values\n",
    "\n",
    "    country_df.columns=[colname+'_1' for colname in country_df_colnames]\n",
    "    cross_ref_df=cross_ref_df.merge(country_df, how='left', on='SR_NUM_1')\n",
    "\n",
    "    country_df.columns=[colname+'_2' for colname in country_df_colnames]\n",
    "    cross_ref_df=cross_ref_df.merge(country_df, how='left', on='SR_NUM_2')\n",
    "\n",
    "    columns_in_report_format=['SR_NUM_1', 'SR_NUM_2', 'SITE_NAME_1','SITE_NAME_2','SITE_NAME_COMPARISON_SCORE','STATE_1','STATE_2','STATE_COMPARISON_SCORE', 'CITY_1', 'CITY_2','CITY_COMPARISON_SCORE','CONCAT_ADDRESS_1','CONCAT_ADDRESS_2','CONCAT_ADDRESS_COMPARISON_SCORE', 'POSTAL_CODE_1','POSTAL_CODE_2',   'POSTAL_CODE_COMPARISON_SCORE','NUM_OF_MATCHES_FOUND']\n",
    "    cross_ref_df=cross_ref_df[columns_in_report_format]\n",
    "    write_df_to_csv(df=cross_ref_df, root_dir=target_dir, curr_country=curr_country, file_suffix='_Cross_Ref_Full_Report.csv')\n",
    "\n",
    "\n",
    "\n",
    "site_master_df=pd.read_excel(_STATIC_FILE_NAME, index_col=0)\n",
    "preprocess_dataframe(site_master_df)\n",
    "print('\\nColumns: ', site_master_df.columns.values,'\\n')\n",
    "countries=list(site_master_df['COUNTRY'].unique())\n",
    "small_countries=list(site_master_df['COUNTRY'].value_counts()[site_master_df['COUNTRY'].value_counts() <= _MAXSIZE].index)\n",
    "for c in small_countries:\n",
    "    countries.remove(c)\n",
    "print('\\nUnique Countries having counts greater than ', _MAXSIZE, ': ', countries)"
   ]
  },
  {
   "source": [
    "# TESTING OUT THE RECURSIVE APPROACH WITH MINI-BATCHES OF MASTERS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Special Character that will be replaced are:   !\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\u001a\n",
      "\n",
      "\n",
      "Starting Batch[ 0 ]...\n",
      "\n",
      "Successfully created \\United_States_country_df.csv!\n",
      "\n",
      "United_States_0 has 1000 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "R OUTPUT:\n",
      " [1] \"levenshtein .dll 0.85 0.75 3 United_States Raw_Scores 4 Dedup NA NA\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 120133  3.3     350000  9.4   302969  8.1\n",
      "Vcells 169834  1.3     786432  6.0   697526  5.4\n",
      "[1] \"NRows= 1000 , Candidate-pairs= 499500 , Columns are \"\n",
      "[1] \"SR_NUM\"         \"CONCAT_ADDRESS\" \"SITE_NAME\"      \"STATE\"         \n",
      "[5] \"CITY\"           \"POSTAL_CODE\"   \n",
      "[1] \"N_combinations= 499500 , Columns are \"\n",
      "[1] \"id1\"            \"id2\"            \"CONCAT_ADDRESS\" \"SITE_NAME\"     \n",
      "[5] \"STATE\"          \"CITY\"           \"POSTAL_CODE\"    \"is_match\"      \n",
      "[1] \"Scaling up column scores if threshold crossed\"\n",
      "[1] \"SITE_NAME  :  0.85\"\n",
      "[1] \"STATE  :  0.85\"\n",
      "[1] \"CITY  :  0.85\"\n",
      "[1] \"POSTAL_CODE  :  0.85\"\n",
      "[1] \"CONCAT_ADDRESS  :  0.75\"\n",
      "[1] \"Raw_Scores//United_States_Score_Features.csv\"\n",
      "[1] \"Successfully created //Raw_Scores//United_States_Score_Features.csv !\"\n",
      "Time difference of 12.92128 secs\n",
      "\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\United_States_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "1000 records get merged into 258\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_0_Master.csv!\n",
      "\n",
      "\n",
      "Starting Batch[ 1 ]...\n",
      "\n",
      "Successfully created \\United_States_country_df.csv!\n",
      "\n",
      "United_States_1 has 1000 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "R OUTPUT:\n",
      " [1] \"levenshtein .dll 0.85 0.75 3 United_States Raw_Scores 4 Dedup NA NA\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 120133  3.3     350000  9.4   302969  8.1\n",
      "Vcells 169834  1.3     786432  6.0   697526  5.4\n",
      "[1] \"NRows= 1000 , Candidate-pairs= 499500 , Columns are \"\n",
      "[1] \"SR_NUM\"         \"CONCAT_ADDRESS\" \"SITE_NAME\"      \"STATE\"         \n",
      "[5] \"CITY\"           \"POSTAL_CODE\"   \n",
      "[1] \"N_combinations= 499500 , Columns are \"\n",
      "[1] \"id1\"            \"id2\"            \"CONCAT_ADDRESS\" \"SITE_NAME\"     \n",
      "[5] \"STATE\"          \"CITY\"           \"POSTAL_CODE\"    \"is_match\"      \n",
      "[1] \"Scaling up column scores if threshold crossed\"\n",
      "[1] \"SITE_NAME  :  0.85\"\n",
      "[1] \"STATE  :  0.85\"\n",
      "[1] \"CITY  :  0.85\"\n",
      "[1] \"POSTAL_CODE  :  0.85\"\n",
      "[1] \"CONCAT_ADDRESS  :  0.75\"\n",
      "[1] \"Raw_Scores//United_States_Score_Features.csv\"\n",
      "[1] \"Successfully created //Raw_Scores//United_States_Score_Features.csv !\"\n",
      "Time difference of 14.29852 secs\n",
      "\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\United_States_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "1000 records get merged into 252\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_1_Master.csv!\n",
      "\n",
      "\n",
      "Starting Batch[ 2 ]...\n",
      "\n",
      "Successfully created \\United_States_country_df.csv!\n",
      "\n",
      "United_States_2 has 1000 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "R OUTPUT:\n",
      " [1] \"levenshtein .dll 0.85 0.75 3 United_States Raw_Scores 4 Dedup NA NA\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 120133  3.3     350000  9.4   302969  8.1\n",
      "Vcells 169834  1.3     786432  6.0   697526  5.4\n",
      "[1] \"NRows= 1000 , Candidate-pairs= 499500 , Columns are \"\n",
      "[1] \"SR_NUM\"         \"CONCAT_ADDRESS\" \"SITE_NAME\"      \"STATE\"         \n",
      "[5] \"CITY\"           \"POSTAL_CODE\"   \n",
      "[1] \"N_combinations= 499500 , Columns are \"\n",
      "[1] \"id1\"            \"id2\"            \"CONCAT_ADDRESS\" \"SITE_NAME\"     \n",
      "[5] \"STATE\"          \"CITY\"           \"POSTAL_CODE\"    \"is_match\"      \n",
      "[1] \"Scaling up column scores if threshold crossed\"\n",
      "[1] \"SITE_NAME  :  0.85\"\n",
      "[1] \"STATE  :  0.85\"\n",
      "[1] \"CITY  :  0.85\"\n",
      "[1] \"POSTAL_CODE  :  0.85\"\n",
      "[1] \"CONCAT_ADDRESS  :  0.75\"\n",
      "[1] \"Raw_Scores//United_States_Score_Features.csv\"\n",
      "[1] \"Successfully created //Raw_Scores//United_States_Score_Features.csv !\"\n",
      "Time difference of 14.55517 secs\n",
      "\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\United_States_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "1000 records get merged into 262\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_2_Master.csv!\n",
      "\n",
      "\n",
      "Starting Batch[ 3 ]...\n",
      "\n",
      "Successfully created \\United_States_country_df.csv!\n",
      "\n",
      "United_States_3 has 1000 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "R OUTPUT:\n",
      " [1] \"levenshtein .dll 0.85 0.75 3 United_States Raw_Scores 4 Dedup NA NA\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 120133  3.3     350000  9.4   302969  8.1\n",
      "Vcells 169834  1.3     786432  6.0   697526  5.4\n",
      "[1] \"NRows= 1000 , Candidate-pairs= 499500 , Columns are \"\n",
      "[1] \"SR_NUM\"         \"CONCAT_ADDRESS\" \"SITE_NAME\"      \"STATE\"         \n",
      "[5] \"CITY\"           \"POSTAL_CODE\"   \n",
      "[1] \"N_combinations= 499500 , Columns are \"\n",
      "[1] \"id1\"            \"id2\"            \"CONCAT_ADDRESS\" \"SITE_NAME\"     \n",
      "[5] \"STATE\"          \"CITY\"           \"POSTAL_CODE\"    \"is_match\"      \n",
      "[1] \"Scaling up column scores if threshold crossed\"\n",
      "[1] \"SITE_NAME  :  0.85\"\n",
      "[1] \"STATE  :  0.85\"\n",
      "[1] \"CITY  :  0.85\"\n",
      "[1] \"POSTAL_CODE  :  0.85\"\n",
      "[1] \"CONCAT_ADDRESS  :  0.75\"\n",
      "[1] \"Raw_Scores//United_States_Score_Features.csv\"\n",
      "[1] \"Successfully created //Raw_Scores//United_States_Score_Features.csv !\"\n",
      "Time difference of 14.50277 secs\n",
      "\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\United_States_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "1000 records get merged into 297\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_3_Master.csv!\n",
      "\n",
      "\n",
      "Starting Batch[ 4 ]...\n",
      "\n",
      "Successfully created \\United_States_country_df.csv!\n",
      "\n",
      "United_States_4 has 338 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "R OUTPUT:\n",
      " [1] \"levenshtein .dll 0.85 0.75 3 United_States Raw_Scores 4 Dedup NA NA\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 120133  3.3     350000  9.4   302969  8.1\n",
      "Vcells 169834  1.3     786432  6.0   697526  5.4\n",
      "[1] \"NRows= 338 , Candidate-pairs= 56953 , Columns are \"\n",
      "[1] \"SR_NUM\"         \"CONCAT_ADDRESS\" \"SITE_NAME\"      \"STATE\"         \n",
      "[5] \"CITY\"           \"POSTAL_CODE\"   \n",
      "[1] \"N_combinations= 56953 , Columns are \"\n",
      "[1] \"id1\"            \"id2\"            \"CONCAT_ADDRESS\" \"SITE_NAME\"     \n",
      "[5] \"STATE\"          \"CITY\"           \"POSTAL_CODE\"    \"is_match\"      \n",
      "[1] \"Scaling up column scores if threshold crossed\"\n",
      "[1] \"SITE_NAME  :  0.85\"\n",
      "[1] \"STATE  :  0.85\"\n",
      "[1] \"CITY  :  0.85\"\n",
      "[1] \"POSTAL_CODE  :  0.85\"\n",
      "[1] \"CONCAT_ADDRESS  :  0.75\"\n",
      "[1] \"Raw_Scores//United_States_Score_Features.csv\"\n",
      "[1] \"Successfully created //Raw_Scores//United_States_Score_Features.csv !\"\n",
      "Time difference of 2.167294 secs\n",
      "\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\United_States_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "338 records get merged into 93\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_4_Master.csv!\n",
      "5 csvs generated are: ['United_States_0_Master.csv', 'United_States_1_Master.csv', 'United_States_2_Master.csv', 'United_States_3_Master.csv', 'United_States_4_Master.csv']\n"
     ]
    }
   ],
   "source": [
    "# todo: for loop- Research on multithreading to speed up country-wise batches. RAM might crash for incoming batch-size>_MAXSIZE.\n",
    "c=0\n",
    "curr_country=countries[c]\n",
    "entire_country_df=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "clean_dataframe(entire_country_df, columns_to_clean=_COLUMNS_TO_CLEAN, fields_to_concat=_FIELDS_TO_CONCAT, replace_punctuations=True)\n",
    "entire_country_df_copy=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "clean_dataframe(entire_country_df_copy, columns_to_clean=_COLUMNS_TO_CLEAN, fields_to_concat=_FIELDS_TO_CONCAT, replace_punctuations=False)\n",
    "nrows=entire_country_df.shape[0]\n",
    "m=(nrows//_MAXSIZE)+1\n",
    "entire_country_cross_ref_df=pd.DataFrame()\n",
    "csv_file_names=list()\n",
    "_CREATE_MASTER_MINIBATCHES=True\n",
    "\n",
    "for i in range(m):\n",
    "    print(\"\\n\\nStarting Batch[\",i,\"]...\")\n",
    "    country_df=entire_country_df.iloc[i*_MAXSIZE : (i+1)*_MAXSIZE]\n",
    "    country_df_copy=entire_country_df_copy.iloc[i*_MAXSIZE : (i+1)*_MAXSIZE]\n",
    "    write_df_to_csv(df=country_df[_THRESHOLDS_DICT.keys()], curr_country=curr_country, file_suffix='_country_df.csv', index_flag=True)\n",
    "\n",
    "    if _CREATE_MASTER_MINIBATCHES==False:\n",
    "\n",
    "        # Get the unique set of master-record-ids\n",
    "        master_record_ids = country_df.index.values.astype(list)\n",
    "        # Get the country-master-df\n",
    "        country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, target_dir=_MASTER_DATA_DIRECTORY, write_csv=False)\n",
    "        # Create a dummy set of cross-refs for masters\n",
    "        cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Invoke the Rscript and generate the Raw_score_features csv file for each minibatch\n",
    "        args=f\"{_BINARIES_NAME} {_BINARIES_EXTENSION} {_THRESHOLD_FOR_INDIVIDUAL} {_THRESHOLD_FOR_ADDRESS_COMBINED} {_SCALING_FACTOR} {curr_country} {_RAW_SCORES_DIRECTORY} {_TOTAL_MATCHES_THRESHOLD} {_DEDUP_METHOD} NA NA\"\n",
    "        print(f'\\n{curr_country}_{i} has {country_df.shape[0]} records.\\n\\nInvoking the Rscript now...')\n",
    "        deduplicate_dataset_R( rscript_command=\"C:/Program Files/R/R-3.4.4/bin/i386/Rscript\",  script_name=\"Site_Master_Record_Linkage.R\", args=args )\n",
    "\n",
    "        normalized_duplicates=pd.DataFrame()\n",
    "        # Clean and normalize the score features\n",
    "        normalized_duplicates = clean_score_features(curr_country=curr_country, source_dir=_RAW_SCORES_DIRECTORY, target_dir=_CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "\n",
    "        if normalized_duplicates.shape[0]!=0:\n",
    "            \n",
    "            print(\"\\nFound potential duplicates. Processing their master and cross-reference...\\n\")\n",
    "            # Get the unique set of master-record-ids\n",
    "            master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=country_df)\n",
    "            # Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, target_dir=_MASTER_DATA_DIRECTORY, write_csv=False)\n",
    "            # Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "            # Create full set of cross-refs for country-df\n",
    "            cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, target_dir=_MASTER_DATA_DIRECTORY, write_csv=False)\n",
    "            # Create the csv for the cross-ref report\n",
    "            generate_cross_ref_report(cross_ref_df=cross_ref_df, country_df=country_df_copy, target_dir=_MASTER_DATA_DIRECTORY)\n",
    "\n",
    "\n",
    "            new_file_name=f'_{i}_Master.csv'\n",
    "            write_df_to_csv(df=country_master_df, root_dir=_MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "            new_file_name=curr_country+new_file_name\n",
    "            csv_file_names.append(new_file_name)\n",
    "            entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\n",
    "\n",
    "    del country_df, country_df_copy, master_record_ids\n",
    "    if _CREATE_MASTER_MINIBATCHES:\n",
    "        del normalized_duplicates\n",
    "\n",
    "print(f\"{len(csv_file_names)} csvs generated are: {csv_file_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of levels for the recursive computations\n",
    "d=(m+1)//2\n",
    "\t\n",
    "#for j in range(1,d+1):\n",
    "j=1\n",
    "M_CR_df=pd.DataFrame()\n",
    "n_csvs_to_read=len(csv_file_names)\n",
    "length=n_csvs_to_read if n_csvs_to_read%2==0 else n_csvs_to_read+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Master_Data\\United_States_0_Master.csv has 258 records, and Master_Data\\United_States_1_Master.csv has 252 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "\n",
      "R OUTPUT:\n",
      " [1] \"levenshtein .dll 0.85 0.75 3 United_States Raw_Scores 4 Linkage Master_Data\\\\United_States_0_Master.csv Master_Data\\\\United_States_1_Master.csv\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 120134  3.3     350000  9.4   302970  8.1\n",
      "Vcells 169849  1.3     786432  6.0   697543  5.4\n",
      "[1] \"NRows= 258 , Columns are \"\n",
      " [1] \"SR_NUM\"            \"SOURCE_IDENTIFIER\" \"DATA_SOURCE_NAME\" \n",
      " [4] \"PROTOCOL_NUMBER\"   \"SITE_NUM\"          \"UNIQUE_SITE_ID\"   \n",
      " [7] \"COUNTRY\"           \"SITE_NAME\"         \"STATE\"            \n",
      "[10] \"CITY\"              \"POSTAL_CODE\"       \"SITE_STATUS\"      \n",
      "[13] \"CONCAT_ADDRESS\"   \n",
      "[1] \"NRows= 252 , Columns are \"\n",
      " [1] \"SR_NUM\"            \"SOURCE_IDENTIFIER\" \"DATA_SOURCE_NAME\" \n",
      " [4] \"PROTOCOL_NUMBER\"   \"SITE_NUM\"          \"UNIQUE_SITE_ID\"   \n",
      " [7] \"COUNTRY\"           \"SITE_NAME\"         \"STATE\"            \n",
      "[10] \"CITY\"              \"POSTAL_CODE\"       \"SITE_STATUS\"      \n",
      "[13] \"CONCAT_ADDRESS\"   \n",
      "[1] \"N_combinations= 65016 , Columns are \"\n",
      " [1] \"id1\"               \"id2\"               \"SOURCE_IDENTIFIER\"\n",
      " [4] \"DATA_SOURCE_NAME\"  \"PROTOCOL_NUMBER\"   \"SITE_NUM\"         \n",
      " [7] \"UNIQUE_SITE_ID\"    \"COUNTRY\"           \"SITE_NAME\"        \n",
      "[10] \"STATE\"             \"CITY\"              \"POSTAL_CODE\"      \n",
      "[13] \"SITE_STATUS\"       \"CONCAT_ADDRESS\"    \"is_match\"         \n",
      "[1] \"Scaling up column scores if threshold crossed\"\n",
      "[1] \"SITE_NAME  :  0.85\"\n",
      "[1] \"STATE  :  0.85\"\n",
      "[1] \"CITY  :  0.85\"\n",
      "[1] \"POSTAL_CODE  :  0.85\"\n",
      "[1] \"CONCAT_ADDRESS  :  0.75\"\n",
      "[1] \"Raw_Scores//United_States_Score_Features.csv\"\n",
      "[1] \"Successfully created //Raw_Scores//United_States_Score_Features.csv !\"\n",
      "Time difference of 2.583831 secs\n",
      "\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\United_States_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "510 records get merged into 508\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_d1_0_Master.csv!\n",
      "\n",
      "Master_Data\\United_States_2_Master.csv has 262 records, and Master_Data\\United_States_3_Master.csv has 297 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "\n",
      "R OUTPUT:\n",
      " [1] \"levenshtein .dll 0.85 0.75 3 United_States Raw_Scores 4 Linkage Master_Data\\\\United_States_2_Master.csv Master_Data\\\\United_States_3_Master.csv\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 120134  3.3     350000  9.4   302970  8.1\n",
      "Vcells 169849  1.3     786432  6.0   697543  5.4\n",
      "[1] \"NRows= 262 , Columns are \"\n",
      " [1] \"SR_NUM\"            \"SOURCE_IDENTIFIER\" \"DATA_SOURCE_NAME\" \n",
      " [4] \"PROTOCOL_NUMBER\"   \"SITE_NUM\"          \"UNIQUE_SITE_ID\"   \n",
      " [7] \"COUNTRY\"           \"SITE_NAME\"         \"STATE\"            \n",
      "[10] \"CITY\"              \"POSTAL_CODE\"       \"SITE_STATUS\"      \n",
      "[13] \"CONCAT_ADDRESS\"   \n",
      "[1] \"NRows= 297 , Columns are \"\n",
      " [1] \"SR_NUM\"            \"SOURCE_IDENTIFIER\" \"DATA_SOURCE_NAME\" \n",
      " [4] \"PROTOCOL_NUMBER\"   \"SITE_NUM\"          \"UNIQUE_SITE_ID\"   \n",
      " [7] \"COUNTRY\"           \"SITE_NAME\"         \"STATE\"            \n",
      "[10] \"CITY\"              \"POSTAL_CODE\"       \"SITE_STATUS\"      \n",
      "[13] \"CONCAT_ADDRESS\"   \n",
      "[1] \"N_combinations= 77814 , Columns are \"\n",
      " [1] \"id1\"               \"id2\"               \"SOURCE_IDENTIFIER\"\n",
      " [4] \"DATA_SOURCE_NAME\"  \"PROTOCOL_NUMBER\"   \"SITE_NUM\"         \n",
      " [7] \"UNIQUE_SITE_ID\"    \"COUNTRY\"           \"SITE_NAME\"        \n",
      "[10] \"STATE\"             \"CITY\"              \"POSTAL_CODE\"      \n",
      "[13] \"SITE_STATUS\"       \"CONCAT_ADDRESS\"    \"is_match\"         \n",
      "[1] \"Scaling up column scores if threshold crossed\"\n",
      "[1] \"SITE_NAME  :  0.85\"\n",
      "[1] \"STATE  :  0.85\"\n",
      "[1] \"CITY  :  0.85\"\n",
      "[1] \"POSTAL_CODE  :  0.85\"\n",
      "[1] \"CONCAT_ADDRESS  :  0.75\"\n",
      "[1] \"Raw_Scores//United_States_Score_Features.csv\"\n",
      "[1] \"Successfully created //Raw_Scores//United_States_Score_Features.csv !\"\n",
      "Time difference of 5.271272 secs\n",
      "\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\United_States_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "559 records get merged into 556\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_d1_2_Master.csv!\n",
      "\n",
      "Get the unique set of all record-ids since there isn't a second file to compare.\n",
      "\n",
      "93 records get merged into 93\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_d1_4_Master.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\United_States_d1_Raw_Cross_Ref.csv!\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, length, 2):\n",
    "\tM_df_file1=os.path.join(_MASTER_DATA_DIRECTORY, csv_file_names[i])\n",
    "\tM_df_1=pd.read_csv(M_df_file1, index_col=0)\n",
    "\tif i+1<n_csvs_to_read:\n",
    "\t\tM_df_file2=os.path.join(_MASTER_DATA_DIRECTORY, csv_file_names[i+1])\n",
    "\t\tM_df_2=pd.read_csv(M_df_file2, index_col=0)\n",
    "\t\t# Invoke the Rscript and generate the Raw_score_features csv file\n",
    "\t\tprint(f'\\n{M_df_file1} has {M_df_1.shape[0]} records, and {M_df_file2} has {M_df_2.shape[0]} records.\\n\\nInvoking the Rscript now...\\n')\n",
    "\t\targs=f\"{_BINARIES_NAME} {_BINARIES_EXTENSION} {_THRESHOLD_FOR_INDIVIDUAL} {_THRESHOLD_FOR_ADDRESS_COMBINED} {_SCALING_FACTOR} {curr_country} {_RAW_SCORES_DIRECTORY} {_TOTAL_MATCHES_THRESHOLD} {_LINKAGE_METHOD} {M_df_file1} {M_df_file2}\"\n",
    "\t\tdeduplicate_dataset_R( rscript_command=\"C:/Program Files/R/R-3.4.4/bin/i386/Rscript\",  script_name=\"Site_Master_Record_Linkage.R\", args=args )\n",
    "\t\n",
    "\t\n",
    "\t\tnormalized_duplicates=pd.DataFrame()\n",
    "\t\t# Clean and normalize the score features\n",
    "\t\tnormalized_duplicates = clean_score_features(curr_country=curr_country, source_dir=_RAW_SCORES_DIRECTORY, target_dir=_CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "\t\n",
    "\t\tif normalized_duplicates.shape[0]!=0:\n",
    "\t\t\tprint(\"\\nFound potential duplicates. Processing their master and cross-reference...\\n\")\n",
    "\t\t\t# Get the unique set of master-record-ids\n",
    "\t\t\tmaster_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=M_df_1.append(M_df_2))\n",
    "\t\t\t# Get the country-master-df\n",
    "\t\t\tcountry_master_df = generate_deduplicated_master(country_df=M_df_1.append(M_df_2), master_record_ids=list(master_record_ids), target_dir=_MASTER_DATA_DIRECTORY, write_csv=False)\n",
    "\t\t\t# Create a dummy set of cross-refs for masters\n",
    "\t\t\tcross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "\t\t\t# Create full set of cross-refs for country-df\n",
    "\t\t\tcross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, target_dir=_MASTER_DATA_DIRECTORY, write_csv=False)\n",
    "\t\t\t# Create the csv for the cross-ref report\n",
    "\t\t\tgenerate_cross_ref_report(cross_ref_df=cross_ref_df, country_df=M_df_1.append(M_df_2), target_dir=_MASTER_DATA_DIRECTORY)\n",
    "\t\n",
    "\t\telse:\n",
    "\t\t\t\n",
    "\t\t\tprint(\"\\nGet the unique set of all record-ids since there aren't any potential duplicates.\\n\")\n",
    "\t\t\t# Get the unique set of all-record-ids since there aren't any potential duplicates\n",
    "\t\t\tmaster_record_ids = M_df_1.append(M_df_2).index.values.astype(list)\n",
    "\t\t\t# Get the country-master-df\n",
    "\t\t\tcountry_master_df = generate_deduplicated_master(country_df=M_df_1.append(M_df_2), master_record_ids=master_record_ids, target_dir=_MASTER_DATA_DIRECTORY, write_csv=False)\n",
    "\t\t\t# Create a dummy set of cross-refs for masters\n",
    "\t\t\tcross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "\t\n",
    "\telse:\n",
    "\t\n",
    "\t\tprint(\"\\nGet the unique set of all record-ids since there isn't a second file to compare.\\n\")\n",
    "\t\t# Get the unique set of master-record-ids\n",
    "\t\tmaster_record_ids = M_df_1.index.values.astype(list)\n",
    "\t\t# Get the country-master-df\n",
    "\t\tcountry_master_df = generate_deduplicated_master(country_df=M_df_1, master_record_ids=master_record_ids, target_dir=_MASTER_DATA_DIRECTORY, write_csv=False)\n",
    "\t\t# Create a dummy set of cross-refs for masters\n",
    "\t\tcross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "\t\n",
    "\tM_CR_df=M_CR_df.append(cross_ref_df)\n",
    "\tnew_file_name=f\"_d{j}_{i}_Master.csv\"\n",
    "\twrite_df_to_csv(df=country_master_df, root_dir=_MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "\tnew_file_name=curr_country+new_file_name\n",
    "\tcsv_file_names.append(new_file_name)\n",
    "\tdel master_record_ids, M_df_1\n",
    "\tif i+1<n_csvs_to_read:\n",
    "\t\tdel normalized_duplicates, M_df_2\n",
    "\t\n",
    "write_df_to_csv(df=M_CR_df, root_dir=_MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix=f\"_d{j}_Raw_Cross_Ref.csv\", index_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   SR_NUM_1  SR_NUM_2  SITE_NAME_COMPARISON_SCORE  STATE_COMPARISON_SCORE  \\\n",
       "0     15541     16342                         1.0                     0.0   \n",
       "1     15993     16492                         0.0                     1.0   \n",
       "0     17921     18138                         1.0                     1.0   \n",
       "1     17925     18139                         1.0                     1.0   \n",
       "2     17931     18153                         1.0                     1.0   \n",
       "\n",
       "   CITY_COMPARISON_SCORE  CONCAT_ADDRESS_COMPARISON_SCORE  \\\n",
       "0                    0.0                              3.0   \n",
       "1                    0.0                              3.0   \n",
       "0                    1.0                              0.0   \n",
       "1                    1.0                              0.0   \n",
       "2                    1.0                              0.0   \n",
       "\n",
       "   POSTAL_CODE_COMPARISON_SCORE        COUNTRY  NUM_OF_MATCHES_FOUND  \n",
       "0                           0.0  United_States                   4.0  \n",
       "1                           0.0  United_States                   4.0  \n",
       "0                           1.0  United_States                   4.0  \n",
       "1                           1.0  United_States                   4.0  \n",
       "2                           1.0  United_States                   4.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SR_NUM_1</th>\n      <th>SR_NUM_2</th>\n      <th>SITE_NAME_COMPARISON_SCORE</th>\n      <th>STATE_COMPARISON_SCORE</th>\n      <th>CITY_COMPARISON_SCORE</th>\n      <th>CONCAT_ADDRESS_COMPARISON_SCORE</th>\n      <th>POSTAL_CODE_COMPARISON_SCORE</th>\n      <th>COUNTRY</th>\n      <th>NUM_OF_MATCHES_FOUND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15541</td>\n      <td>16342</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>United_States</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15993</td>\n      <td>16492</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>United_States</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>17921</td>\n      <td>18138</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>United_States</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17925</td>\n      <td>18139</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>United_States</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>17931</td>\n      <td>18153</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>United_States</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "cross_refs_to_update=M_CR_df[M_CR_df['SR_NUM_1']!=M_CR_df['SR_NUM_2']]\n",
    "cross_refs_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     SR_NUM_1  SR_NUM_2  SITE_NAME_COMPARISON_SCORE  STATE_COMPARISON_SCORE  \\\n",
       "42      15541     15541                         1.0                     1.0   \n",
       "179     15993     15993                         1.0                     1.0   \n",
       "124     17921     17921                         1.0                     1.0   \n",
       "126     17925     17925                         1.0                     1.0   \n",
       "130     17931     17931                         1.0                     1.0   \n",
       "\n",
       "     CITY_COMPARISON_SCORE  CONCAT_ADDRESS_COMPARISON_SCORE  \\\n",
       "42                     1.0                              3.0   \n",
       "179                    1.0                              3.0   \n",
       "124                    1.0                              3.0   \n",
       "126                    1.0                              3.0   \n",
       "130                    1.0                              3.0   \n",
       "\n",
       "     POSTAL_CODE_COMPARISON_SCORE        COUNTRY  NUM_OF_MATCHES_FOUND  \n",
       "42                            1.0  United_States                   7.0  \n",
       "179                           1.0  United_States                   7.0  \n",
       "124                           1.0  United_States                   7.0  \n",
       "126                           1.0  United_States                   7.0  \n",
       "130                           1.0  United_States                   7.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SR_NUM_1</th>\n      <th>SR_NUM_2</th>\n      <th>SITE_NAME_COMPARISON_SCORE</th>\n      <th>STATE_COMPARISON_SCORE</th>\n      <th>CITY_COMPARISON_SCORE</th>\n      <th>CONCAT_ADDRESS_COMPARISON_SCORE</th>\n      <th>POSTAL_CODE_COMPARISON_SCORE</th>\n      <th>COUNTRY</th>\n      <th>NUM_OF_MATCHES_FOUND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42</th>\n      <td>15541</td>\n      <td>15541</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>United_States</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>15993</td>\n      <td>15993</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>United_States</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>17921</td>\n      <td>17921</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>United_States</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>17925</td>\n      <td>17925</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>United_States</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>17931</td>\n      <td>17931</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>United_States</td>\n      <td>7.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "entire_country_cross_ref_df[entire_country_cross_ref_df['SR_NUM_1'].isin(cross_refs_to_update['SR_NUM_1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_country_cross_ref_df.set_index('SR_NUM_1', inplace=True)\n",
    "cross_refs_to_update.set_index('SR_NUM_1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_refs_to_update.index.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entire_country_cross_ref_df.update(cross_refs_to_update, join='left', overwrite=True)"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381jvsc74a57bd0df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407",
   "display_name": "Python 3.8.1 32-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. Initialization\n",
    "\n",
    "> a. Read the excel/csv containing the entire query-output\n",
    "\n",
    "> b. Initialize the parameters of thresholds, directories, and fields to concat for generating individual as well as combined match-score\n",
    "\n",
    "> c. Get the unique list of countries in present dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read the Source-file SM_Temp_Shortlist.xlsx\n\nColumns: ['SOURCE_IDENTIFIER' 'DATA_SOURCE_NAME' 'PROTOCOL_NUMBER' 'SITE_NUM'\n 'UNIQUE_SITE_ID' 'COUNTRY' 'SITE_NAME' 'STATE' 'CITY' 'ADDRESS_LINE_1'\n 'ADDRESS_LINE_2' 'ADDRESS_LINE_3' 'POSTAL_CODE' 'SITE_STATUS']\n\n\nCountries : ['Algeria', 'Argentina', 'Australia', 'Turkey', 'United_Kingdom', 'United_States']\n"
     ]
    }
   ],
   "source": [
    "import time, numpy as np, pandas as pd, re, string, subprocess, os\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "_STATIC_FILE_NAME=\"SM_Temp_Shortlist.xlsx\"\n",
    "# _STATIC_QUERY=\"SELECT SITE_INFO_ID SR_NUM, SOURCE_IDENTIFIER, DATA_SOURCE_NAME, PROTOCOL_NUMBER, SITE_NUMBER SITE_NUM, UNIQUE_SITE_ID, COUNTRY,SITE_NAME, SITE_STATESTATE, SITE_CITY CITY, SITE_ADDRESS_LINE_1 ADDRESS_LINE_1, SITE_ADDRESS_LINE_2 ADDRESS_LINE_2, SITE_ADDRESS_LINE_3 ADDRESS_LINE_3, SITE_ZIP_CODE POSTAL_CODE, NULL SITE_STATUS FROM RnD_MART.DM_SITE_INFORMATION WHERE (SITE_STATE+SITE_CITY+SITE_ADDRESS_LINE_1+SITE_ZIP_CODE) IS NOT NULL ORDER BY SITE_ZIP_CODE, SITE_NAME, SITE_STATE, SITE_CITY, (SITE_STATE+SITE_CITY+SITE_ADDRESS_LINE_1+SITE_ZIP_CODE);\"\n",
    "# _SQL_SERVER_CONN=pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};'+'SERVER=takdev-rnd.rda.onetakeda.com;'+'DATABASE=RnD_DB;'+'UID=Rnd_ETL;'+'PWD=rndetl@2017', charset='utf8')\n",
    "\n",
    "\n",
    "_RAW_SCORES_DIRECTORY='Raw_Scores'\n",
    "_CLEANED_SCORES_DIRECTORY='Cleaned_Scores'\n",
    "_MASTER_DATA_DIRECTORY='Master_Data'\n",
    "_STAGING_AREA_DIRECTORY=os.path.join(_MASTER_DATA_DIRECTORY, 'Recursive_Staging_Area')\n",
    "_FIELDS_TO_CONCAT={ 'CONCAT_ADDRESS':   ['ADDRESS_LINE_1','ADDRESS_LINE_2','ADDRESS_LINE_3'] }\n",
    "\n",
    "_COLUMNS_TO_CLEAN=['ADDRESS_LINE_1','ADDRESS_LINE_2','ADDRESS_LINE_3','SITE_NAME','STATE','CITY','POSTAL_CODE']\n",
    "_BINARIES_NAME=\"levenshtein\"\n",
    "#_BINARIES_EXTENSION=\".so\"\n",
    "_BINARIES_EXTENSION=\".dll\"\n",
    "_MAXSIZE=5000\n",
    "\n",
    "_THRESHOLD_FOR_INDIVIDUAL=0.85\n",
    "_THRESHOLD_FOR_ADDRESS_COMBINED=0.75\n",
    "\n",
    "_THRESHOLDS_DICT={\n",
    "    'CONCAT_ADDRESS': _THRESHOLD_FOR_ADDRESS_COMBINED,\n",
    "    'SITE_NAME': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'STATE': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'CITY': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'POSTAL_CODE': _THRESHOLD_FOR_INDIVIDUAL\n",
    "    }\n",
    "_COLS_FOR_TOTAL_MATCH_CALC=[colname+'_COMPARISON_SCORE' for colname in _THRESHOLDS_DICT]\n",
    "\n",
    "_SCALING_FACTOR=3\n",
    "\n",
    "_TOTAL_MATCHES_THRESHOLD=4\n",
    "\n",
    "_RSCRIPT_CMD=\"C:/Program Files/R/R-3.4.4/bin/i386/Rscript\"\n",
    "#_RSCRIPT_CMD=\"Rscript\"\n",
    "_SCRIPT_NAME=\"Site_Master_Record_Linkage.R\"\n",
    "_DEDUP_METHOD='Dedup'\n",
    "_LINKAGE_METHOD='Linkage'\n",
    "\n",
    "def write_df_to_csv(df, root_dir='', curr_country='', file_suffix='_temp.csv', index_flag=False):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Writes the dataframe to a csv file and throw error if it fails.\n",
    "        INPUT:      Dataframe, Target-Directory, Country-name, Suffix-of-csv-file, Index-Flag\n",
    "        OUTPUT:     Dataframe csv at target-directory, or error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        abs_path=os.path.join(root_dir, curr_country+file_suffix)\n",
    "        df.to_csv(abs_path, index=index_flag)\n",
    "        print('\\nSuccessfully created \\{}!'.format(abs_path))\n",
    "    except:\n",
    "        print('\\nSomething went wrong while writing the file. Please check if it is currently in use.')\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Imputes blank cells with '', replaces whitespace with underscore in country-name, and strips whitespace in cells.\n",
    "        INPUT:      Dataframe\n",
    "        OUTPUT:     Imputed and cleaned dataframe.\n",
    "    \"\"\"\n",
    "    df.replace(np.nan, '', inplace=True)\n",
    "    for colname in df.columns.values:\n",
    "        if colname=='COUNTRY':\n",
    "            df[colname]=df[colname].apply(lambda x: x.replace(' ','_'))            \n",
    "        df[colname]=df[colname].astype(str).apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "def clean_dataframe(df, columns_to_clean=_COLUMNS_TO_CLEAN, fields_to_concat=_FIELDS_TO_CONCAT, replace_punctuations=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Replaces special-chars in lowercase-converted cells if replace_punctuation==True, for the columns relevant to computing match-scores.\n",
    "                    Generates the concatenated address fields, and drops the individual ones.\n",
    "                    Overall will be left with alphanumeric chars in UTF-8 encoding.\n",
    "        INPUT:      Dataframe, columns-to-clean, address-fields-to-concat, flag-to-replace-punctuations\n",
    "        OUTPUT:     Imputed and cleaned dataframe.\n",
    "    \"\"\"\n",
    "    copy_df=df.copy(deep=True)\n",
    "    # Added another special character which was causing Italy CSV file read to fail in R\n",
    "    if replace_punctuations:\n",
    "        special_chars=re.escape(string.punctuation)+''\n",
    "        print('\\nSpecial Character that will be replaced are:  {}'.format(special_chars))\n",
    "    for colname in copy_df.columns.values:\n",
    "        if colname in columns_to_clean and replace_punctuations:\n",
    "            copy_df[colname]=copy_df[colname].replace(r'['+special_chars+']', '', regex=True).str.lower()\n",
    "    for colname, cols_to_concat in fields_to_concat.items():\n",
    "        copy_df[colname]=copy_df[cols_to_concat].apply(lambda single_row: ''.join(single_row.values), axis=1)\n",
    "    copy_df.drop(labels=fields_to_concat['CONCAT_ADDRESS'], axis=1, inplace=True)\n",
    "    return copy_df\n",
    "\n",
    "\n",
    "\n",
    "def deduplicate_dataset_R(rscript_command, script_name, args):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Invokes the R-code from Python using 32-bit Rscript 3.4.4 command.\n",
    "                    Uses the Python subprocess module to create a new Pipe.\n",
    "        INPUT:      Abs-path-of-32bit-Rscript-command, Script-to-invoke, Args-for-script\n",
    "        OUTPUT:     Prints R-console output based on return-code. Rscript command generates a csv of the score_features, or errors out.\n",
    "    \"\"\"\n",
    "    cmd = [rscript_command, script_name, args]\n",
    "    pipe = Popen( cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE )\n",
    "    output, error = pipe.communicate()\n",
    "\n",
    "    if pipe.returncode==0:\n",
    "        print('R OUTPUT:\\n')\n",
    "        print(output)\n",
    "    else:\n",
    "        print('R OUTPUT:\\n')\n",
    "        print(output.decode())\n",
    "        print('R ERROR:\\n')\n",
    "        print(error.decode())\n",
    "\n",
    "\n",
    "\n",
    "def scale_up_comparison_score(df, colname='SITE_NAME_COMPARISON_SCORE', scaling_factor=_SCALING_FACTOR):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Scale-up a column's binary-valued score by a factor\n",
    "        INPUT:      Dataframe, score-colname, scaling-factor\n",
    "        OUTPUT:     Scaled up dataframe.\n",
    "    \"\"\"\n",
    "    df[colname]=df[colname].apply(lambda x: x*scaling_factor)\n",
    "\n",
    "\n",
    "\n",
    "def return_top_match(df, child_column, score_key_column):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Input Dataframe has SR_NUM_1 (child-col) matching against multiple SR_NUM_2.\n",
    "                    Orders by child-col asc, score-col desc, and chooses the first possible entry of child-col.\n",
    "        INPUT:      Dataframe-of-score-features-above-a-total-threshold, index-column (SR_NUM_1), total-score-column (NUM_OF_MATCHES_FOUND)\n",
    "        OUTPUT:     Dataframe of normalized-score-features.\n",
    "    \"\"\"\n",
    "    normalized_duplicates=df.sort_values(by=[child_column, score_key_column],ascending=[True,False])\n",
    "    normalized_duplicates=normalized_duplicates.groupby(child_column).head(1).sort_values(by=[child_column])\n",
    "    return normalized_duplicates\n",
    "\n",
    "\n",
    "\n",
    "def replace_cyclic_dependencies_backup(df, child_indicator, master_indicator, verbose=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Input Dataframe has cases like-     Record45 matches with Record44, and Record67 matches with Record45.\n",
    "                    In this case we should maintain-    Record67 matches with Record44.\n",
    "                    Applies a for-loop and replaces values in master-column whenever such a cyclic-occurence observed.\n",
    "        INPUT:      Dataframe-of-score-features-with-cyclic-indexes, child-column, master-column\n",
    "        OUTPUT:     Dataframe of normalized-score-features.\n",
    "    \"\"\"\n",
    "    arr=set(df[child_indicator].array)\n",
    "    for val in df[master_indicator]:\n",
    "        if val in arr:\n",
    "            replace_val=df[df[child_indicator]==val][master_indicator].values[0]\n",
    "            if verbose: print('{} found in normalized_duplicates[{}]. Replacement: {}'.format(val, child_indicator, replace_val))\n",
    "            df[master_indicator].replace(val, replace_val, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def replace_cyclic_dependencies(df, country_df, child_indicator, master_indicator, verbose=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Input Dataframe has cases like-     Record45 matches with Record44, and Record67 matches with Record45.\n",
    "                    In this case we should maintain-    Record67 matches with Record44.\n",
    "                    Applies a for-loop and replaces values in master-column whenever such a cyclic-occurence observed.\n",
    "        INPUT:      Dataframe-of-score-features-with-cyclic-indexes, child-column, master-column\n",
    "        OUTPUT:     Dataframe of normalized-score-features.\n",
    "    \"\"\"\n",
    "    df.sort_values(by=['SR_NUM_2', 'NUM_OF_MATCHES_FOUND'], ascending=[True, False], inplace=True)\n",
    "    arr1=set(df['SR_NUM_1'].array)\n",
    "    arr2=set(df['SR_NUM_2'].array)\n",
    "    for val in arr2:\n",
    "        if val in arr1:\n",
    "            replace_val=df.loc[df['SR_NUM_1']==val,'SR_NUM_2'].values[0]\n",
    "            score_for_original=df.loc[df['SR_NUM_2']==val, 'NUM_OF_MATCHES_FOUND'].values[0]\n",
    "            score_for_replacement=df.loc[df['SR_NUM_1']==val, 'NUM_OF_MATCHES_FOUND'].values[0]\n",
    "            original_sitename=country_df.loc[val, 'SITE_NAME']\n",
    "            replacement_sitename=country_df.loc[replace_val, 'SITE_NAME']\n",
    "            if score_for_original>score_for_replacement and original_sitename!=replacement_sitename:\n",
    "                df.loc[df['SR_NUM_1']==val, 'COUNTRY']='To be deleted'\n",
    "                if verbose: print(\"Delete from df where SR_NUM_1={} and SR_NUM_2={}\".format(val,replace_val))\n",
    "            else:\n",
    "                df['SR_NUM_2'].replace(val, replace_val, inplace=True)\n",
    "                if verbose: print(\"{} [{}] will be replaced with {} [{}]\".format(val,score_for_original,replace_val,score_for_replacement))\n",
    "\n",
    "    indexes_to_delete=df[df['COUNTRY']=='To be deleted'].index\n",
    "    print(\"\\n\\n{} raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.\".format(len(indexes_to_delete)))\n",
    "    df.drop(indexes_to_delete, inplace=True)\n",
    "    df.sort_values(by='SR_NUM_1')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replace_cyclic_dependencies_bkp(df, child_indicator, master_indicator, verbose=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Input Dataframe has cases like-     Record45 matches with Record44, and Record67 matches with Record45.\n",
    "                    In this case we should maintain-    Record67 matches with Record44.\n",
    "                    Applies a for-loop and replaces values in master-column whenever such a cyclic-occurence observed.\n",
    "        INPUT:      Dataframe-of-score-features-with-cyclic-indexes, child-column, master-column\n",
    "        OUTPUT:     Dataframe of normalized-score-features.\n",
    "    \"\"\"\n",
    "    df.sort_values(by=['SR_NUM_2', 'NUM_OF_MATCHES_FOUND'], ascending=[True, False], inplace=True)\n",
    "    arr1=set(df['SR_NUM_1'].array)\n",
    "    arr2=set(df['SR_NUM_2'].array)\n",
    "    for val in arr2:\n",
    "        if val in arr1:\n",
    "            replace_val=df.loc[df['SR_NUM_1']==val,'SR_NUM_2'].values[0]\n",
    "            score_for_original=df.loc[df['SR_NUM_2']==val, 'NUM_OF_MATCHES_FOUND'].values[0]\n",
    "            score_for_replacement=df.loc[df['SR_NUM_1']==val, 'NUM_OF_MATCHES_FOUND'].values[0]\n",
    "            if score_for_original>score_for_replacement:\n",
    "                df.loc[df['SR_NUM_1']==val, 'COUNTRY']='To be deleted'\n",
    "                if verbose: print(\"Delete from df where SR_NUM_1={} and SR_NUM_2={}\".format(val,replace_val))\n",
    "            else:\n",
    "                df['SR_NUM_2'].replace(val, replace_val, inplace=True)\n",
    "                if verbose: print(\"{} [{}] will be replaced with {} [{}]\".format(val,score_for_original,replace_val,score_for_replacement))\n",
    "\n",
    "    indexes_to_delete=df[df['COUNTRY']=='To be deleted'].index\n",
    "    print(\"\\n\\n{} raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.\".format(len(indexes_to_delete)))\n",
    "    df.drop(indexes_to_delete, inplace=True)\n",
    "    df.sort_values(by='SR_NUM_1')\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_score_features(curr_country, country_df, source_dir=_RAW_SCORES_DIRECTORY, target_dir=_CLEANED_SCORES_DIRECTORY, verbose=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Reads the output of the Rscript command that is a csv of score_features having total-score greater than a total-threshold.\n",
    "                    Invokes the top-match function, and the replace-cyclic-occurences function to get a set of clean-score-features.\n",
    "                    Writes the dataframe in the Cleaned-Scores directory.\n",
    "        INPUT:      country-name\n",
    "        OUTPUT:     Dataframe of cleaned-normalized-score-features.\n",
    "    \"\"\"\n",
    "    duplicates=pd.read_csv(os.path.join(source_dir, curr_country+'_Score_Features.csv'))\n",
    "    # if no potential duplicates found, return an empty df\n",
    "    if duplicates.shape[0]==1 and duplicates['SR_NUM_1'][0]==0 and duplicates['SR_NUM_2'][0]==0:\n",
    "        return duplicates.head(0)\n",
    "    \n",
    "    duplicates['COUNTRY']=curr_country\n",
    "    duplicates=return_top_match(df=duplicates, child_column='SR_NUM_1', score_key_column='NUM_OF_MATCHES_FOUND')\n",
    "    duplicates=replace_cyclic_dependencies(df=duplicates, country_df=country_df, child_indicator='SR_NUM_1', master_indicator='SR_NUM_2', verbose=verbose)\n",
    "    write_df_to_csv(df=duplicates, root_dir=target_dir, curr_country=curr_country, file_suffix='_Cleaned_Feature_Scores.csv')\n",
    "    print('\\n\"SR_NUM_2\" will be the master record')\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "def get_deduplicated_master_records(normalized_duplicates, country_df):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  From the list of cleaned-normalized-score-features, use set-theory to find the unique list of masters.\n",
    "                        a.  Think of 'SR_NUM_1' as the list of incoming Primary-keys, and 'SR_NUM_2' as the value to which it should be mapped based on match-score.\n",
    "                        b.  Hence, union of 'SR_NUM_1' & 'SR_NUM_2' will be entire set of duplicates.\n",
    "                        c.  Stand-alone records in the current country_batch_dataframe will not fall in this entire set of duplicates.\n",
    "                        d.  Master-records set wil be the sets of 'SR_NUM_2' & #c above.\n",
    "                        >   Universe                            = {SR_NUM}\n",
    "                        >   a1                                  = {SR_NUM_1}\n",
    "                        >   a2                                  = {SR_NUM_2}\n",
    "                        >   Falls into any duplication-scenario = anymatch  = {a1 U a2}\n",
    "                        >   Falls into no duplication-scenario  = nomatch   = {Universe - anymatch}\n",
    "                        >   Total masters                       = {nomatch U a2}\n",
    "        INPUT:      Dataframe-of-cleaned-normalized-score_features\n",
    "        OUTPUT:     Unique set of master-record-ids (SR_NUM)\n",
    "    \"\"\"\n",
    "    a1=set(normalized_duplicates['SR_NUM_1'].values.tolist())\n",
    "    a2=set(normalized_duplicates['SR_NUM_2'].values.tolist())\n",
    "    country_set=set(country_df.index.values.tolist())\n",
    "    entire_duplicates_set=a1.union(a2)\n",
    "    no_match_set=country_set.difference(entire_duplicates_set)\n",
    "    master_record_ids=no_match_set.union(a2)\n",
    "    return master_record_ids\n",
    "\n",
    "\n",
    "def generate_deduplicated_master(country_df, master_record_ids, target_dir=_MASTER_DATA_DIRECTORY, write_csv=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Use the original df to extract columns-info and generate the country-specific Master file.\n",
    "        INPUT:      Original-country-Dataframe, Unique set of master-record-ids (SR_NUM)\n",
    "        OUTPUT:     Dataframe-for-country-with-original-info, Master-Dataframe\n",
    "    \"\"\"\n",
    "    country_master_df=country_df.loc[master_record_ids]\n",
    "    if write_csv:\n",
    "        write_df_to_csv(df=country_master_df, root_dir=target_dir, curr_country=curr_country, index_flag=True, file_suffix='_Master.csv')\n",
    "    print('{} records get merged into {}'.format(country_df.shape[0],len(master_record_ids)))\n",
    "    return country_master_df\n",
    "\n",
    "\n",
    "\n",
    "def generate_dummy_cross_refs_for_masters(master_record_ids):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Create a dummy cross-reference dataframe for master-records; Record45 matches with Record45 having a total match-score of maximum.\n",
    "        INPUT:      Unique set of master-record-ids (SR_NUM)\n",
    "        OUTPUT:     Dataframe-of-dummy-entries-for-master-cross-references.\n",
    "    \"\"\"\n",
    "    master_record_score_array=[1.0]*len(master_record_ids)\n",
    "    master_record_df_dict={\n",
    "        'SR_NUM_1': list(master_record_ids),\n",
    "        'SR_NUM_2': list(master_record_ids),\n",
    "        'SITE_NAME_COMPARISON_SCORE': master_record_score_array,\n",
    "        'STATE_COMPARISON_SCORE': master_record_score_array,\n",
    "        'CITY_COMPARISON_SCORE': master_record_score_array,\n",
    "        'CONCAT_ADDRESS_COMPARISON_SCORE': master_record_score_array,\n",
    "        'POSTAL_CODE_COMPARISON_SCORE': master_record_score_array }\n",
    "\n",
    "    cross_ref_df=pd.DataFrame(master_record_df_dict)\n",
    "    cross_ref_df['COUNTRY']=curr_country\n",
    "    scale_up_comparison_score(cross_ref_df,'CONCAT_ADDRESS_COMPARISON_SCORE',_SCALING_FACTOR)\n",
    "    cross_ref_df['NUM_OF_MATCHES_FOUND']=cross_ref_df[_COLS_FOR_TOTAL_MATCH_CALC].sum(axis=1)\n",
    "    return cross_ref_df\n",
    "\n",
    "\n",
    "def generate_final_cross_refs(cross_ref_df, normalized_duplicates, target_dir=_MASTER_DATA_DIRECTORY, write_csv=True):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Merges the dummy cross-reference of masters, with the cleaned-normalized-feature-scores.\n",
    "        INPUT:      Dataframe-of-dummy-entries-for-master-cross-references, Dataframe-of-cleaned-normalized-score_features\n",
    "        OUTPUT:     Dataframe-of-cross-references.\n",
    "    \"\"\"\n",
    "    cross_ref_df=cross_ref_df.append(normalized_duplicates)\n",
    "    cross_ref_df.sort_values(by=['SR_NUM_1'], axis=0, inplace=True)\n",
    "    if write_csv:\n",
    "        write_df_to_csv(df=cross_ref_df, root_dir=target_dir, curr_country=curr_country, file_suffix='_Raw_Cross_Ref.csv')\n",
    "    return cross_ref_df\n",
    "\n",
    "\n",
    "\n",
    "def update_entire_country_cross_ref(new_depth_cross_ref_df, entire_country_cross_ref_df):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  --Specific to recursively processing a huge country-batch--\n",
    "                    Updates the entire cross-reference for a country at depth=0, with the merges that are observed for new depth=d.\n",
    "                    The update function performs left-join on the indexes, hence we set-index before the operation, and reset-it later.\n",
    "        INPUT:      Dataframe-of-cross-references-at-new-depth, Dataframe-of-existing-cross-references\n",
    "        OUTPUT:     Dataframe-of-updated-cross-references.\n",
    "    \"\"\"\n",
    "    cross_refs_with_merges=new_depth_cross_ref_df[new_depth_cross_ref_df['SR_NUM_1'] != new_depth_cross_ref_df['SR_NUM_2']]\n",
    "    entire_country_cross_ref_df.set_index('SR_NUM_1', inplace=True)\n",
    "    cross_refs_with_merges.set_index('SR_NUM_1', inplace=True)\n",
    "\n",
    "    entire_country_cross_ref_df.update(cross_refs_with_merges, join='left', overwrite=True)\n",
    "    entire_country_cross_ref_df['SR_NUM_2']=entire_country_cross_ref_df['SR_NUM_2'].apply(pd.to_numeric)\n",
    "    entire_country_cross_ref_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_cross_ref_report(cross_ref_df, country_df, target_dir=_MASTER_DATA_DIRECTORY):\n",
    "    \"\"\"\n",
    "        DOCSTRING:  Creates cross-reference report by performing left-join of cross-reference-dataframe with the original-info in country-df.\n",
    "                        a. Merge the master_cross_reference_df with the country_batch_dataframe as a left-outer-join on Primary-key='SR_NUM_1'\n",
    "                        b. Merge this master_cross_reference_df with the country_batch_dataframe as a left-outer-join on Primary-key='SR_NUM_2'\n",
    "                    Writes the dataframe in the Master-Data directory.\n",
    "        INPUT:      Dataframe-of-cross-references, Dataframe-for-country-with-original-info\n",
    "        OUTPUT:     Dataframe-of-cross-references-with-original-info.\n",
    "    \"\"\"\n",
    "    country_df.reset_index(inplace=True)\n",
    "    country_df_colnames=country_df.columns.values\n",
    "\n",
    "    country_df.columns=[colname+'_1' for colname in country_df_colnames]\n",
    "    cross_ref_df=cross_ref_df.merge(country_df, how='left', on='SR_NUM_1')\n",
    "\n",
    "    country_df.columns=[colname+'_2' for colname in country_df_colnames]\n",
    "    cross_ref_df=cross_ref_df.merge(country_df, how='left', on='SR_NUM_2')\n",
    "\n",
    "    columns_in_report_format=['SR_NUM_1', 'SR_NUM_2', 'SITE_NAME_1','SITE_NAME_2','SITE_NAME_COMPARISON_SCORE','STATE_1','STATE_2','STATE_COMPARISON_SCORE', 'CITY_1', 'CITY_2','CITY_COMPARISON_SCORE','CONCAT_ADDRESS_1','CONCAT_ADDRESS_2','CONCAT_ADDRESS_COMPARISON_SCORE', 'POSTAL_CODE_1','POSTAL_CODE_2',   'POSTAL_CODE_COMPARISON_SCORE','NUM_OF_MATCHES_FOUND']\n",
    "    cross_ref_df=cross_ref_df[columns_in_report_format]\n",
    "    write_df_to_csv(df=cross_ref_df, root_dir=target_dir, curr_country=curr_country, file_suffix='_Cross_Ref_Full_Report.csv')\n",
    "\n",
    "\n",
    "#site_master_df=pd.read_csv(_STATIC_FILE_NAME, index_col=0)\n",
    "site_master_df=pd.read_excel(_STATIC_FILE_NAME, index_col=0)\n",
    "print(\"Read the Source-file {}\".format(_STATIC_FILE_NAME))\n",
    "#site_master_df=pd.read_sql_query(_STATIC_QUERY, _SQL_SERVER_CONN)\n",
    "#site_master_df.set_index('SR_NUM', inplace=True)\n",
    "preprocess_dataframe(site_master_df)\n",
    "print('\\nColumns: {}\\n'.format(site_master_df.columns.values))\n",
    "countries=list(site_master_df['COUNTRY'].unique())\n",
    "print('\\nCountries : {}'.format(countries))"
   ]
  },
  {
   "source": [
    "# TESTING OUT THE RECURSIVE APPROACH WITH MINI-BATCHES OF MASTERS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Special Character that will be replaced are:  !\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n",
      "\n",
      "\n",
      "Starting Batch[0]...\n",
      "\n",
      "Successfully created \\Australia_country_df.csv!\n",
      "\n",
      "Australia_0 has 576 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "R OUTPUT:\n",
      "\n",
      "b'[1] \"levenshtein .dll 0.85 0.75 3 Australia Raw_Scores 4 Dedup NA NA\"\\r\\n[1] \"Loading levenshtein.dll !\"\\r\\n         used (Mb) gc trigger (Mb) max used (Mb)\\r\\nNcells 120133  3.3     350000  9.4   302969  8.1\\r\\nVcells 169834  1.3     786432  6.0   697518  5.4\\r\\n[1] \"NRows= 576 , Candidate-pairs= 165600 , Columns are \"\\r\\n[1] \"SR_NUM\"         \"CONCAT_ADDRESS\" \"SITE_NAME\"      \"STATE\"         \\r\\n[5] \"CITY\"           \"POSTAL_CODE\"   \\r\\n[1] \"N_combinations= 165600 , Columns are \"\\r\\n[1] \"id1\"            \"id2\"            \"CONCAT_ADDRESS\" \"SITE_NAME\"     \\r\\n[5] \"STATE\"          \"CITY\"           \"POSTAL_CODE\"    \"is_match\"      \\r\\n[1] \"Scaling up column scores if threshold crossed\"\\r\\n[1] \"SITE_NAME  :  0.85\"\\r\\n[1] \"STATE  :  0.85\"\\r\\n[1] \"CITY  :  0.85\"\\r\\n[1] \"POSTAL_CODE  :  0.85\"\\r\\n[1] \"CONCAT_ADDRESS  :  0.75\"\\r\\n[1] \"Raw_Scores//Australia_Score_Features.csv\"\\r\\n[1] \"Successfully created //Raw_Scores//Australia_Score_Features.csv !\"\\r\\n\\r\\nTime difference of 5.180448 secs\\r\\n'\n",
      "\n",
      "\n",
      "16 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\Australia_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "576 records get merged into 178\n",
      "\n",
      "Successfully created \\Master_Data\\Recursive_Staging_Area\\Australia_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\Recursive_Staging_Area\\Australia_0_Master.csv!\n",
      "1 csvs generated are: ['Australia_0_Master.csv']\n"
     ]
    }
   ],
   "source": [
    "# for c in range(len(countries)):\n",
    "c=2\n",
    "curr_country=countries[c]\n",
    "entire_country_df=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "entire_country_df=clean_dataframe(entire_country_df, columns_to_clean=_COLUMNS_TO_CLEAN, fields_to_concat=_FIELDS_TO_CONCAT, replace_punctuations=True)\n",
    "entire_country_df_copy=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "entire_country_df_copy=clean_dataframe(entire_country_df_copy, columns_to_clean=_COLUMNS_TO_CLEAN, fields_to_concat=_FIELDS_TO_CONCAT, replace_punctuations=False)\n",
    "nrows=entire_country_df.shape[0]\n",
    "m=(nrows//_MAXSIZE)+1\n",
    "entire_country_cross_ref_df=pd.DataFrame()\n",
    "queue_of_csvs=list()\n",
    "_CREATE_MASTER_MINIBATCHES=True\n",
    "\n",
    "for i in range(m):\n",
    "    print(\"\\n\\nStarting Batch[{}]...\".format(i))\n",
    "    country_df=entire_country_df.iloc[i*_MAXSIZE : (i+1)*_MAXSIZE]\n",
    "    country_df_copy=entire_country_df_copy.iloc[i*_MAXSIZE : (i+1)*_MAXSIZE]\n",
    "    write_df_to_csv(df=country_df[_THRESHOLDS_DICT.keys()], curr_country=curr_country, file_suffix='_country_df.csv', index_flag=True)\n",
    "\n",
    "    if _CREATE_MASTER_MINIBATCHES==False or country_df.shape[0]==1:\n",
    "\n",
    "        print(\"\\n\\nGet the unique set of all record-ids, since Layer-zero cannot create mastered mini-batches.\\n\")\n",
    "        # Get the unique set of master-record-ids\n",
    "        master_record_ids = country_df.index.values.astype(list)\n",
    "        # Create the country-master-df\n",
    "        country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, target_dir=_STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "        # Create a dummy set of cross-refs for masters\n",
    "        cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "\n",
    "        new_file_name='_{}_Master.csv'.format(i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=_STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Invoke the Rscript and generate the Raw_score_features csv file for each minibatch\n",
    "        args=\"{} {} {} {} {} {} {} {} {} NA NA\".format(_BINARIES_NAME, _BINARIES_EXTENSION, _THRESHOLD_FOR_INDIVIDUAL, _THRESHOLD_FOR_ADDRESS_COMBINED, _SCALING_FACTOR, curr_country, _RAW_SCORES_DIRECTORY, _TOTAL_MATCHES_THRESHOLD, _DEDUP_METHOD)\n",
    "        print('\\n{}_{} has {} records.\\n\\nInvoking the Rscript now...'.format(curr_country, i, country_df.shape[0]))\n",
    "        deduplicate_dataset_R( rscript_command=_RSCRIPT_CMD,  script_name=_SCRIPT_NAME, args=args )\n",
    "\n",
    "        normalized_duplicates=pd.DataFrame()\n",
    "        # Clean and normalize the score features\n",
    "        normalized_duplicates = clean_score_features(curr_country=curr_country, country_df=country_df, source_dir=_RAW_SCORES_DIRECTORY, target_dir=_CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "\n",
    "        if normalized_duplicates.shape[0]!=0:\n",
    "            \n",
    "            print(\"\\n\\nFound potential duplicates. Processing their master and cross-reference...\\n\")\n",
    "            # Get the unique set of master-record-ids\n",
    "            master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=country_df)\n",
    "            # Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, target_dir=_STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "            # Create full set of cross-refs for country-df\n",
    "            cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, target_dir=_STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create the csv for the cross-ref report\n",
    "            generate_cross_ref_report(cross_ref_df=cross_ref_df, country_df=country_df_copy, target_dir=_STAGING_AREA_DIRECTORY)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"\\n\\nGet the unique set of all record-ids since there aren't any potential duplicates.\\n\")\n",
    "\t\t\t# Get the unique set of all-record-ids since there aren't any potential duplicates\n",
    "            master_record_ids = country_df.index.values.astype(list)\n",
    "\t\t\t# Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, target_dir=_STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "\t\t\t# Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "                \n",
    "        new_file_name='_{}_Master.csv'.format(i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=_STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\n",
    "\n",
    "        del country_df, country_df_copy, master_record_ids\n",
    "        if _CREATE_MASTER_MINIBATCHES:\n",
    "            del normalized_duplicates\n",
    "\n",
    "print(\"{} csvs generated are: {}\".format(len(queue_of_csvs), queue_of_csvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nMax-depth for Australia will be 1\n1 csvs need to be processed: ['Australia_0_Master.csv'] , length=2\n\n\nGet the unique set of all record-ids since there isn't a second file to compare.\n\n178 records get merged into 178\n\nSuccessfully created \\Master_Data\\Recursive_Staging_Area\\Australia_d1_0_Master.csv!\n\nSuccessfully created \\Master_Data\\Recursive_Staging_Area\\Australia_d1_Raw_Cross_Ref.csv!\n\n\nDepth[1] processed successfully.\n\n\n\n\nProcessed all 1 levels. Generating the master and cross-reference at the final-layer...\n\nSuccessfully created \\Master_Data\\Australia_Master.csv!\n576 records get merged into 178\n\nSuccessfully created \\Master_Data\\Australia_Raw_Cross_Ref.csv!\n\nSuccessfully created \\Master_Data\\Australia_Cross_Ref_Full_Report.csv!\n"
     ]
    }
   ],
   "source": [
    "# Number of levels for the recursive computations\n",
    "d=(m+1)//2\n",
    "print(\"\\nMax-depth for {} will be {}\".format(curr_country,d))\n",
    "\n",
    "for j in range(1,d+1):\n",
    "    M_CR_df=pd.DataFrame()\n",
    "    n_csvs_to_read=len(queue_of_csvs)\n",
    "    length=n_csvs_to_read if n_csvs_to_read%2==0 else n_csvs_to_read+1\n",
    "    print(\"{} csvs need to be processed: {} , length={}\".format(n_csvs_to_read, queue_of_csvs, length))\n",
    "    for i in range(0, length, 2):\n",
    "        M_df_file1=os.path.join(_STAGING_AREA_DIRECTORY, queue_of_csvs[i])\n",
    "        M_df_1=pd.read_csv(M_df_file1, index_col=0)\n",
    "        \n",
    "        if i+1<n_csvs_to_read:\n",
    "            M_df_file2=os.path.join(_STAGING_AREA_DIRECTORY, queue_of_csvs[i+1])\n",
    "            M_df_2=pd.read_csv(M_df_file2, index_col=0)\n",
    "            \n",
    "\t\t\t# Invoke the Rscript and generate the Raw_score_features csv file\n",
    "            print('\\n{} has {} records, and {} has {} records.\\n\\nInvoking the Rscript now...\\n'.format(M_df_file1, M_df_1.shape[0],M_df_file2, M_df_2.shape[0]))\n",
    "            args=\"{} {} {} {} {} {} {} {} {} {} {}\".format(_BINARIES_NAME, _BINARIES_EXTENSION, _THRESHOLD_FOR_INDIVIDUAL, _THRESHOLD_FOR_ADDRESS_COMBINED, _SCALING_FACTOR, curr_country, _RAW_SCORES_DIRECTORY, _TOTAL_MATCHES_THRESHOLD, _LINKAGE_METHOD, M_df_file1, M_df_file2)\n",
    "            deduplicate_dataset_R( rscript_command=_RSCRIPT_CMD,  script_name=_SCRIPT_NAME, args=args )\n",
    "            \n",
    "            \n",
    "            normalized_duplicates=pd.DataFrame()\n",
    "            # Clean and normalize the score features\n",
    "            normalized_duplicates = clean_score_features(curr_country=curr_country, country_df=M_df_1.append(M_df_2), source_dir=_RAW_SCORES_DIRECTORY, target_dir=_CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "            \n",
    "            if normalized_duplicates.shape[0]!=0:\n",
    "                \n",
    "                print(\"\\n\\nFound potential duplicates. Processing their master and cross-reference...\\n\")\n",
    "                # Get the unique set of master-record-ids\n",
    "                master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=M_df_1.append(M_df_2))\n",
    "                # Get the country-master-df\n",
    "                country_master_df = generate_deduplicated_master(country_df=M_df_1.append(M_df_2), master_record_ids=list(master_record_ids), target_dir=_STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create a dummy set of cross-refs for masters\n",
    "                cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "                # Create full set of cross-refs for country-df\n",
    "                cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, target_dir=_STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create the csv for the cross-ref report\n",
    "                generate_cross_ref_report(cross_ref_df=cross_ref_df, country_df=M_df_1.append(M_df_2), target_dir=_STAGING_AREA_DIRECTORY)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                print(\"\\n\\nGet the unique set of all record-ids since there aren't any potential duplicates.\\n\")\n",
    "                # Get the unique set of all-record-ids since there aren't any potential duplicates\n",
    "                master_record_ids = M_df_1.append(M_df_2).index.values.astype(list)\n",
    "                # Get the country-master-df\n",
    "                country_master_df = generate_deduplicated_master(country_df=M_df_1.append(M_df_2), master_record_ids=master_record_ids, target_dir=_STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create a dummy set of cross-refs for masters\n",
    "                cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            print(\"\\n\\nGet the unique set of all record-ids since there isn't a second file to compare.\\n\")\n",
    "            # Get the unique set of master-record-ids\n",
    "            master_record_ids = M_df_1.index.values.astype(list)\n",
    "            # Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=M_df_1, master_record_ids=master_record_ids, target_dir=_STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids)\n",
    "            \n",
    "            \n",
    "        M_CR_df=M_CR_df.append(cross_ref_df)\n",
    "        new_file_name=\"_d{}_{}_Master.csv\".format(j,i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=_STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        del master_record_ids, M_df_1\n",
    "        if i+1<n_csvs_to_read:\n",
    "            del normalized_duplicates, M_df_2\n",
    "    \n",
    "    write_df_to_csv(df=M_CR_df, root_dir=_STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=\"_d{}_Raw_Cross_Ref.csv\".format(j), index_flag=False)\n",
    "    print(\"\\n\\nDepth[{}] processed successfully.\".format(j))\n",
    "    update_entire_country_cross_ref(new_depth_cross_ref_df=M_CR_df, entire_country_cross_ref_df=entire_country_cross_ref_df)\n",
    "    queue_of_csvs=queue_of_csvs[n_csvs_to_read:]\n",
    "\n",
    "\n",
    "\n",
    "if len(queue_of_csvs)==1:\n",
    "    print(\"\\n\\n\\n\\nProcessed all {} levels. Generating the master and cross-reference at the final-layer...\".format(d))\n",
    "    M_df_file1=os.path.join(_STAGING_AREA_DIRECTORY, queue_of_csvs[i])\n",
    "    M_df_1=pd.read_csv(M_df_file1, index_col=0)\n",
    "    # Get the unique set of master-record-ids\n",
    "    master_record_ids = M_df_1.index.values.astype(list)\n",
    "    # Get the country-master-df\n",
    "    country_master_df = generate_deduplicated_master(country_df=entire_country_df_copy, master_record_ids=master_record_ids, target_dir=_MASTER_DATA_DIRECTORY, write_csv=True)\n",
    "\t# Write the final raw-cross-ref to a csv\n",
    "    write_df_to_csv(df=entire_country_cross_ref_df, root_dir=_MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix=\"_Raw_Cross_Ref.csv\", index_flag=False)\n",
    "    # Create the csv for the cross-ref report\n",
    "    generate_cross_ref_report(cross_ref_df=entire_country_cross_ref_df, country_df=entire_country_df_copy, target_dir=_MASTER_DATA_DIRECTORY)"
   ]
  }
 ]
}
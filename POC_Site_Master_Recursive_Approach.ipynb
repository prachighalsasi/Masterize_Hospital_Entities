{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381jvsc74a57bd0df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407",
   "display_name": "Python 3.8.1 32-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Successfully created \\hospital_account_info.csv!\n",
      "\n",
      "Standardized the input data columns, and sorted them to ensure better compression-statistics!\n",
      "hospital_account_info.csv is now ready to be processed by the algorithm.\n",
      "\n",
      "Formatted the hospital_account_info_raw.csv file into hospital_account_info.csv using PySpark successfully.\n",
      "\n",
      "Read the Source-file hospital_account_info.csv\n",
      "\n",
      "Columns: ['COUNTRY' 'POSTAL_CODE' 'SITE_NAME' 'STATE' 'CITY' 'ADDRESS_LINE_1'\n",
      " 'ADDRESS_LINE_2' 'ADDRESS_LINE_3' 'COUNTY_NAME' 'PHONE_NUM' 'SITE_TYPE'\n",
      " 'SITE_OWNERSHIP' 'ACCOUNT_NUM']\n",
      "\n",
      "\n",
      "Countries : ['USA']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, config as conf\n",
    "from util_functions import *\n",
    "#import time, numpy as np, re, string, subprocess, os\n",
    "#from subprocess import Popen, PIPE\n",
    "\n",
    "preformat_input_using_sparksql()\n",
    "print('\\nFormatted the {} file into {} using PySpark successfully.'.format(conf._RAW_STATIC_FILE_NAME, conf._STATIC_FILE_NAME))\n",
    "\n",
    "site_master_df=pd.read_csv(conf._STATIC_FILE_NAME, index_col=0)\n",
    "print('\\nRead the Source-file {}'.format(conf._STATIC_FILE_NAME))\n",
    "\n",
    "site_master_df=preprocess_dataframe(df=site_master_df)\n",
    "print('\\nColumns: {}\\n'.format(site_master_df.columns.values))\n",
    "\n",
    "countries=list(site_master_df['COUNTRY'].unique())\n",
    "print('\\nCountries : {}'.format(countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "COUNTRY\n",
       "USA    3057\n",
       "Name: COUNTRY, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "site_master_df.groupby(by='COUNTRY')['COUNTRY'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Special Character that will be replaced are:  !\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n",
      "\n",
      "There will be 2 batches since incoming dataset-size=3057 and minibatch-size=2000\n",
      "\n",
      "\n",
      "Starting Batch[0]...\n",
      "\n",
      "Successfully created \\USA_country_df.csv!\n",
      "\n",
      "USA_0 has 2000 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "R OUTPUT:\n",
      "\n",
      "[1] \"levenshtein .dll 0.95 0.95 3 USA Raw_Scores 4 Dedup NA NA\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "[1] 4000\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 120138  3.3     350000  9.4   303049  8.1\n",
      "Vcells 169828  1.3     786432  6.0   698074  5.4\n",
      "[1] \"NRows= 2000 , Candidate-pairs= 1999000 , Columns are \"\n",
      "[1] \"SR_NUM\"         \"CONCAT_ADDRESS\" \"SITE_NAME\"      \"STATE\"         \n",
      "[5] \"CITY\"           \"POSTAL_CODE\"   \n",
      "[1] \"N_combinations= 1999000 , Columns are \"\n",
      "[1] \"id1\"            \"id2\"            \"CONCAT_ADDRESS\" \"SITE_NAME\"     \n",
      "[5] \"STATE\"          \"CITY\"           \"POSTAL_CODE\"    \"is_match\"      \n",
      "[1] \"Scaling up column scores if threshold crossed\"\n",
      "[1] \"SITE_NAME  :  0.95\"\n",
      "[1] \"STATE  :  0.95\"\n",
      "[1] \"CITY  :  0.95\"\n",
      "[1] \"POSTAL_CODE  :  0.95\"\n",
      "[1] \"CONCAT_ADDRESS  :  0.95\"\n",
      "[1] \"Raw_Scores//USA_Score_Features.csv\"\n",
      "[1] \"Successfully created //Raw_Scores//USA_Score_Features.csv !\"\n",
      "\n",
      "Time difference of 38.83503 secs\n",
      "\n",
      "\n",
      "\n",
      "0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\USA_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "2000 records get merged into 1998\n",
      "\n",
      "Successfully created \\Master_Data\\Recursive_Staging_Area\\USA_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\Recursive_Staging_Area\\USA_0_Master.csv!\n",
      "\n",
      "\n",
      "Starting Batch[1]...\n",
      "\n",
      "Successfully created \\USA_country_df.csv!\n",
      "\n",
      "USA_1 has 1057 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "R OUTPUT:\n",
      "\n",
      "[1] \"levenshtein .dll 0.95 0.95 3 USA Raw_Scores 4 Dedup NA NA\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "[1] 4000\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 120138  3.3     350000  9.4   303049  8.1\n",
      "Vcells 169828  1.3     786432  6.0   698074  5.4\n",
      "[1] \"NRows= 1057 , Candidate-pairs= 558096 , Columns are \"\n",
      "[1] \"SR_NUM\"         \"CONCAT_ADDRESS\" \"SITE_NAME\"      \"STATE\"         \n",
      "[5] \"CITY\"           \"POSTAL_CODE\"   \n",
      "[1] \"N_combinations= 558096 , Columns are \"\n",
      "[1] \"id1\"            \"id2\"            \"CONCAT_ADDRESS\" \"SITE_NAME\"     \n",
      "[5] \"STATE\"          \"CITY\"           \"POSTAL_CODE\"    \"is_match\"      \n",
      "[1] \"Scaling up column scores if threshold crossed\"\n",
      "[1] \"SITE_NAME  :  0.95\"\n",
      "[1] \"STATE  :  0.95\"\n",
      "[1] \"CITY  :  0.95\"\n",
      "[1] \"POSTAL_CODE  :  0.95\"\n",
      "[1] \"CONCAT_ADDRESS  :  0.95\"\n",
      "[1] \"Raw_Scores//USA_Score_Features.csv\"\n",
      "[1] \"Successfully created //Raw_Scores//USA_Score_Features.csv !\"\n",
      "\n",
      "Time difference of 10.01762 secs\n",
      "\n",
      "\n",
      "\n",
      "0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\USA_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "1057 records get merged into 1054\n",
      "\n",
      "Successfully created \\Master_Data\\Recursive_Staging_Area\\USA_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\Recursive_Staging_Area\\USA_1_Master.csv!\n",
      "2 csvs generated are: ['USA_0_Master.csv', 'USA_1_Master.csv']\n"
     ]
    }
   ],
   "source": [
    "# for c in range(len(countries)):\n",
    "c=0\n",
    "curr_country=countries[c]\n",
    "entire_country_df=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "entire_country_df=clean_dataframe(entire_country_df, columns_to_clean=conf._COLUMNS_TO_CLEAN, fields_to_concat=conf._FIELDS_TO_CONCAT, replace_punctuations=True)\n",
    "entire_country_df_copy=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "entire_country_df_copy=clean_dataframe(entire_country_df_copy, columns_to_clean=conf._COLUMNS_TO_CLEAN, fields_to_concat=conf._FIELDS_TO_CONCAT, replace_punctuations=False)\n",
    "nrows=entire_country_df.shape[0]\n",
    "m=int(np.ceil(np.divide(nrows, conf._MAXSIZE)))\n",
    "print('\\nThere will be {} batches since incoming dataset-size={} and minibatch-size={}'.format(m, entire_country_df.shape[0], conf._MAXSIZE))\n",
    "entire_country_cross_ref_df=pd.DataFrame()\n",
    "queue_of_csvs=list()\n",
    "\n",
    "for i in range(m):\n",
    "    print('\\n\\nStarting Batch[{}]...'.format(i))\n",
    "    country_df=entire_country_df.iloc[i*conf._MAXSIZE : (i+1)*conf._MAXSIZE]\n",
    "    country_df_copy=entire_country_df_copy.iloc[i*conf._MAXSIZE : (i+1)*conf._MAXSIZE]\n",
    "    write_df_to_csv(df=country_df[conf._THRESHOLDS_DICT.keys()], curr_country=curr_country, file_suffix='_country_df.csv', index_flag=True)\n",
    "    _CREATE_MASTER_MINIBATCHES=(country_df.shape[0]>1)\n",
    "\t\n",
    "    if not _CREATE_MASTER_MINIBATCHES:\n",
    "\n",
    "        print('\\n\\nGet the unique set of all record-ids, since Layer-zero cannot create mastered mini-batches.\\n')\n",
    "        # Get the unique set of master-record-ids\n",
    "        master_record_ids = country_df.index.values.astype(list)\n",
    "        # Create the country-master-df\n",
    "        country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "        # Create a dummy set of cross-refs for masters\n",
    "        cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "\n",
    "        # Write the current master dataset to a csv, add the filename to the queue of csvs, and append currently generated cross-ref to existing cross-ref\n",
    "        new_file_name='_{}_Master.csv'.format(i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\n",
    "\n",
    "\n",
    "    elif _CREATE_MASTER_MINIBATCHES:\n",
    "\n",
    "        # Invoke the Rscript and generate the Raw_score_features csv file for each minibatch\n",
    "        args='{} {} {} {} {} {} {} {} {} NA NA'.format(conf._BINARIES_NAME, conf._BINARIES_EXTENSION, conf._THRESHOLD_FOR_INDIVIDUAL, conf._THRESHOLD_FOR_ADDRESS_COMBINED, conf._SCALING_FACTOR, curr_country, conf._RAW_SCORES_DIRECTORY, conf._TOTAL_MATCHES_THRESHOLD, conf._DEDUP_METHOD)\n",
    "        print('\\n{}_{} has {} records.\\n\\nInvoking the Rscript now...'.format(curr_country, i, country_df.shape[0]))\n",
    "        deduplicate_dataset_R( rscript_command=conf._RSCRIPT_CMD,  script_name=conf._SCRIPT_NAME, args=args )\n",
    "\n",
    "        normalized_duplicates=pd.DataFrame()\n",
    "        # Clean and normalize the score features\n",
    "        normalized_duplicates = clean_score_features(curr_country=curr_country, country_df=country_df, source_dir=conf._RAW_SCORES_DIRECTORY, target_dir=conf._CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "\n",
    "        if normalized_duplicates.shape[0]!=0:\n",
    "            \n",
    "            print('\\n\\nFound potential duplicates. Processing their master and cross-reference...\\n')\n",
    "            # Get the unique set of master-record-ids\n",
    "            master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=country_df)\n",
    "            # Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "            # Create full set of cross-refs for country-df\n",
    "            cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create the csv for the cross-ref report\n",
    "            generate_cross_ref_report(cross_ref_df=cross_ref_df, curr_country=curr_country, country_df=country_df_copy, target_dir=conf._STAGING_AREA_DIRECTORY)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('\\n\\nGet the unique set of all record-ids since there aren\\'t any potential duplicates.\\n')\n",
    "\t\t\t# Get the unique set of all-record-ids since there aren't any potential duplicates\n",
    "            master_record_ids = country_df.index.values.astype(list)\n",
    "\t\t\t# Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=country_df, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "\t\t\t# Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "                \n",
    "        # Write the current master dataset to a csv, add the filename to the queue of csvs, and append currently generated cross-ref to existing cross-ref\n",
    "        new_file_name='_{}_Master.csv'.format(i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        entire_country_cross_ref_df = entire_country_cross_ref_df.append(cross_ref_df)\n",
    "\n",
    "        del country_df, country_df_copy, master_record_ids\n",
    "        if _CREATE_MASTER_MINIBATCHES:\n",
    "            del normalized_duplicates\n",
    "\n",
    "print('{} csvs generated are: {}'.format(len(queue_of_csvs), queue_of_csvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Max-depth for USA will be 1\n",
      "2 csvs need to be processed: ['USA_0_Master.csv', 'USA_1_Master.csv'] , length=2\n",
      "\n",
      "Master_Data\\Recursive_Staging_Area\\USA_0_Master.csv has 1998 records, and Master_Data\\Recursive_Staging_Area\\USA_1_Master.csv has 1054 records.\n",
      "\n",
      "Invoking the Rscript now...\n",
      "\n",
      "R OUTPUT:\n",
      "\n",
      "[1] \"levenshtein .dll 0.95 0.95 3 USA Raw_Scores 4 Linkage Master_Data\\\\Recursive_Staging_Area\\\\USA_0_Master.csv Master_Data\\\\Recursive_Staging_Area\\\\USA_1_Master.csv\"\n",
      "[1] \"Loading levenshtein.dll !\"\n",
      "[1] 4000\n",
      "         used (Mb) gc trigger (Mb) max used (Mb)\n",
      "Ncells 120139  3.3     350000  9.4   303050  8.1\n",
      "Vcells 169843  1.3     786432  6.0   698101  5.4\n",
      "[1] \"NRows= 1998 , Columns are \"\n",
      " [1] \"SR_NUM\"         \"COUNTRY\"        \"POSTAL_CODE\"    \"SITE_NAME\"     \n",
      " [5] \"STATE\"          \"CITY\"           \"COUNTY_NAME\"    \"PHONE_NUM\"     \n",
      " [9] \"SITE_TYPE\"      \"SITE_OWNERSHIP\" \"ACCOUNT_NUM\"    \"CONCAT_ADDRESS\"\n",
      "[1] \"NRows= 1054 , Columns are \"\n",
      " [1] \"SR_NUM\"         \"COUNTRY\"        \"POSTAL_CODE\"    \"SITE_NAME\"     \n",
      " [5] \"STATE\"          \"CITY\"           \"COUNTY_NAME\"    \"PHONE_NUM\"     \n",
      " [9] \"SITE_TYPE\"      \"SITE_OWNERSHIP\" \"ACCOUNT_NUM\"    \"CONCAT_ADDRESS\"\n",
      "[1] \"Please check the size of incoming dataframe. We've seen issues with batch-size > 4000, since n(computations) will be ncols*[n(n-1)/2] !\"\n",
      "<simpleError: memory exhausted (limit reached?)>\n",
      "\n",
      "R ERROR:\n",
      "\n",
      "Loading required package: tools\n",
      "Error in if (nrow(candidate_pairs) > 0) { : argument is of length zero\n",
      "Execution halted\n",
      "\n",
      "\n",
      "\n",
      "0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.\n",
      "\n",
      "Successfully created \\Cleaned_Scores\\USA_Cleaned_Feature_Scores.csv!\n",
      "\n",
      "\"SR_NUM_2\" will be the master record\n",
      "\n",
      "\n",
      "Found potential duplicates. Processing their master and cross-reference...\n",
      "\n",
      "3052 records get merged into 3052\n",
      "\n",
      "Successfully created \\Master_Data\\Recursive_Staging_Area\\USA_Cross_Ref_Full_Report.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\Recursive_Staging_Area\\USA_d1_0_Master.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\Recursive_Staging_Area\\USA_d1_Raw_Cross_Ref.csv!\n",
      "\n",
      "\n",
      "Depth[1] processed successfully.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processed all 1 levels. Generating the master and cross-reference at the final-layer...\n",
      "\n",
      "Successfully created \\Master_Data\\USA_Master.csv!\n",
      "3057 records get merged into 3052\n",
      "\n",
      "Successfully created \\Master_Data\\USA_Raw_Cross_Ref.csv!\n",
      "\n",
      "Successfully created \\Master_Data\\USA_Cross_Ref_Full_Report.csv!\n"
     ]
    }
   ],
   "source": [
    "# Number of levels for the recursive computations\n",
    "d=(m+1)//2\n",
    "print('\\nMax-depth for {} will be {}'.format(curr_country, d))\n",
    "\n",
    "\n",
    "for j in range(1,d+1):\n",
    "    combined_crossref_at_depth=pd.DataFrame()\n",
    "    n_csvs_to_read=len(queue_of_csvs)\n",
    "    length=n_csvs_to_read if n_csvs_to_read%2==0 else n_csvs_to_read+1\n",
    "    print('{} csvs need to be processed: {} , length={}'.format(n_csvs_to_read, queue_of_csvs, length))\n",
    "    for i in range(0, length, 2):\n",
    "        master_csv_1=os.path.join(conf._STAGING_AREA_DIRECTORY, queue_of_csvs[i])\n",
    "        master_df_1=pd.read_csv(master_csv_1, index_col=0)\n",
    "        \n",
    "        if i+1<n_csvs_to_read:\n",
    "            master_csv_2=os.path.join(conf._STAGING_AREA_DIRECTORY, queue_of_csvs[i+1])\n",
    "            master_df_2=pd.read_csv(master_csv_2, index_col=0)\n",
    "            \n",
    "\t\t\t# Invoke the Rscript and generate the Raw_score_features csv file\n",
    "            print('\\n{} has {} records, and {} has {} records.\\n\\nInvoking the Rscript now...\\n'.format(master_csv_1, master_df_1.shape[0],master_csv_2, master_df_2.shape[0]))\n",
    "            args='{} {} {} {} {} {} {} {} {} {} {}'.format(conf._BINARIES_NAME, conf._BINARIES_EXTENSION, conf._THRESHOLD_FOR_INDIVIDUAL, conf._THRESHOLD_FOR_ADDRESS_COMBINED, conf._SCALING_FACTOR, curr_country, conf._RAW_SCORES_DIRECTORY, conf._TOTAL_MATCHES_THRESHOLD, conf._LINKAGE_METHOD, master_csv_1, master_csv_2)\n",
    "            deduplicate_dataset_R( rscript_command=conf._RSCRIPT_CMD,  script_name=conf._SCRIPT_NAME, args=args )\n",
    "            \n",
    "            \n",
    "            normalized_duplicates=pd.DataFrame()\n",
    "            # Clean and normalize the score features\n",
    "            normalized_duplicates = clean_score_features(curr_country=curr_country, country_df=master_df_1.append(master_df_2), source_dir=conf._RAW_SCORES_DIRECTORY, target_dir=conf._CLEANED_SCORES_DIRECTORY, verbose=False)\n",
    "            \n",
    "            if normalized_duplicates.shape[0]!=0:\n",
    "                \n",
    "                print('\\n\\nFound potential duplicates. Processing their master and cross-reference...\\n')\n",
    "                # Get the unique set of master-record-ids\n",
    "                master_record_ids = get_deduplicated_master_records(normalized_duplicates=normalized_duplicates, country_df=master_df_1.append(master_df_2))\n",
    "                # Get the country-master-df\n",
    "                country_master_df = generate_deduplicated_master(country_df=master_df_1.append(master_df_2), master_record_ids=list(master_record_ids), curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create a dummy set of cross-refs for masters\n",
    "                cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "                # Create full set of cross-refs for country-df\n",
    "                cross_ref_df = generate_final_cross_refs(cross_ref_df=cross_ref_df, normalized_duplicates=normalized_duplicates, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create the csv for the cross-ref report\n",
    "                generate_cross_ref_report(cross_ref_df=cross_ref_df, country_df=master_df_1.append(master_df_2), curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                print('\\n\\nGet the unique set of all record-ids since there aren\\'t any potential duplicates.\\n')\n",
    "                # Get the unique set of all-record-ids since there aren't any potential duplicates\n",
    "                master_record_ids = master_df_1.append(master_df_2).index.values.astype(list)\n",
    "                # Get the country-master-df\n",
    "                country_master_df = generate_deduplicated_master(country_df=master_df_1.append(master_df_2), master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "                # Create a dummy set of cross-refs for masters\n",
    "                cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            print('\\n\\nGet the unique set of all record-ids since there isn\\'t a second file to compare.\\n')\n",
    "            # Get the unique set of master-record-ids\n",
    "            master_record_ids = master_df_1.index.values.astype(list)\n",
    "            # Get the country-master-df\n",
    "            country_master_df = generate_deduplicated_master(country_df=master_df_1, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._STAGING_AREA_DIRECTORY, write_csv=False)\n",
    "            # Create a dummy set of cross-refs for masters\n",
    "            cross_ref_df = generate_dummy_cross_refs_for_masters(master_record_ids=master_record_ids, curr_country=curr_country)\n",
    "            \n",
    "        \n",
    "        # Write the current master dataset to a csv, add the filename to the queue of csvs, and append currently generated cross-ref to existing cross-ref\n",
    "        combined_crossref_at_depth = combined_crossref_at_depth.append(cross_ref_df)\n",
    "        new_file_name='_d{}_{}_Master.csv'.format(j,i)\n",
    "        write_df_to_csv(df=country_master_df, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix=new_file_name, index_flag=True)\n",
    "        new_file_name=curr_country+new_file_name\n",
    "        queue_of_csvs.append(new_file_name)\n",
    "        del master_record_ids, master_df_1\n",
    "        if i+1<n_csvs_to_read:\n",
    "            del normalized_duplicates, master_df_2\n",
    "    \n",
    "    write_df_to_csv(df=combined_crossref_at_depth, root_dir=conf._STAGING_AREA_DIRECTORY, curr_country=curr_country, file_suffix='_d{}_Raw_Cross_Ref.csv'.format(j), index_flag=False)\n",
    "    print('\\n\\nDepth[{}] processed successfully.'.format(j))\n",
    "    update_entire_country_cross_ref(new_depth_cross_ref_df=combined_crossref_at_depth, entire_country_cross_ref_df=entire_country_cross_ref_df)\n",
    "    queue_of_csvs=queue_of_csvs[n_csvs_to_read:]\n",
    "\n",
    "\n",
    "\n",
    "if len(queue_of_csvs)==1:\n",
    "    print('\\n\\n\\n\\nProcessed all {} levels. Generating the master and cross-reference at the final-layer...'.format(d))\n",
    "    master_csv_1=os.path.join(conf._STAGING_AREA_DIRECTORY, queue_of_csvs[i])\n",
    "    master_df_1=pd.read_csv(master_csv_1, index_col=0)\n",
    "    # Get the unique set of master-record-ids\n",
    "    master_record_ids = master_df_1.index.values.astype(list)\n",
    "    # Get the country-master-df\n",
    "    country_master_df = generate_deduplicated_master(country_df=entire_country_df_copy, master_record_ids=master_record_ids, curr_country=curr_country, target_dir=conf._MASTER_DATA_DIRECTORY, write_csv=True)\n",
    "\t# Write the final raw-cross-ref to a csv\n",
    "    write_df_to_csv(df=entire_country_cross_ref_df, root_dir=conf._MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix='_Raw_Cross_Ref.csv', index_flag=False)\n",
    "    # Create the csv for the cross-ref report\n",
    "    generate_cross_ref_report(cross_ref_df=entire_country_cross_ref_df, country_df=entire_country_df_copy, curr_country=curr_country, target_dir=conf._MASTER_DATA_DIRECTORY)"
   ]
  }
 ]
}
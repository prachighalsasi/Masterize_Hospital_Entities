Starting the Recursive_Python_Site_Master script...
Successfully read the Source-file DM_SITE_INFORMATION_SRC_EXTRACT.csv

Columns: ['SOURCE_IDENTIFIER' 'DATA_SOURCE_NAME' 'PROTOCOL_NUMBER' 'SITE_NUM'
 'UNIQUE_SITE_ID' 'COUNTRY' 'SITE_NAME' 'STATE' 'CITY' 'ADDRESS_LINE_1'
 'ADDRESS_LINE_2' 'ADDRESS_LINE_3' 'POSTAL_CODE' 'SITE_STATUS']


Countries : ['Albania', 'Algeria', 'Argentina', 'Australia', 'Austria', 'Belarus', 'Belgium', 'Bosnia_and_Herzegovina', 'Brazil', 'Bulgaria', 'Canada', 'Chile', 'China', 'Colombia', 'Costa_Rica', 'Croatia', 'Czechia', 'Denmark', 'Dominican_Republic_(the)', 'Egypt', 'Estonia', 'Finland', 'France', 'Georgia', 'Germany', 'Greece', 'Guatemala', 'Hong_Kong', 'Hungary', 'Iceland', 'India', 'Ireland', 'Israel', 'Italy', 'Japan', 'Jordan', 'Kenya', 'Korea_(the_Republic_of)', 'Kuwait', 'Latvia', 'Lebanon', 'Lithuania', 'Luxembourg', 'Macedonia', 'Malaysia', 'Mexico', 'Moldova_(the_Republic_of)', 'Netherlands_(the)', 'New_Zealand', 'Nicaragua', 'North_Macedonia', 'Norway', 'Oman', 'Panama', 'Paraguay', 'Peru', 'Philippines_(the)', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russian_Federation_(the)', 'Saudi_Arabia', 'Serbia', 'Singapore', 'Slovakia', 'Slovenia', 'South_Africa', 'Spain', 'Sri_Lanka', 'Sweden', 'Switzerland', 'Taiwan_(Province_of_China)', 'Thailand', 'Tunisia', 'Turkey', 'Ukraine', 'United_Arab_Emirates_(the)', 'United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)', 'United_States_of_America_(the)', 'Venezuela_(Bolivarian_Republic_of)', 'Viet_Nam']

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Albania_country_df.csv!


Get the unique set of all record-ids, since Layer-Zero cannot create mastered mini-batches.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Albania_0_Master.csv!
1 csvs generated are: ['Albania_0_Master.csv']

Max-depth for Albania will be 1
1 csvs need to be processed: ['Albania_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Albania_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Albania_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Albania_Master.csv!
1 records get merged into 1

Successfully created \Master_Data/Albania_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Albania_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=18 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Algeria_country_df.csv!

Algeria_0 has 18 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Algeria Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 18 , Candidate-pairs= 153 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 153 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Algeria_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Algeria_Score_Features.csv !"

Time difference of 0.162385 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Algeria_Clea/usr/lib64/python2.7/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass 'sort=False'.

To retain the current behavior and silence the warning, pass 'sort=True'.

  sort=sort)
ned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

18 records get merged into 14

Successfully created \Master_Data/Recursive_Staging_Area/Algeria_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Algeria_0_Master.csv!
1 csvs generated are: ['Algeria_0_Master.csv']

Max-depth for Algeria will be 1
1 csvs need to be processed: ['Algeria_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

14 records get merged into 14

Successfully created \Master_Data/Recursive_Staging_Area/Algeria_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Algeria_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Algeria_Master.csv!
18 records get merged into 14

Successfully created \Master_Data/Algeria_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Algeria_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=461 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Argentina_country_df.csv!

Argentina_0 has 461 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Argentina Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 461 , Candidate-pairs= 106030 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 106030 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Argentina_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Argentina_Score_Features.csv !"

Time difference of 2.514823 secs



14 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Argentina_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

461 records get merged into 186

Successfully created \Master_Data/Recursive_Staging_Area/Argentina_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Argentina_0_Master.csv!
1 csvs generated are: ['Argentina_0_Master.csv']

Max-depth for Argentina will be 1
1 csvs need to be processed: ['Argentina_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

186 records get merged into 186

Successfully created \Master_Data/Recursive_Staging_Area/Argentina_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Argentina_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Argentina_Master.csv!
461 records get merged into 186

Successfully created \Master_Data/Argentina_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Argentina_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=739 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Australia_country_df.csv!

Australia_0 has 739 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Australia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 739 , Candidate-pairs= 272691 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 272691 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Australia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Australia_Score_Features.csv !"

Time difference of 5.358154 secs



19 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Australia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

739 records get merged into 235

Successfully created \Master_Data/Recursive_Staging_Area/Australia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Australia_0_Master.csv!
1 csvs generated are: ['Australia_0_Master.csv']

Max-depth for Australia will be 1
1 csvs need to be processed: ['Australia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

235 records get merged into 235

Successfully created \Master_Data/Recursive_Staging_Area/Australia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Australia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Australia_Master.csv!
739 records get merged into 235

Successfully created \Master_Data/Australia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Australia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=478 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Austria_country_df.csv!

Austria_0 has 478 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Austria Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 478 , Candidate-pairs= 114003 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 114003 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Austria_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Austria_Score_Features.csv !"

Time difference of 2.798855 secs



25 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Austria_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

478 records get merged into 153

Successfully created \Master_Data/Recursive_Staging_Area/Austria_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Austria_0_Master.csv!
1 csvs generated are: ['Austria_0_Master.csv']

Max-depth for Austria will be 1
1 csvs need to be processed: ['Austria_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

153 records get merged into 153

Successfully created \Master_Data/Recursive_Staging_Area/Austria_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Austria_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Austria_Master.csv!
478 records get merged into 153

Successfully created \Master_Data/Austria_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Austria_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=5 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Belarus_country_df.csv!

Belarus_0 has 5 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Belarus Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 5 , Candidate-pairs= 10 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 10 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "No potential matches found in the incoming dataset! Creating a dummy csv..."
[1] "Raw_Scores//Belarus_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Belarus_Score_Features.csv !"

Time difference of 0.1638198 secs



Get the unique set of all record-ids since there aren't any potential duplicates.

5 records get merged into 5

Successfully created \Master_Data/Recursive_Staging_Area/Belarus_0_Master.csv!
1 csvs generated are: ['Belarus_0_Master.csv']

Max-depth for Belarus will be 1
1 csvs need to be processed: ['Belarus_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

5 records get merged into 5

Successfully created \Master_Data/Recursive_Staging_Area/Belarus_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Belarus_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Belarus_Master.csv!
5 records get merged into 5

Successfully created \Master_Data/Belarus_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Belarus_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=692 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Belgium_country_df.csv!

Belgium_0 has 692 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Belgium Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 692 , Candidate-pairs= 239086 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 239086 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Belgium_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Belgium_Score_Features.csv !"

Time difference of 4.786693 secs



29 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Belgium_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

692 records get merged into 171

Successfully created \Master_Data/Recursive_Staging_Area/Belgium_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Belgium_0_Master.csv!
1 csvs generated are: ['Belgium_0_Master.csv']

Max-depth for Belgium will be 1
1 csvs need to be processed: ['Belgium_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

171 records get merged into 171

Successfully created \Master_Data/Recursive_Staging_Area/Belgium_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Belgium_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Belgium_Master.csv!
692 records get merged into 171

Successfully created \Master_Data/Belgium_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Belgium_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=143 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Bosnia_and_Herzegovina_country_df.csv!

Bosnia_and_Herzegovina_0 has 143 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Bosnia_and_Herzegovina Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 143 , Candidate-pairs= 10153 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 10153 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Bosnia_and_Herzegovina_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Bosnia_and_Herzegovina_Score_Features.csv !"

Time difference of 0.3991098 secs



3 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Bosnia_and_Herzegovina_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

143 records get merged into 25

Successfully created \Master_Data/Recursive_Staging_Area/Bosnia_and_Herzegovina_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Bosnia_and_Herzegovina_0_Master.csv!
1 csvs generated are: ['Bosnia_and_Herzegovina_0_Master.csv']

Max-depth for Bosnia_and_Herzegovina will be 1
1 csvs need to be processed: ['Bosnia_and_Herzegovina_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

25 records get merged into 25

Successfully created \Master_Data/Recursive_Staging_Area/Bosnia_and_Herzegovina_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Bosnia_and_Herzegovina_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Bosnia_and_Herzegovina_Master.csv!
143 records get merged into 25

Successfully created \Master_Data/Bosnia_and_Herzegovina_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Bosnia_and_Herzegovina_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=691 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Brazil_country_df.csv!

Brazil_0 has 691 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Brazil Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 691 , Candidate-pairs= 238395 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 238395 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Brazil_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Brazil_Score_Features.csv !"

Time difference of 6.278231 secs



21 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Brazil_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

691 records get merged into 306

Successfully created \Master_Data/Recursive_Staging_Area/Brazil_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Brazil_0_Master.csv!
1 csvs generated are: ['Brazil_0_Master.csv']

Max-depth for Brazil will be 1
1 csvs need to be processed: ['Brazil_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

306 records get merged into 306

Successfully created \Master_Data/Recursive_Staging_Area/Brazil_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Brazil_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Brazil_Master.csv!
691 records get merged into 306

Successfully created \Master_Data/Brazil_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Brazil_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=295 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Bulgaria_country_df.csv!

Bulgaria_0 has 295 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Bulgaria Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 295 , Candidate-pairs= 43365 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 43365 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Bulgaria_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Bulgaria_Score_Features.csv !"

Time difference of 1.390667 secs



10 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Bulgaria_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

295 records get merged into 101

Successfully created \Master_Data/Recursive_Staging_Area/Bulgaria_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Bulgaria_0_Master.csv!
1 csvs generated are: ['Bulgaria_0_Master.csv']

Max-depth for Bulgaria will be 1
1 csvs need to be processed: ['Bulgaria_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

101 records get merged into 101

Successfully created \Master_Data/Recursive_Staging_Area/Bulgaria_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Bulgaria_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Bulgaria_Master.csv!
295 records get merged into 101

Successfully created \Master_Data/Bulgaria_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Bulgaria_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1391 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Canada_country_df.csv!

Canada_0 has 1391 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Canada Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 1391 , Candidate-pairs= 966745 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 966745 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Canada_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Canada_Score_Features.csv !"

Time difference of 20.84592 secs



67 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Canada_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

1391 records get merged into 536

Successfully created \Master_Data/Recursive_Staging_Area/Canada_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Canada_0_Master.csv!
1 csvs generated are: ['Canada_0_Master.csv']

Max-depth for Canada will be 1
1 csvs need to be processed: ['Canada_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

536 records get merged into 536

Successfully created \Master_Data/Recursive_Staging_Area/Canada_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Canada_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Canada_Master.csv!
1391 records get merged into 536

Successfully created \Master_Data/Canada_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Canada_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=85 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Chile_country_df.csv!

Chile_0 has 85 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Chile Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 85 , Candidate-pairs= 3570 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 3570 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Chile_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Chile_Score_Features.csv !"

Time difference of 0.2186606 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Chile_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

85 records get merged into 57

Successfully created \Master_Data/Recursive_Staging_Area/Chile_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Chile_0_Master.csv!
1 csvs generated are: ['Chile_0_Master.csv']

Max-depth for Chile will be 1
1 csvs need to be processed: ['Chile_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

57 records get merged into 57

Successfully created \Master_Data/Recursive_Staging_Area/Chile_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Chile_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Chile_Master.csv!
85 records get merged into 57

Successfully created \Master_Data/Chile_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Chile_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=790 and minibatch-size=4000


Starting Batch[0]...

Successfully created \China_country_df.csv!

China_0 has 790 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 China Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 790 , Candidate-pairs= 311655 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 311655 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//China_Score_Features.csv"
[1] "Successfully created //Raw_Scores//China_Score_Features.csv !"

Time difference of 8.051106 secs



21 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/China_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

790 records get merged into 339

Successfully created \Master_Data/Recursive_Staging_Area/China_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/China_0_Master.csv!
1 csvs generated are: ['China_0_Master.csv']

Max-depth for China will be 1
1 csvs need to be processed: ['China_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

339 records get merged into 339

Successfully created \Master_Data/Recursive_Staging_Area/China_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/China_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/China_Master.csv!
790 records get merged into 339

Successfully created \Master_Data/China_Raw_Cross_Ref.csv!

Successfully created \Master_Data/China_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=289 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Colombia_country_df.csv!

Colombia_0 has 289 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Colombia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 289 , Candidate-pairs= 41616 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 41616 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Colombia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Colombia_Score_Features.csv !"

Time difference of 1.236409 secs



3 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Colombia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

289 records get merged into 94

Successfully created \Master_Data/Recursive_Staging_Area/Colombia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Colombia_0_Master.csv!
1 csvs generated are: ['Colombia_0_Master.csv']

Max-depth for Colombia will be 1
1 csvs need to be processed: ['Colombia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

94 records get merged into 94

Successfully created \Master_Data/Recursive_Staging_Area/Colombia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Colombia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Colombia_Master.csv!
289 records get merged into 94

Successfully created \Master_Data/Colombia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Colombia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=4 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Costa_Rica_country_df.csv!

Costa_Rica_0 has 4 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Costa_Rica Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 4 , Candidate-pairs= 6 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 6 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "No potential matches found in the incoming dataset! Creating a dummy csv..."
[1] "Raw_Scores//Costa_Rica_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Costa_Rica_Score_Features.csv !"

Time difference of 0.164834 secs



Get the unique set of all record-ids since there aren't any potential duplicates.

4 records get merged into 4

Successfully created \Master_Data/Recursive_Staging_Area/Costa_Rica_0_Master.csv!
1 csvs generated are: ['Costa_Rica_0_Master.csv']

Max-depth for Costa_Rica will be 1
1 csvs need to be processed: ['Costa_Rica_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

4 records get merged into 4

Successfully created \Master_Data/Recursive_Staging_Area/Costa_Rica_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Costa_Rica_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Costa_Rica_Master.csv!
4 records get merged into 4

Successfully created \Master_Data/Costa_Rica_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Costa_Rica_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=259 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Croatia_country_df.csv!

Croatia_0 has 259 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Croatia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 259 , Candidate-pairs= 33411 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 33411 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Croatia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Croatia_Score_Features.csv !"

Time difference of 0.8634248 secs



7 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Croatia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

259 records get merged into 62

Successfully created \Master_Data/Recursive_Staging_Area/Croatia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Croatia_0_Master.csv!
1 csvs generated are: ['Croatia_0_Master.csv']

Max-depth for Croatia will be 1
1 csvs need to be processed: ['Croatia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

62 records get merged into 62

Successfully created \Master_Data/Recursive_Staging_Area/Croatia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Croatia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Croatia_Master.csv!
259 records get merged into 62

Successfully created \Master_Data/Croatia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Croatia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=490 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Czechia_country_df.csv!

Czechia_0 has 490 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Czechia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 490 , Candidate-pairs= 119805 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 119805 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Czechia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Czechia_Score_Features.csv !"

Time difference of 2.519342 secs



4 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Czechia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

490 records get merged into 163

Successfully created \Master_Data/Recursive_Staging_Area/Czechia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Czechia_0_Master.csv!
1 csvs generated are: ['Czechia_0_Master.csv']

Max-depth for Czechia will be 1
1 csvs need to be processed: ['Czechia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

163 records get merged into 163

Successfully created \Master_Data/Recursive_Staging_Area/Czechia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Czechia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Czechia_Master.csv!
490 records get merged into 163

Successfully created \Master_Data/Czechia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Czechia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=245 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Denmark_country_df.csv!

Denmark_0 has 245 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Denmark Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 245 , Candidate-pairs= 29890 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 29890 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Denmark_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Denmark_Score_Features.csv !"

Time difference of 0.7913284 secs



3 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Denmark_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

245 records get merged into 99

Successfully created \Master_Data/Recursive_Staging_Area/Denmark_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Denmark_0_Master.csv!
1 csvs generated are: ['Denmark_0_Master.csv']

Max-depth for Denmark will be 1
1 csvs need to be processed: ['Denmark_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

99 records get merged into 99

Successfully created \Master_Data/Recursive_Staging_Area/Denmark_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Denmark_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Denmark_Master.csv!
245 records get merged into 99

Successfully created \Master_Data/Denmark_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Denmark_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=4 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Dominican_Republic_(the)_country_df.csv!

Dominican_Republic_(the)_0 has 4 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Dominican_Republic_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 4 , Candidate-pairs= 6 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 6 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Dominican_Republic_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Dominican_Republic_(the)_Score_Features.csv !"

Time difference of 0.1868141 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Dominican_Republic_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

4 records get merged into 3

Successfully created \Master_Data/Recursive_Staging_Area/Dominican_Republic_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Dominican_Republic_(the)_0_Master.csv!
1 csvs generated are: ['Dominican_Republic_(the)_0_Master.csv']

Max-depth for Dominican_Republic_(the) will be 1
1 csvs need to be processed: ['Dominican_Republic_(the)_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

3 records get merged into 3

Successfully created \Master_Data/Recursive_Staging_Area/Dominican_Republic_(the)_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Dominican_Republic_(the)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Dominican_Republic_(the)_Master.csv!
4 records get merged into 3

Successfully created \Master_Data/Dominican_Republic_(the)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Dominican_Republic_(the)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=28 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Egypt_country_df.csv!

Egypt_0 has 28 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Egypt Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 28 , Candidate-pairs= 378 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 378 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Egypt_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Egypt_Score_Features.csv !"

Time difference of 0.1782372 secs



1 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Egypt_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

28 records get merged into 12

Successfully created \Master_Data/Recursive_Staging_Area/Egypt_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Egypt_0_Master.csv!
1 csvs generated are: ['Egypt_0_Master.csv']

Max-depth for Egypt will be 1
1 csvs need to be processed: ['Egypt_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

12 records get merged into 12

Successfully created \Master_Data/Recursive_Staging_Area/Egypt_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Egypt_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Egypt_Master.csv!
28 records get merged into 12

Successfully created \Master_Data/Egypt_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Egypt_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=63 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Estonia_country_df.csv!

Estonia_0 has 63 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Estonia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 63 , Candidate-pairs= 1953 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 1953 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Estonia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Estonia_Score_Features.csv !"

Time difference of 0.1952877 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Estonia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

63 records get merged into 16

Successfully created \Master_Data/Recursive_Staging_Area/Estonia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Estonia_0_Master.csv!
1 csvs generated are: ['Estonia_0_Master.csv']

Max-depth for Estonia will be 1
1 csvs need to be processed: ['Estonia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

16 records get merged into 16

Successfully created \Master_Data/Recursive_Staging_Area/Estonia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Estonia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Estonia_Master.csv!
63 records get merged into 16

Successfully created \Master_Data/Estonia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Estonia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=152 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Finland_country_df.csv!

Finland_0 has 152 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Finland Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 152 , Candidate-pairs= 11476 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 11476 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Finland_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Finland_Score_Features.csv !"

Time difference of 0.439065 secs



5 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Finland_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

152 records get merged into 56

Successfully created \Master_Data/Recursive_Staging_Area/Finland_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Finland_0_Master.csv!
1 csvs generated are: ['Finland_0_Master.csv']

Max-depth for Finland will be 1
1 csvs need to be processed: ['Finland_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

56 records get merged into 56

Successfully created \Master_Data/Recursive_Staging_Area/Finland_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Finland_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Finland_Master.csv!
152 records get merged into 56

Successfully created \Master_Data/Finland_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Finland_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1628 and minibatch-size=4000


Starting Batch[0]...

Successfully created \France_country_df.csv!

France_0 has 1628 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 France Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 1628 , Candidate-pairs= 1324378 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 1324378 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//France_Score_Features.csv"
[1] "Successfully created //Raw_Scores//France_Score_Features.csv !"

Time difference of 30.74594 secs



61 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/France_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

1628 records get merged into 639

Successfully created \Master_Data/Recursive_Staging_Area/France_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/France_0_Master.csv!
1 csvs generated are: ['France_0_Master.csv']

Max-depth for France will be 1
1 csvs need to be processed: ['France_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

639 records get merged into 639

Successfully created \Master_Data/Recursive_Staging_Area/France_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/France_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/France_Master.csv!
1628 records get merged into 639

Successfully created \Master_Data/France_Raw_Cross_Ref.csv!

Successfully created \Master_Data/France_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=31 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Georgia_country_df.csv!

Georgia_0 has 31 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Georgia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 31 , Candidate-pairs= 465 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 465 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Georgia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Georgia_Score_Features.csv !"

Time difference of 0.2138782 secs



2 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Georgia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

31 records get merged into 7

Successfully created \Master_Data/Recursive_Staging_Area/Georgia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Georgia_0_Master.csv!
1 csvs generated are: ['Georgia_0_Master.csv']

Max-depth for Georgia will be 1
1 csvs need to be processed: ['Georgia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

7 records get merged into 7

Successfully created \Master_Data/Recursive_Staging_Area/Georgia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Georgia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Georgia_Master.csv!
31 records get merged into 7

Successfully created \Master_Data/Georgia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Georgia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=2275 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Germany_country_df.csv!

Germany_0 has 2275 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Germany Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 2275 , Candidate-pairs= 2586675 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 2586675 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Germany_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Germany_Score_Features.csv !"

Time difference of 1.011339 mins



74 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Germany_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

2275 records get merged into 764

Successfully created \Master_Data/Recursive_Staging_Area/Germany_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Germany_0_Master.csv!
1 csvs generated are: ['Germany_0_Master.csv']

Max-depth for Germany will be 1
1 csvs need to be processed: ['Germany_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

764 records get merged into 764

Successfully created \Master_Data/Recursive_Staging_Area/Germany_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Germany_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Germany_Master.csv!
2275 records get merged into 764

Successfully created \Master_Data/Germany_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Germany_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=344 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Greece_country_df.csv!

Greece_0 has 344 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Greece Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 344 , Candidate-pairs= 58996 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 58996 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Greece_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Greece_Score_Features.csv !"

Time difference of 1.530595 secs



7 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Greece_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

344 records get merged into 121

Successfully created \Master_Data/Recursive_Staging_Area/Greece_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Greece_0_Master.csv!
1 csvs generated are: ['Greece_0_Master.csv']

Max-depth for Greece will be 1
1 csvs need to be processed: ['Greece_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

121 records get merged into 121

Successfully created \Master_Data/Recursive_Staging_Area/Greece_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Greece_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Greece_Master.csv!
344 records get merged into 121

Successfully created \Master_Data/Greece_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Greece_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Guatemala_country_df.csv!


Get the unique set of all record-ids, since Layer-Zero cannot create mastered mini-batches.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Guatemala_0_Master.csv!
1 csvs generated are: ['Guatemala_0_Master.csv']

Max-depth for Guatemala will be 1
1 csvs need to be processed: ['Guatemala_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Guatemala_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Guatemala_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Guatemala_Master.csv!
1 records get merged into 1

Successfully created \Master_Data/Guatemala_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Guatemala_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=67 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Hong_Kong_country_df.csv!

Hong_Kong_0 has 67 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Hong_Kong Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 67 , Candidate-pairs= 2211 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 2211 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Hong_Kong_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Hong_Kong_Score_Features.csv !"

Time difference of 0.2374897 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Hong_Kong_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

67 records get merged into 33

Successfully created \Master_Data/Recursive_Staging_Area/Hong_Kong_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Hong_Kong_0_Master.csv!
1 csvs generated are: ['Hong_Kong_0_Master.csv']

Max-depth for Hong_Kong will be 1
1 csvs need to be processed: ['Hong_Kong_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

33 records get merged into 33

Successfully created \Master_Data/Recursive_Staging_Area/Hong_Kong_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Hong_Kong_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Hong_Kong_Master.csv!
67 records get merged into 33

Successfully created \Master_Data/Hong_Kong_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Hong_Kong_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=454 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Hungary_country_df.csv!

Hungary_0 has 454 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Hungary Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 454 , Candidate-pairs= 102831 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 102831 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Hungary_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Hungary_Score_Features.csv !"

Time difference of 2.5992 secs



9 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Hungary_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

454 records get merged into 175

Successfully created \Master_Data/Recursive_Staging_Area/Hungary_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Hungary_0_Master.csv!
1 csvs generated are: ['Hungary_0_Master.csv']

Max-depth for Hungary will be 1
1 csvs need to be processed: ['Hungary_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

175 records get merged into 175

Successfully created \Master_Data/Recursive_Staging_Area/Hungary_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Hungary_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Hungary_Master.csv!
454 records get merged into 175

Successfully created \Master_Data/Hungary_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Hungary_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Iceland_country_df.csv!


Get the unique set of all record-ids, since Layer-Zero cannot create mastered mini-batches.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Iceland_0_Master.csv!
1 csvs generated are: ['Iceland_0_Master.csv']

Max-depth for Iceland will be 1
1 csvs need to be processed: ['Iceland_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Iceland_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Iceland_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Iceland_Master.csv!
1 records get merged into 1

Successfully created \Master_Data/Iceland_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Iceland_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=244 and minibatch-size=4000


Starting Batch[0]...

Successfully created \India_country_df.csv!

India_0 has 244 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 India Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 244 , Candidate-pairs= 29646 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 29646 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//India_Score_Features.csv"
[1] "Successfully created //Raw_Scores//India_Score_Features.csv !"

Time difference of 0.8169038 secs



11 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/India_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

244 records get merged into 149

Successfully created \Master_Data/Recursive_Staging_Area/India_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/India_0_Master.csv!
1 csvs generated are: ['India_0_Master.csv']

Max-depth for India will be 1
1 csvs need to be processed: ['India_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

149 records get merged into 149

Successfully created \Master_Data/Recursive_Staging_Area/India_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/India_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/India_Master.csv!
244 records get merged into 149

Successfully created \Master_Data/India_Raw_Cross_Ref.csv!

Successfully created \Master_Data/India_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=142 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Ireland_country_df.csv!

Ireland_0 has 142 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Ireland Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 142 , Candidate-pairs= 10011 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 10011 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Ireland_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Ireland_Score_Features.csv !"

Time difference of 0.3752799 secs



5 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Ireland_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

142 records get merged into 46

Successfully created \Master_Data/Recursive_Staging_Area/Ireland_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Ireland_0_Master.csv!
1 csvs generated are: ['Ireland_0_Master.csv']

Max-depth for Ireland will be 1
1 csvs need to be processed: ['Ireland_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

46 records get merged into 46

Successfully created \Master_Data/Recursive_Staging_Area/Ireland_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Ireland_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Ireland_Master.csv!
142 records get merged into 46

Successfully created \Master_Data/Ireland_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Ireland_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=468 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Israel_country_df.csv!

Israel_0 has 468 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Israel Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 468 , Candidate-pairs= 109278 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 109278 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Israel_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Israel_Score_Features.csv !"

Time difference of 2.369104 secs



8 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Israel_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

468 records get merged into 146

Successfully created \Master_Data/Recursive_Staging_Area/Israel_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Israel_0_Master.csv!
1 csvs generated are: ['Israel_0_Master.csv']

Max-depth for Israel will be 1
1 csvs need to be processed: ['Israel_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

146 records get merged into 146

Successfully created \Master_Data/Recursive_Staging_Area/Israel_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Israel_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Israel_Master.csv!
468 records get merged into 146

Successfully created \Master_Data/Israel_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Israel_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1380 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Italy_country_df.csv!

Italy_0 has 1380 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Italy Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 1380 , Candidate-pairs= 951510 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 951510 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Italy_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Italy_Score_Features.csv !"

Time difference of 27.70621 secs



53 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Italy_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

1380 records get merged into 472

Successfully created \Master_Data/Recursive_Staging_Area/Italy_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Italy_0_Master.csv!
1 csvs generated are: ['Italy_0_Master.csv']

Max-depth for Italy will be 1
1 csvs need to be processed: ['Italy_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

472 records get merged into 472

Successfully created \Master_Data/Recursive_Staging_Area/Italy_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Italy_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Italy_Master.csv!
1380 records get merged into 472

Successfully created \Master_Data/Italy_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Italy_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1569 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Japan_country_df.csv!

Japan_0 has 1569 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Japan Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 1569 , Candidate-pairs= 1230096 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 1230096 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Japan_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Japan_Score_Features.csv !"

Time difference of 27.63436 secs



5 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Japan_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

1569 records get merged into 454

Successfully created \Master_Data/Recursive_Staging_Area/Japan_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Japan_0_Master.csv!
1 csvs generated are: ['Japan_0_Master.csv']

Max-depth for Japan will be 1
1 csvs need to be processed: ['Japan_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

454 records get merged into 454

Successfully created \Master_Data/Recursive_Staging_Area/Japan_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Japan_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Japan_Master.csv!
1569 records get merged into 454

Successfully created \Master_Data/Japan_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Japan_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=30 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Jordan_country_df.csv!

Jordan_0 has 30 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Jordan Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 30 , Candidate-pairs= 435 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 435 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Jordan_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Jordan_Score_Features.csv !"

Time difference of 0.1884813 secs



3 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Jordan_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

30 records get merged into 8

Successfully created \Master_Data/Recursive_Staging_Area/Jordan_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Jordan_0_Master.csv!
1 csvs generated are: ['Jordan_0_Master.csv']

Max-depth for Jordan will be 1
1 csvs need to be processed: ['Jordan_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

8 records get merged into 8

Successfully created \Master_Data/Recursive_Staging_Area/Jordan_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Jordan_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Jordan_Master.csv!
30 records get merged into 8

Successfully created \Master_Data/Jordan_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Jordan_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=14 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Kenya_country_df.csv!

Kenya_0 has 14 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Kenya Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 14 , Candidate-pairs= 91 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 91 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Kenya_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Kenya_Score_Features.csv !"

Time difference of 0.1794927 secs



1 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Kenya_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

14 records get merged into 7

Successfully created \Master_Data/Recursive_Staging_Area/Kenya_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Kenya_0_Master.csv!
1 csvs generated are: ['Kenya_0_Master.csv']

Max-depth for Kenya will be 1
1 csvs need to be processed: ['Kenya_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

7 records get merged into 7

Successfully created \Master_Data/Recursive_Staging_Area/Kenya_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Kenya_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Kenya_Master.csv!
14 records get merged into 7

Successfully created \Master_Data/Kenya_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Kenya_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=498 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Korea_(the_Republic_of)_country_df.csv!

Korea_(the_Republic_of)_0 has 498 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Korea_(the_Republic_of) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 498 , Candidate-pairs= 123753 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 123753 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Korea_(the_Republic_of)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Korea_(the_Republic_of)_Score_Features.csv !"

Time difference of 3.257926 secs



10 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Korea_(the_Republic_of)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

498 records get merged into 136

Successfully created \Master_Data/Recursive_Staging_Area/Korea_(the_Republic_of)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Korea_(the_Republic_of)_0_Master.csv!
1 csvs generated are: ['Korea_(the_Republic_of)_0_Master.csv']

Max-depth for Korea_(the_Republic_of) will be 1
1 csvs need to be processed: ['Korea_(the_Republic_of)_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

136 records get merged into 136

Successfully created \Master_Data/Recursive_Staging_Area/Korea_(the_Republic_of)_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Korea_(the_Republic_of)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Korea_(the_Republic_of)_Master.csv!
498 records get merged into 136

Successfully created \Master_Data/Korea_(the_Republic_of)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Korea_(the_Republic_of)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Kuwait_country_df.csv!


Get the unique set of all record-ids, since Layer-Zero cannot create mastered mini-batches.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Kuwait_0_Master.csv!
1 csvs generated are: ['Kuwait_0_Master.csv']

Max-depth for Kuwait will be 1
1 csvs need to be processed: ['Kuwait_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Kuwait_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Kuwait_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Kuwait_Master.csv!
1 records get merged into 1

Successfully created \Master_Data/Kuwait_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Kuwait_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=27 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Latvia_country_df.csv!

Latvia_0 has 27 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Latvia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 27 , Candidate-pairs= 351 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 351 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Latvia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Latvia_Score_Features.csv !"

Time difference of 0.1953666 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Latvia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

27 records get merged into 13

Successfully created \Master_Data/Recursive_Staging_Area/Latvia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Latvia_0_Master.csv!
1 csvs generated are: ['Latvia_0_Master.csv']

Max-depth for Latvia will be 1
1 csvs need to be processed: ['Latvia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

13 records get merged into 13

Successfully created \Master_Data/Recursive_Staging_Area/Latvia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Latvia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Latvia_Master.csv!
27 records get merged into 13

Successfully created \Master_Data/Latvia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Latvia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=112 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Lebanon_country_df.csv!

Lebanon_0 has 112 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Lebanon Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 112 , Candidate-pairs= 6216 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 6216 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Lebanon_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Lebanon_Score_Features.csv !"

Time difference of 0.3807364 secs



3 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Lebanon_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

112 records get merged into 31

Successfully created \Master_Data/Recursive_Staging_Area/Lebanon_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Lebanon_0_Master.csv!
1 csvs generated are: ['Lebanon_0_Master.csv']

Max-depth for Lebanon will be 1
1 csvs need to be processed: ['Lebanon_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

31 records get merged into 31

Successfully created \Master_Data/Recursive_Staging_Area/Lebanon_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Lebanon_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Lebanon_Master.csv!
112 records get merged into 31

Successfully created \Master_Data/Lebanon_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Lebanon_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=87 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Lithuania_country_df.csv!

Lithuania_0 has 87 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Lithuania Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 87 , Candidate-pairs= 3741 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 3741 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Lithuania_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Lithuania_Score_Features.csv !"

Time difference of 0.2593045 secs



2 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Lithuania_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

87 records get merged into 32

Successfully created \Master_Data/Recursive_Staging_Area/Lithuania_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Lithuania_0_Master.csv!
1 csvs generated are: ['Lithuania_0_Master.csv']

Max-depth for Lithuania will be 1
1 csvs need to be processed: ['Lithuania_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

32 records get merged into 32

Successfully created \Master_Data/Recursive_Staging_Area/Lithuania_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Lithuania_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Lithuania_Master.csv!
87 records get merged into 32

Successfully created \Master_Data/Lithuania_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Lithuania_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Luxembourg_country_df.csv!


Get the unique set of all record-ids, since Layer-Zero cannot create mastered mini-batches.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Luxembourg_0_Master.csv!
1 csvs generated are: ['Luxembourg_0_Master.csv']

Max-depth for Luxembourg will be 1
1 csvs need to be processed: ['Luxembourg_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Luxembourg_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Luxembourg_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Luxembourg_Master.csv!
1 records get merged into 1

Successfully created \Master_Data/Luxembourg_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Luxembourg_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=2 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Macedonia_country_df.csv!

Macedonia_0 has 2 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Macedonia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 2 , Candidate-pairs= 1 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 1 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Macedonia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Macedonia_Score_Features.csv !"

Time difference of 0.1764984 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Macedonia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

2 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Macedonia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Macedonia_0_Master.csv!
1 csvs generated are: ['Macedonia_0_Master.csv']

Max-depth for Macedonia will be 1
1 csvs need to be processed: ['Macedonia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Macedonia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Macedonia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Macedonia_Master.csv!
2 records get merged into 1

Successfully created \Master_Data/Macedonia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Macedonia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=181 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Malaysia_country_df.csv!

Malaysia_0 has 181 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Malaysia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 181 , Candidate-pairs= 16290 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 16290 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Malaysia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Malaysia_Score_Features.csv !"

Time difference of 0.5314238 secs



3 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Malaysia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

181 records get merged into 67

Successfully created \Master_Data/Recursive_Staging_Area/Malaysia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Malaysia_0_Master.csv!
1 csvs generated are: ['Malaysia_0_Master.csv']

Max-depth for Malaysia will be 1
1 csvs need to be processed: ['Malaysia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

67 records get merged into 67

Successfully created \Master_Data/Recursive_Staging_Area/Malaysia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Malaysia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Malaysia_Master.csv!
181 records get merged into 67

Successfully created \Master_Data/Malaysia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Malaysia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=578 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Mexico_country_df.csv!

Mexico_0 has 578 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Mexico Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 578 , Candidate-pairs= 166753 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 166753 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Mexico_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Mexico_Score_Features.csv !"

Time difference of 4.509128 secs



5 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Mexico_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

578 records get merged into 272

Successfully created \Master_Data/Recursive_Staging_Area/Mexico_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Mexico_0_Master.csv!
1 csvs generated are: ['Mexico_0_Master.csv']

Max-depth for Mexico will be 1
1 csvs need to be processed: ['Mexico_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

272 records get merged into 272

Successfully created \Master_Data/Recursive_Staging_Area/Mexico_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Mexico_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Mexico_Master.csv!
578 records get merged into 272

Successfully created \Master_Data/Mexico_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Mexico_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=10 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Moldova_(the_Republic_of)_country_df.csv!

Moldova_(the_Republic_of)_0 has 10 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Moldova_(the_Republic_of) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 10 , Candidate-pairs= 45 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 45 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Moldova_(the_Republic_of)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Moldova_(the_Republic_of)_Score_Features.csv !"

Time difference of 0.1939182 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Moldova_(the_Republic_of)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

10 records get merged into 8

Successfully created \Master_Data/Recursive_Staging_Area/Moldova_(the_Republic_of)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Moldova_(the_Republic_of)_0_Master.csv!
1 csvs generated are: ['Moldova_(the_Republic_of)_0_Master.csv']

Max-depth for Moldova_(the_Republic_of) will be 1
1 csvs need to be processed: ['Moldova_(the_Republic_of)_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

8 records get merged into 8

Successfully created \Master_Data/Recursive_Staging_Area/Moldova_(the_Republic_of)_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Moldova_(the_Republic_of)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Moldova_(the_Republic_of)_Master.csv!
10 records get merged into 8

Successfully created \Master_Data/Moldova_(the_Republic_of)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Moldova_(the_Republic_of)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=572 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Netherlands_(the)_country_df.csv!

Netherlands_(the)_0 has 572 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Netherlands_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 572 , Candidate-pairs= 163306 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 163306 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Netherlands_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Netherlands_(the)_Score_Features.csv !"

Time difference of 3.778172 secs



14 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Netherlands_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

572 records get merged into 160

Successfully created \Master_Data/Recursive_Staging_Area/Netherlands_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Netherlands_(the)_0_Master.csv!
1 csvs generated are: ['Netherlands_(the)_0_Master.csv']

Max-depth for Netherlands_(the) will be 1
1 csvs need to be processed: ['Netherlands_(the)_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

160 records get merged into 160

Successfully created \Master_Data/Recursive_Staging_Area/Netherlands_(the)_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Netherlands_(the)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Netherlands_(the)_Master.csv!
572 records get merged into 160

Successfully created \Master_Data/Netherlands_(the)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Netherlands_(the)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=146 and minibatch-size=4000


Starting Batch[0]...

Successfully created \New_Zealand_country_df.csv!

New_Zealand_0 has 146 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 New_Zealand Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782214  6.0
[1] "NRows= 146 , Candidate-pairs= 10585 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 10585 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//New_Zealand_Score_Features.csv"
[1] "Successfully created //Raw_Scores//New_Zealand_Score_Features.csv !"

Time difference of 0.4195981 secs



2 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/New_Zealand_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

146 records get merged into 44

Successfully created \Master_Data/Recursive_Staging_Area/New_Zealand_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/New_Zealand_0_Master.csv!
1 csvs generated are: ['New_Zealand_0_Master.csv']

Max-depth for New_Zealand will be 1
1 csvs need to be processed: ['New_Zealand_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

44 records get merged into 44

Successfully created \Master_Data/Recursive_Staging_Area/New_Zealand_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/New_Zealand_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/New_Zealand_Master.csv!
146 records get merged into 44

Successfully created \Master_Data/New_Zealand_Raw_Cross_Ref.csv!

Successfully created \Master_Data/New_Zealand_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=2 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Nicaragua_country_df.csv!

Nicaragua_0 has 2 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Nicaragua Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 2 , Candidate-pairs= 1 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 1 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "No potential matches found in the incoming dataset! Creating a dummy csv..."
[1] "Raw_Scores//Nicaragua_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Nicaragua_Score_Features.csv !"

Time difference of 0.1878366 secs



Get the unique set of all record-ids since there aren't any potential duplicates.

2 records get merged into 2

Successfully created \Master_Data/Recursive_Staging_Area/Nicaragua_0_Master.csv!
1 csvs generated are: ['Nicaragua_0_Master.csv']

Max-depth for Nicaragua will be 1
1 csvs need to be processed: ['Nicaragua_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

2 records get merged into 2

Successfully created \Master_Data/Recursive_Staging_Area/Nicaragua_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Nicaragua_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Nicaragua_Master.csv!
2 records get merged into 2

Successfully created \Master_Data/Nicaragua_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Nicaragua_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=3 and minibatch-size=4000


Starting Batch[0]...

Successfully created \North_Macedonia_country_df.csv!

North_Macedonia_0 has 3 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 North_Macedonia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782214  6.0
[1] "NRows= 3 , Candidate-pairs= 3 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 3 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//North_Macedonia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//North_Macedonia_Score_Features.csv !"

Time difference of 0.1799228 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/North_Macedonia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

3 records get merged into 2

Successfully created \Master_Data/Recursive_Staging_Area/North_Macedonia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/North_Macedonia_0_Master.csv!
1 csvs generated are: ['North_Macedonia_0_Master.csv']

Max-depth for North_Macedonia will be 1
1 csvs need to be processed: ['North_Macedonia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

2 records get merged into 2

Successfully created \Master_Data/Recursive_Staging_Area/North_Macedonia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/North_Macedonia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/North_Macedonia_Master.csv!
3 records get merged into 2

Successfully created \Master_Data/North_Macedonia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/North_Macedonia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=235 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Norway_country_df.csv!

Norway_0 has 235 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Norway Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 235 , Candidate-pairs= 27495 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 27495 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Norway_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Norway_Score_Features.csv !"

Time difference of 0.8465466 secs



5 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Norway_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

235 records get merged into 105

Successfully created \Master_Data/Recursive_Staging_Area/Norway_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Norway_0_Master.csv!
1 csvs generated are: ['Norway_0_Master.csv']

Max-depth for Norway will be 1
1 csvs need to be processed: ['Norway_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

105 records get merged into 105

Successfully created \Master_Data/Recursive_Staging_Area/Norway_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Norway_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Norway_Master.csv!
235 records get merged into 105

Successfully created \Master_Data/Norway_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Norway_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=3 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Oman_country_df.csv!

Oman_0 has 3 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Oman Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 3 , Candidate-pairs= 3 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 3 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Oman_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Oman_Score_Features.csv !"

Time difference of 0.1865003 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Oman_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

3 records get merged into 2

Successfully created \Master_Data/Recursive_Staging_Area/Oman_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Oman_0_Master.csv!
1 csvs generated are: ['Oman_0_Master.csv']

Max-depth for Oman will be 1
1 csvs need to be processed: ['Oman_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

2 records get merged into 2

Successfully created \Master_Data/Recursive_Staging_Area/Oman_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Oman_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Oman_Master.csv!
3 records get merged into 2

Successfully created \Master_Data/Oman_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Oman_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=14 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Panama_country_df.csv!

Panama_0 has 14 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Panama Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 14 , Candidate-pairs= 91 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 91 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Panama_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Panama_Score_Features.csv !"

Time difference of 0.1970809 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Panama_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

14 records get merged into 12

Successfully created \Master_Data/Recursive_Staging_Area/Panama_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Panama_0_Master.csv!
1 csvs generated are: ['Panama_0_Master.csv']

Max-depth for Panama will be 1
1 csvs need to be processed: ['Panama_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

12 records get merged into 12

Successfully created \Master_Data/Recursive_Staging_Area/Panama_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Panama_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Panama_Master.csv!
14 records get merged into 12

Successfully created \Master_Data/Panama_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Panama_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Paraguay_country_df.csv!


Get the unique set of all record-ids, since Layer-Zero cannot create mastered mini-batches.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Paraguay_0_Master.csv!
1 csvs generated are: ['Paraguay_0_Master.csv']

Max-depth for Paraguay will be 1
1 csvs need to be processed: ['Paraguay_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

1 records get merged into 1

Successfully created \Master_Data/Recursive_Staging_Area/Paraguay_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Paraguay_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Paraguay_Master.csv!
1 records get merged into 1

Successfully created \Master_Data/Paraguay_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Paraguay_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=42 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Peru_country_df.csv!

Peru_0 has 42 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Peru Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 42 , Candidate-pairs= 861 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 861 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Peru_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Peru_Score_Features.csv !"

Time difference of 0.1981814 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Peru_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

42 records get merged into 37

Successfully created \Master_Data/Recursive_Staging_Area/Peru_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Peru_0_Master.csv!
1 csvs generated are: ['Peru_0_Master.csv']

Max-depth for Peru will be 1
1 csvs need to be processed: ['Peru_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

37 records get merged into 37

Successfully created \Master_Data/Recursive_Staging_Area/Peru_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Peru_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Peru_Master.csv!
42 records get merged into 37

Successfully created \Master_Data/Peru_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Peru_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=58 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Philippines_(the)_country_df.csv!

Philippines_(the)_0 has 58 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Philippines_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 58 , Candidate-pairs= 1653 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 1653 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Philippines_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Philippines_(the)_Score_Features.csv !"

Time difference of 0.2119491 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Philippines_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

58 records get merged into 48

Successfully created \Master_Data/Recursive_Staging_Area/Philippines_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Philippines_(the)_0_Master.csv!
1 csvs generated are: ['Philippines_(the)_0_Master.csv']

Max-depth for Philippines_(the) will be 1
1 csvs need to be processed: ['Philippines_(the)_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

48 records get merged into 48

Successfully created \Master_Data/Recursive_Staging_Area/Philippines_(the)_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Philippines_(the)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Philippines_(the)_Master.csv!
58 records get merged into 48

Successfully created \Master_Data/Philippines_(the)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Philippines_(the)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1223 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Poland_country_df.csv!

Poland_0 has 1223 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Poland Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 1223 , Candidate-pairs= 747253 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 747253 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Poland_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Poland_Score_Features.csv !"

Time difference of 19.56762 secs



41 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Poland_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

1223 records get merged into 465

Successfully created \Master_Data/Recursive_Staging_Area/Poland_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Poland_0_Master.csv!
1 csvs generated are: ['Poland_0_Master.csv']

Max-depth for Poland will be 1
1 csvs need to be processed: ['Poland_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

465 records get merged into 465

Successfully created \Master_Data/Recursive_Staging_Area/Poland_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Poland_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Poland_Master.csv!
1223 records get merged into 465

Successfully created \Master_Data/Poland_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Poland_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=325 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Portugal_country_df.csv!

Portugal_0 has 325 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Portugal Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 325 , Candidate-pairs= 52650 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 52650 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Portugal_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Portugal_Score_Features.csv !"

Time difference of 1.766668 secs



9 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Portugal_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

325 records get merged into 89

Successfully created \Master_Data/Recursive_Staging_Area/Portugal_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Portugal_0_Master.csv!
1 csvs generated are: ['Portugal_0_Master.csv']

Max-depth for Portugal will be 1
1 csvs need to be processed: ['Portugal_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

89 records get merged into 89

Successfully created \Master_Data/Recursive_Staging_Area/Portugal_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Portugal_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Portugal_Master.csv!
325 records get merged into 89

Successfully created \Master_Data/Portugal_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Portugal_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=2 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Qatar_country_df.csv!

Qatar_0 has 2 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Qatar Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 2 , Candidate-pairs= 1 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 1 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "No potential matches found in the incoming dataset! Creating a dummy csv..."
[1] "Raw_Scores//Qatar_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Qatar_Score_Features.csv !"

Time difference of 0.1652262 secs



Get the unique set of all record-ids since there aren't any potential duplicates.

2 records get merged into 2

Successfully created \Master_Data/Recursive_Staging_Area/Qatar_0_Master.csv!
1 csvs generated are: ['Qatar_0_Master.csv']

Max-depth for Qatar will be 1
1 csvs need to be processed: ['Qatar_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

2 records get merged into 2

Successfully created \Master_Data/Recursive_Staging_Area/Qatar_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Qatar_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Qatar_Master.csv!
2 records get merged into 2

Successfully created \Master_Data/Qatar_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Qatar_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=431 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Romania_country_df.csv!

Romania_0 has 431 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Romania Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 431 , Candidate-pairs= 92665 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 92665 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Romania_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Romania_Score_Features.csv !"

Time difference of 2.32609 secs



10 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Romania_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

431 records get merged into 145

Successfully created \Master_Data/Recursive_Staging_Area/Romania_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Romania_0_Master.csv!
1 csvs generated are: ['Romania_0_Master.csv']

Max-depth for Romania will be 1
1 csvs need to be processed: ['Romania_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

145 records get merged into 145

Successfully created \Master_Data/Recursive_Staging_Area/Romania_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Romania_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Romania_Master.csv!
431 records get merged into 145

Successfully created \Master_Data/Romania_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Romania_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=720 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Russian_Federation_(the)_country_df.csv!

Russian_Federation_(the)_0 has 720 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Russian_Federation_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 720 , Candidate-pairs= 258840 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 258840 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Russian_Federation_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Russian_Federation_(the)_Score_Features.csv !"

Time difference of 6.596787 secs



11 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Russian_Federation_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

720 records get merged into 282

Successfully created \Master_Data/Recursive_Staging_Area/Russian_Federation_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Russian_Federation_(the)_0_Master.csv!
1 csvs generated are: ['Russian_Federation_(the)_0_Master.csv']

Max-depth for Russian_Federation_(the) will be 1
1 csvs need to be processed: ['Russian_Federation_(the)_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

282 records get merged into 282

Successfully created \Master_Data/Recursive_Staging_Area/Russian_Federation_(the)_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Russian_Federation_(the)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Russian_Federation_(the)_Master.csv!
720 records get merged into 282

Successfully created \Master_Data/Russian_Federation_(the)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Russian_Federation_(the)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=39 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Saudi_Arabia_country_df.csv!

Saudi_Arabia_0 has 39 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Saudi_Arabia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782214  6.0
[1] "NRows= 39 , Candidate-pairs= 741 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 741 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Saudi_Arabia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Saudi_Arabia_Score_Features.csv !"

Time difference of 0.1779418 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Saudi_Arabia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

39 records get merged into 23

Successfully created \Master_Data/Recursive_Staging_Area/Saudi_Arabia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Saudi_Arabia_0_Master.csv!
1 csvs generated are: ['Saudi_Arabia_0_Master.csv']

Max-depth for Saudi_Arabia will be 1
1 csvs need to be processed: ['Saudi_Arabia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

23 records get merged into 23

Successfully created \Master_Data/Recursive_Staging_Area/Saudi_Arabia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Saudi_Arabia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Saudi_Arabia_Master.csv!
39 records get merged into 23

Successfully created \Master_Data/Saudi_Arabia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Saudi_Arabia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=189 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Serbia_country_df.csv!

Serbia_0 has 189 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Serbia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 189 , Candidate-pairs= 17766 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 17766 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Serbia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Serbia_Score_Features.csv !"

Time difference of 0.5509074 secs



9 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Serbia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

189 records get merged into 56

Successfully created \Master_Data/Recursive_Staging_Area/Serbia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Serbia_0_Master.csv!
1 csvs generated are: ['Serbia_0_Master.csv']

Max-depth for Serbia will be 1
1 csvs need to be processed: ['Serbia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

56 records get merged into 56

Successfully created \Master_Data/Recursive_Staging_Area/Serbia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Serbia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Serbia_Master.csv!
189 records get merged into 56

Successfully created \Master_Data/Serbia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Serbia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=70 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Singapore_country_df.csv!

Singapore_0 has 70 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Singapore Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 70 , Candidate-pairs= 2415 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 2415 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Singapore_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Singapore_Score_Features.csv !"

Time difference of 0.2214856 secs



1 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Singapore_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

70 records get merged into 29

Successfully created \Master_Data/Recursive_Staging_Area/Singapore_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Singapore_0_Master.csv!
1 csvs generated are: ['Singapore_0_Master.csv']

Max-depth for Singapore will be 1
1 csvs need to be processed: ['Singapore_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

29 records get merged into 29

Successfully created \Master_Data/Recursive_Staging_Area/Singapore_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Singapore_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Singapore_Master.csv!
70 records get merged into 29

Successfully created \Master_Data/Singapore_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Singapore_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=321 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Slovakia_country_df.csv!

Slovakia_0 has 321 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Slovakia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 321 , Candidate-pairs= 51360 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 51360 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Slovakia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Slovakia_Score_Features.csv !"

Time difference of 1.231915 secs



6 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Slovakia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

321 records get merged into 88

Successfully created \Master_Data/Recursive_Staging_Area/Slovakia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Slovakia_0_Master.csv!
1 csvs generated are: ['Slovakia_0_Master.csv']

Max-depth for Slovakia will be 1
1 csvs need to be processed: ['Slovakia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

88 records get merged into 88

Successfully created \Master_Data/Recursive_Staging_Area/Slovakia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Slovakia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Slovakia_Master.csv!
321 records get merged into 88

Successfully created \Master_Data/Slovakia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Slovakia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=27 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Slovenia_country_df.csv!

Slovenia_0 has 27 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Slovenia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 27 , Candidate-pairs= 351 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 351 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Slovenia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Slovenia_Score_Features.csv !"

Time difference of 0.1906068 secs



1 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Slovenia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

27 records get merged into 14

Successfully created \Master_Data/Recursive_Staging_Area/Slovenia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Slovenia_0_Master.csv!
1 csvs generated are: ['Slovenia_0_Master.csv']

Max-depth for Slovenia will be 1
1 csvs need to be processed: ['Slovenia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

14 records get merged into 14

Successfully created \Master_Data/Recursive_Staging_Area/Slovenia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Slovenia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Slovenia_Master.csv!
27 records get merged into 14

Successfully created \Master_Data/Slovenia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Slovenia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=283 and minibatch-size=4000


Starting Batch[0]...

Successfully created \South_Africa_country_df.csv!

South_Africa_0 has 283 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 South_Africa Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782214  6.0
[1] "NRows= 283 , Candidate-pairs= 39903 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 39903 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//South_Africa_Score_Features.csv"
[1] "Successfully created //Raw_Scores//South_Africa_Score_Features.csv !"

Time difference of 0.9570034 secs



1 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/South_Africa_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

283 records get merged into 121

Successfully created \Master_Data/Recursive_Staging_Area/South_Africa_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/South_Africa_0_Master.csv!
1 csvs generated are: ['South_Africa_0_Master.csv']

Max-depth for South_Africa will be 1
1 csvs need to be processed: ['South_Africa_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

121 records get merged into 121

Successfully created \Master_Data/Recursive_Staging_Area/South_Africa_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/South_Africa_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/South_Africa_Master.csv!
283 records get merged into 121

Successfully created \Master_Data/South_Africa_Raw_Cross_Ref.csv!

Successfully created \Master_Data/South_Africa_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1466 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Spain_country_df.csv!

Spain_0 has 1466 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Spain Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 1466 , Candidate-pairs= 1073845 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 1073845 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Spain_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Spain_Score_Features.csv !"

Time difference of 27.47529 secs



31 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Spain_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

1466 records get merged into 391

Successfully created \Master_Data/Recursive_Staging_Area/Spain_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Spain_0_Master.csv!
1 csvs generated are: ['Spain_0_Master.csv']

Max-depth for Spain will be 1
1 csvs need to be processed: ['Spain_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

391 records get merged into 391

Successfully created \Master_Data/Recursive_Staging_Area/Spain_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Spain_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Spain_Master.csv!
1466 records get merged into 391

Successfully created \Master_Data/Spain_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Spain_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=5 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Sri_Lanka_country_df.csv!

Sri_Lanka_0 has 5 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Sri_Lanka Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 5 , Candidate-pairs= 10 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 10 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "No potential matches found in the incoming dataset! Creating a dummy csv..."
[1] "Raw_Scores//Sri_Lanka_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Sri_Lanka_Score_Features.csv !"

Time difference of 0.1640475 secs



Get the unique set of all record-ids since there aren't any potential duplicates.

5 records get merged into 5

Successfully created \Master_Data/Recursive_Staging_Area/Sri_Lanka_0_Master.csv!
1 csvs generated are: ['Sri_Lanka_0_Master.csv']

Max-depth for Sri_Lanka will be 1
1 csvs need to be processed: ['Sri_Lanka_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

5 records get merged into 5

Successfully created \Master_Data/Recursive_Staging_Area/Sri_Lanka_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Sri_Lanka_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Sri_Lanka_Master.csv!
5 records get merged into 5

Successfully created \Master_Data/Sri_Lanka_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Sri_Lanka_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=356 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Sweden_country_df.csv!

Sweden_0 has 356 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Sweden Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 356 , Candidate-pairs= 63190 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 63190 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Sweden_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Sweden_Score_Features.csv !"

Time difference of 1.606666 secs



4 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Sweden_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

356 records get merged into 169

Successfully created \Master_Data/Recursive_Staging_Area/Sweden_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Sweden_0_Master.csv!
1 csvs generated are: ['Sweden_0_Master.csv']

Max-depth for Sweden will be 1
1 csvs need to be processed: ['Sweden_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

169 records get merged into 169

Successfully created \Master_Data/Recursive_Staging_Area/Sweden_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Sweden_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Sweden_Master.csv!
356 records get merged into 169

Successfully created \Master_Data/Sweden_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Sweden_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=308 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Switzerland_country_df.csv!

Switzerland_0 has 308 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Switzerland Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782214  6.0
[1] "NRows= 308 , Candidate-pairs= 47278 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 47278 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Switzerland_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Switzerland_Score_Features.csv !"

Time difference of 1.229162 secs



9 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Switzerland_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

308 records get merged into 91

Successfully created \Master_Data/Recursive_Staging_Area/Switzerland_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Switzerland_0_Master.csv!
1 csvs generated are: ['Switzerland_0_Master.csv']

Max-depth for Switzerland will be 1
1 csvs need to be processed: ['Switzerland_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

91 records get merged into 91

Successfully created \Master_Data/Recursive_Staging_Area/Switzerland_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Switzerland_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Switzerland_Master.csv!
308 records get merged into 91

Successfully created \Master_Data/Switzerland_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Switzerland_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=280 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Taiwan_(Province_of_China)_country_df.csv!

Taiwan_(Province_of_China)_0 has 280 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Taiwan_(Province_of_China) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 280 , Candidate-pairs= 39060 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 39060 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Taiwan_(Province_of_China)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Taiwan_(Province_of_China)_Score_Features.csv !"

Time difference of 1.112172 secs



9 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Taiwan_(Province_of_China)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

280 records get merged into 93

Successfully created \Master_Data/Recursive_Staging_Area/Taiwan_(Province_of_China)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Taiwan_(Province_of_China)_0_Master.csv!
1 csvs generated are: ['Taiwan_(Province_of_China)_0_Master.csv']

Max-depth for Taiwan_(Province_of_China) will be 1
1 csvs need to be processed: ['Taiwan_(Province_of_China)_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

93 records get merged into 93

Successfully created \Master_Data/Recursive_Staging_Area/Taiwan_(Province_of_China)_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Taiwan_(Province_of_China)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Taiwan_(Province_of_China)_Master.csv!
280 records get merged into 93

Successfully created \Master_Data/Taiwan_(Province_of_China)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Taiwan_(Province_of_China)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=56 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Thailand_country_df.csv!

Thailand_0 has 56 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Thailand Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 56 , Candidate-pairs= 1540 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 1540 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Thailand_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Thailand_Score_Features.csv !"

Time difference of 0.1914046 secs



1 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Thailand_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

56 records get merged into 39

Successfully created \Master_Data/Recursive_Staging_Area/Thailand_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Thailand_0_Master.csv!
1 csvs generated are: ['Thailand_0_Master.csv']

Max-depth for Thailand will be 1
1 csvs need to be processed: ['Thailand_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

39 records get merged into 39

Successfully created \Master_Data/Recursive_Staging_Area/Thailand_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Thailand_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Thailand_Master.csv!
56 records get merged into 39

Successfully created \Master_Data/Thailand_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Thailand_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=65 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Tunisia_country_df.csv!

Tunisia_0 has 65 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Tunisia Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 65 , Candidate-pairs= 2080 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 2080 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Tunisia_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Tunisia_Score_Features.csv !"

Time difference of 0.1893914 secs



1 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Tunisia_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

65 records get merged into 26

Successfully created \Master_Data/Recursive_Staging_Area/Tunisia_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Tunisia_0_Master.csv!
1 csvs generated are: ['Tunisia_0_Master.csv']

Max-depth for Tunisia will be 1
1 csvs need to be processed: ['Tunisia_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

26 records get merged into 26

Successfully created \Master_Data/Recursive_Staging_Area/Tunisia_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Tunisia_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Tunisia_Master.csv!
65 records get merged into 26

Successfully created \Master_Data/Tunisia_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Tunisia_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=632 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Turkey_country_df.csv!

Turkey_0 has 632 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Turkey Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 632 , Candidate-pairs= 199396 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 199396 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Turkey_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Turkey_Score_Features.csv !"

Time difference of 5.53076 secs



6 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Turkey_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

632 records get merged into 189

Successfully created \Master_Data/Recursive_Staging_Area/Turkey_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Turkey_0_Master.csv!
1 csvs generated are: ['Turkey_0_Master.csv']

Max-depth for Turkey will be 1
1 csvs need to be processed: ['Turkey_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

189 records get merged into 189

Successfully created \Master_Data/Recursive_Staging_Area/Turkey_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Turkey_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Turkey_Master.csv!
632 records get merged into 189

Successfully created \Master_Data/Turkey_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Turkey_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=747 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Ukraine_country_df.csv!

Ukraine_0 has 747 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Ukraine Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254250  2.0     786432  6.0   782205  6.0
[1] "NRows= 747 , Candidate-pairs= 278631 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 278631 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Ukraine_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Ukraine_Score_Features.csv !"

Time difference of 9.601472 secs



20 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Ukraine_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

747 records get merged into 262

Successfully created \Master_Data/Recursive_Staging_Area/Ukraine_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Ukraine_0_Master.csv!
1 csvs generated are: ['Ukraine_0_Master.csv']

Max-depth for Ukraine will be 1
1 csvs need to be processed: ['Ukraine_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

262 records get merged into 262

Successfully created \Master_Data/Recursive_Staging_Area/Ukraine_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Ukraine_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Ukraine_Master.csv!
747 records get merged into 262

Successfully created \Master_Data/Ukraine_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Ukraine_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=4 and minibatch-size=4000


Starting Batch[0]...

Successfully created \United_Arab_Emirates_(the)_country_df.csv!

United_Arab_Emirates_(the)_0 has 4 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_Arab_Emirates_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 4 , Candidate-pairs= 6 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 6 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "No potential matches found in the incoming dataset! Creating a dummy csv..."
[1] "Raw_Scores//United_Arab_Emirates_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_Arab_Emirates_(the)_Score_Features.csv !"

Time difference of 0.1825411 secs



Get the unique set of all record-ids since there aren't any potential duplicates.

4 records get merged into 4

Successfully created \Master_Data/Recursive_Staging_Area/United_Arab_Emirates_(the)_0_Master.csv!
1 csvs generated are: ['United_Arab_Emirates_(the)_0_Master.csv']

Max-depth for United_Arab_Emirates_(the) will be 1
1 csvs need to be processed: ['United_Arab_Emirates_(the)_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

4 records get merged into 4

Successfully created \Master_Data/Recursive_Staging_Area/United_Arab_Emirates_(the)_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_Arab_Emirates_(the)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/United_Arab_Emirates_(the)_Master.csv!
4 records get merged into 4

Successfully created \Master_Data/United_Arab_Emirates_(the)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/United_Arab_Emirates_(the)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=1885 and minibatch-size=4000


Starting Batch[0]...

Successfully created \United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_country_df.csv!

United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_0 has 1885 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254257  2.0     786432  6.0   782220  6.0
[1] "NRows= 1885 , Candidate-pairs= 1775670 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 1775670 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_Score_Features.csv !"

Time difference of 37.91655 secs



73 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

1885 records get merged into 631

Successfully created \Master_Data/Recursive_Staging_Area/United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_0_Master.csv!
1 csvs generated are: ['United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_0_Master.csv']

Max-depth for United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the) will be 1
1 csvs need to be processed: ['United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

631 records get merged into 631

Successfully created \Master_Data/Recursive_Staging_Area/United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_Master.csv!
1885 records get merged into 631

Successfully created \Master_Data/United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/United_Kingdom_of_Great_Britain_and_Northern_Ireland_(the)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 6 batches since incoming dataset-size=22881 and minibatch-size=4000


Starting Batch[0]...

Successfully created \United_States_of_America_(the)_country_df.csv!

United_States_of_America_(the)_0 has 4000 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 4000 , Candidate-pairs= 7998000 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 7998000 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_States_of_America_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_States_of_America_(the)_Score_Features.csv !"

Time difference of 3.003634 mins



140 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

4000 records get merged into 1475

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_0_Master.csv!


Starting Batch[1]...

Successfully created \United_States_of_America_(the)_country_df.csv!

United_States_of_America_(the)_1 has 4000 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 4000 , Candidate-pairs= 7998000 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 7998000 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_States_of_America_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_States_of_America_(the)_Score_Features.csv !"

Time difference of 2.94008 mins



139 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

4000 records get merged into 1571

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_1_Master.csv!


Starting Batch[2]...

Successfully created \United_States_of_America_(the)_country_df.csv!

United_States_of_America_(the)_2 has 4000 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 4000 , Candidate-pairs= 7998000 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 7998000 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_States_of_America_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_States_of_America_(the)_Score_Features.csv !"

Time difference of 2.887963 mins



140 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

4000 records get merged into 1464

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_2_Master.csv!


Starting Batch[3]...

Successfully created \United_States_of_America_(the)_country_df.csv!

United_States_of_America_(the)_3 has 4000 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 4000 , Candidate-pairs= 7998000 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 7998000 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_States_of_America_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_States_of_America_(the)_Score_Features.csv !"

Time difference of 2.905299 mins



132 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

4000 records get merged into 1501

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_3_Master.csv!


Starting Batch[4]...

Successfully created \United_States_of_America_(the)_country_df.csv!

United_States_of_America_(the)_4 has 4000 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 4000 , Candidate-pairs= 7998000 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 7998000 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_States_of_America_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_States_of_America_(the)_Score_Features.csv !"

Time difference of 2.916773 mins



140 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

4000 records get merged into 1626

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_4_Master.csv!


Starting Batch[5]...

Successfully created \United_States_of_America_(the)_country_df.csv!

United_States_of_America_(the)_5 has 2881 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254253  2.0     786432  6.0   782216  6.0
[1] "NRows= 2881 , Candidate-pairs= 4148640 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 4148640 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_States_of_America_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_States_of_America_(the)_Score_Features.csv !"

Time difference of 1.4536 mins



90 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

2881 records get merged into 1216

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_5_Master.csv!
6 csvs generated are: ['United_States_of_America_(the)_0_Master.csv', 'United_States_of_America_(the)_1_Master.csv', 'United_States_of_America_(the)_2_Master.csv', 'United_States_of_America_(the)_3_Master.csv', 'United_States_of_America_(the)_4_Master.csv', 'United_States_of_America_(the)_5_Master.csv']

Max-depth for United_States_of_America_(the) will be 3
6 csvs need to be processed: ['United_States_of_America_(the)_0_Master.csv', 'United_States_of_America_(the)_1_Master.csv', 'United_States_of_America_(the)_2_Master.csv', 'United_States_of_America_(the)_3_Master.csv', 'United_States_of_America_(the)_4_Master.csv', 'United_States_of_America_(the)_5_Master.csv'] , length=6

Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_0_Master.csv has 1475 records, and Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_1_Master.csv has 1571 records.

Invoking the Rscript now...

R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Linkage Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_0_Master.csv Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_1_Master.csv"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120883  6.5     350000 18.7   283197 15.2
Vcells 254284  2.0     786432  6.0   782261  6.0
[1] "NRows= 1475 , Columns are "
 [1] "SR_NUM"            "SOURCE_IDENTIFIER" "DATA_SOURCE_NAME" 
 [4] "PROTOCOL_NUMBER"   "SITE_NUM"          "UNIQUE_SITE_ID"   
 [7] "COUNTRY"           "SITE_NAME"         "STATE"            
[10] "CITY"              "POSTAL_CODE"       "SITE_STATUS"      
[13] "CONCAT_ADDRESS"   
[1] "NRows= 1571 , Columns are "
 [1] "SR_NUM"            "SOURCE_IDENTIFIER" "DATA_SOURCE_NAME" 
 [4] "PROTOCOL_NUMBER"   "SITE_NUM"          "UNIQUE_SITE_ID"   
 [7] "COUNTRY"           "SITE_NAME"         "STATE"            
[10] "CITY"              "POSTAL_CODE"       "SITE_STATUS"      
[13] "CONCAT_ADDRESS"   
[1] "N_combinations= 2317225 , Columns are "
 [1] "id1"               "id2"               "SOURCE_IDENTIFIER"
 [4] "DATA_SOURCE_NAME"  "PROTOCOL_NUMBER"   "SITE_NUM"         
 [7] "UNIQUE_SITE_ID"    "COUNTRY"           "SITE_NAME"        
[10] "STATE"             "CITY"              "POSTAL_CODE"      
[13] "SITE_STATUS"       "CONCAT_ADDRESS"    "is_match"         
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_States_of_America_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_States_of_America_(the)_Score_Features.csv !"

Time difference of 1.487895 mins



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

3046 records get merged into 3033

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d1_0_Master.csv!

Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_2_Master.csv has 1464 records, and Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_3_Master.csv has 1501 records.

Invoking the Rscript now...

R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Linkage Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_2_Master.csv Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_3_Master.csv"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120883  6.5     350000 18.7   283197 15.2
Vcells 254284  2.0     786432  6.0   782261  6.0
[1] "NRows= 1464 , Columns are "
 [1] "SR_NUM"            "SOURCE_IDENTIFIER" "DATA_SOURCE_NAME" 
 [4] "PROTOCOL_NUMBER"   "SITE_NUM"          "UNIQUE_SITE_ID"   
 [7] "COUNTRY"           "SITE_NAME"         "STATE"            
[10] "CITY"              "POSTAL_CODE"       "SITE_STATUS"      
[13] "CONCAT_ADDRESS"   
[1] "NRows= 1501 , Columns are "
 [1] "SR_NUM"            "SOURCE_IDENTIFIER" "DATA_SOURCE_NAME" 
 [4] "PROTOCOL_NUMBER"   "SITE_NUM"          "UNIQUE_SITE_ID"   
 [7] "COUNTRY"           "SITE_NAME"         "STATE"            
[10] "CITY"              "POSTAL_CODE"       "SITE_STATUS"      
[13] "CONCAT_ADDRESS"   
[1] "N_combinations= 2197464 , Columns are "
 [1] "id1"               "id2"               "SOURCE_IDENTIFIER"
 [4] "DATA_SOURCE_NAME"  "PROTOCOL_NUMBER"   "SITE_NUM"         
 [7] "UNIQUE_SITE_ID"    "COUNTRY"           "SITE_NAME"        
[10] "STATE"             "CITY"              "POSTAL_CODE"      
[13] "SITE_STATUS"       "CONCAT_ADDRESS"    "is_match"         
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_States_of_America_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_States_of_America_(the)_Score_Features.csv !"

Time difference of 1.404116 mins



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

2965 records get merged into 2962

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d1_2_Master.csv!

Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_4_Master.csv has 1626 records, and Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_5_Master.csv has 1216 records.

Invoking the Rscript now...

R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Linkage Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_4_Master.csv Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_5_Master.csv"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120883  6.5     350000 18.7   283197 15.2
Vcells 254284  2.0     786432  6.0   782261  6.0
[1] "NRows= 1626 , Columns are "
 [1] "SR_NUM"            "SOURCE_IDENTIFIER" "DATA_SOURCE_NAME" 
 [4] "PROTOCOL_NUMBER"   "SITE_NUM"          "UNIQUE_SITE_ID"   
 [7] "COUNTRY"           "SITE_NAME"         "STATE"            
[10] "CITY"              "POSTAL_CODE"       "SITE_STATUS"      
[13] "CONCAT_ADDRESS"   
[1] "NRows= 1216 , Columns are "
 [1] "SR_NUM"            "SOURCE_IDENTIFIER" "DATA_SOURCE_NAME" 
 [4] "PROTOCOL_NUMBER"   "SITE_NUM"          "UNIQUE_SITE_ID"   
 [7] "COUNTRY"           "SITE_NAME"         "STATE"            
[10] "CITY"              "POSTAL_CODE"       "SITE_STATUS"      
[13] "CONCAT_ADDRESS"   
[1] "N_combinations= 1977216 , Columns are "
 [1] "id1"               "id2"               "SOURCE_IDENTIFIER"
 [4] "DATA_SOURCE_NAME"  "PROTOCOL_NUMBER"   "SITE_NUM"         
 [7] "UNIQUE_SITE_ID"    "COUNTRY"           "SITE_NAME"        
[10] "STATE"             "CITY"              "POSTAL_CODE"      
[13] "SITE_STATUS"       "CONCAT_ADDRESS"    "is_match"         
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_States_of_America_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_States_of_America_(the)_Score_Features.csv !"

Time difference of 1.261264 mins



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

2842 records get merged into 2840

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d1_4_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.
3 csvs need to be processed: ['United_States_of_America_(the)_d1_0_Master.csv', 'United_States_of_America_(the)_d1_2_Master.csv', 'United_States_of_America_(the)_d1_4_Master.csv'] , length=4

Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d1_0_Master.csv has 3033 records, and Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d1_2_Master.csv has 2962 records.

Invoking the Rscript now...

R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Linkage Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d1_0_Master.csv Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d1_2_Master.csv"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120883  6.5     350000 18.7   283197 15.2
Vcells 254284  2.0     786432  6.0   782262  6.0
[1] "NRows= 3033 , Columns are "
 [1] "SR_NUM"            "SOURCE_IDENTIFIER" "DATA_SOURCE_NAME" 
 [4] "PROTOCOL_NUMBER"   "SITE_NUM"          "UNIQUE_SITE_ID"   
 [7] "COUNTRY"           "SITE_NAME"         "STATE"            
[10] "CITY"              "POSTAL_CODE"       "SITE_STATUS"      
[13] "CONCAT_ADDRESS"   
[1] "NRows= 2962 , Columns are "
 [1] "SR_NUM"            "SOURCE_IDENTIFIER" "DATA_SOURCE_NAME" 
 [4] "PROTOCOL_NUMBER"   "SITE_NUM"          "UNIQUE_SITE_ID"   
 [7] "COUNTRY"           "SITE_NAME"         "STATE"            
[10] "CITY"              "POSTAL_CODE"       "SITE_STATUS"      
[13] "CONCAT_ADDRESS"   
[1] "N_combinations= 8983746 , Columns are "
 [1] "id1"               "id2"               "SOURCE_IDENTIFIER"
 [4] "DATA_SOURCE_NAME"  "PROTOCOL_NUMBER"   "SITE_NUM"         
 [7] "UNIQUE_SITE_ID"    "COUNTRY"           "SITE_NAME"        
[10] "STATE"             "CITY"              "POSTAL_CODE"      
[13] "SITE_STATUS"       "CONCAT_ADDRESS"    "is_match"         
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//United_States_of_America_(the)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//United_States_of_America_(the)_Score_Features.csv !"

Time difference of 5.742136 mins



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

5995 records get merged into 5978

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d2_0_Master.csv!


Get the unique set of all record-ids since there isn't a second file to compare.

2840 records get merged into 2840

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d2_2_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d2_Raw_Cross_Ref.csv!


Depth[2] processed successfully.
2 csvs need to be processed: ['United_States_of_America_(the)_d2_0_Master.csv', 'United_States_of_America_(the)_d2_2_Master.csv'] , length=2

Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d2_0_Master.csv has 5978 records, and Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d2_2_Master.csv has 2840 records.

Invoking the Rscript now...

R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 United_States_of_America_(the) Raw_Scores 4 Linkage Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d2_0_Master.csv Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d2_2_Master.csv"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120883  6.5     350000 18.7   283197 15.2
Vcells 254284  2.0     786432  6.0   782262  6.0
[1] "NRows= 5978 , Columns are "
 [1] "SR_NUM"            "SOURCE_IDENTIFIER" "DATA_SOURCE_NAME" 
 [4] "PROTOCOL_NUMBER"   "SITE_NUM"          "UNIQUE_SITE_ID"   
 [7] "COUNTRY"           "SITE_NAME"         "STATE"            
[10] "CITY"              "POSTAL_CODE"       "SITE_STATUS"      
[13] "CONCAT_ADDRESS"   
[1] "NRows= 2840 , Columns are "
 [1] "SR_NUM"            "SOURCE_IDENTIFIER" "DATA_SOURCE_NAME" 
 [4] "PROTOCOL_NUMBER"   "SITE_NUM"          "UNIQUE_SITE_ID"   
 [7] "COUNTRY"           "SITE_NAME"         "STATE"            
[10] "CITY"              "POSTAL_CODE"       "SITE_STATUS"      
[13] "CONCAT_ADDRESS"   

R ERROR:

Loading required package: tools



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/United_States_of_America_(the)_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

8818 records get merged into 8818

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d3_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/United_States_of_America_(the)_d3_Raw_Cross_Ref.csv!


Depth[3] processed successfully.




Processed all 3 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/United_States_of_America_(the)_Master.csv!
22881 records get merged into 8818

Successfully created \Master_Data/United_States_of_America_(the)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/United_States_of_America_(the)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=4 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Venezuela_(Bolivarian_Republic_of)_country_df.csv!

Venezuela_(Bolivarian_Republic_of)_0 has 4 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Venezuela_(Bolivarian_Republic_of) Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254257  2.0     786432  6.0   782220  6.0
[1] "NRows= 4 , Candidate-pairs= 6 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 6 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "No potential matches found in the incoming dataset! Creating a dummy csv..."
[1] "Raw_Scores//Venezuela_(Bolivarian_Republic_of)_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Venezuela_(Bolivarian_Republic_of)_Score_Features.csv !"

Time difference of 0.1632266 secs



Get the unique set of all record-ids since there aren't any potential duplicates.

4 records get merged into 4

Successfully created \Master_Data/Recursive_Staging_Area/Venezuela_(Bolivarian_Republic_of)_0_Master.csv!
1 csvs generated are: ['Venezuela_(Bolivarian_Republic_of)_0_Master.csv']

Max-depth for Venezuela_(Bolivarian_Republic_of) will be 1
1 csvs need to be processed: ['Venezuela_(Bolivarian_Republic_of)_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

4 records get merged into 4

Successfully created \Master_Data/Recursive_Staging_Area/Venezuela_(Bolivarian_Republic_of)_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Venezuela_(Bolivarian_Republic_of)_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Venezuela_(Bolivarian_Republic_of)_Master.csv!
4 records get merged into 4

Successfully created \Master_Data/Venezuela_(Bolivarian_Republic_of)_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Venezuela_(Bolivarian_Republic_of)_Cross_Ref_Full_Report.csv!

Special Character that will be replaced are:  \!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~

There will be 1 batches since incoming dataset-size=74 and minibatch-size=4000


Starting Batch[0]...

Successfully created \Viet_Nam_country_df.csv!

Viet_Nam_0 has 74 records.

Invoking the Rscript now...
R OUTPUT:

[1] "levenshtein .so 0.85 0.75 3 Viet_Nam Raw_Scores 4 Dedup NA NA"
[1] "Loading levenshtein.so !"
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 120882  6.5     350000 18.7   283196 15.2
Vcells 254251  2.0     786432  6.0   782206  6.0
[1] "NRows= 74 , Candidate-pairs= 2701 , Columns are "
[1] "SR_NUM"         "CITY"           "STATE"          "SITE_NAME"     
[5] "POSTAL_CODE"    "CONCAT_ADDRESS"
[1] "N_combinations= 2701 , Columns are "
[1] "id1"            "id2"            "CITY"           "STATE"         
[5] "SITE_NAME"      "POSTAL_CODE"    "CONCAT_ADDRESS" "is_match"      
[1] "Scaling up column scores if threshold crossed"
[1] "SITE_NAME  :  0.85"
[1] "STATE  :  0.85"
[1] "CITY  :  0.85"
[1] "POSTAL_CODE  :  0.85"
[1] "CONCAT_ADDRESS  :  0.75"
[1] "Raw_Scores//Viet_Nam_Score_Features.csv"
[1] "Successfully created //Raw_Scores//Viet_Nam_Score_Features.csv !"

Time difference of 0.2193062 secs



0 raw-score pairs will be deleted off as their cyclic dependecies have lower score than existing.

Successfully created \Cleaned_Scores/Viet_Nam_Cleaned_Feature_Scores.csv!

"SR_NUM_2" will be the master record


Found potential duplicates. Processing their master and cross-reference...

74 records get merged into 17

Successfully created \Master_Data/Recursive_Staging_Area/Viet_Nam_Cross_Ref_Full_Report.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Viet_Nam_0_Master.csv!
1 csvs generated are: ['Viet_Nam_0_Master.csv']

Max-depth for Viet_Nam will be 1
1 csvs need to be processed: ['Viet_Nam_0_Master.csv'] , length=2


Get the unique set of all record-ids since there isn't a second file to compare.

17 records get merged into 17

Successfully created \Master_Data/Recursive_Staging_Area/Viet_Nam_d1_0_Master.csv!

Successfully created \Master_Data/Recursive_Staging_Area/Viet_Nam_d1_Raw_Cross_Ref.csv!


Depth[1] processed successfully.




Processed all 1 levels. Generating the master and cross-reference at the final-layer...

Successfully created \Master_Data/Viet_Nam_Master.csv!
74 records get merged into 17

Successfully created \Master_Data/Viet_Nam_Raw_Cross_Ref.csv!

Successfully created \Master_Data/Viet_Nam_Cross_Ref_Full_Report.csv!



!!!! Script finished processing !!!!

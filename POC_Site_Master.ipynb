{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. Initialization\n",
    "\n",
    "> a. Read the excel/csv containing the entire query-output\n",
    "\n",
    "> b. Initialize the parameters of thresholds, directories, and fields to concat for generating individual as well as combined match-score\n",
    "\n",
    "> c. Get the unique list of countries in present dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, numpy as np, pandas as pd, recordlinkage, re, string\n",
    "\n",
    "_STATIC_FILE_NAME=\"SM_Temp_Shortlist.xlsx\"\n",
    "_RAW_SCORES_DIRECTORY='Raw_Scores'\n",
    "_CLEANED_SCORES_DIRECTORY='Cleaned_Scores'\n",
    "_MASTER_DATA_DIRECTORY='Master_Data'\n",
    "_FIELDS_TO_CONCAT={\n",
    "    'CONCAT_ADDRESS':   ['ADDRESS_LINE_1','ADDRESS_LINE_2','ADDRESS_LINE_3'],\n",
    "    'CONCAT_SRC':       ['SITE_NAME','STATE','CITY','CONCAT_ADDRESS','POSTAL_CODE']\n",
    "                }\n",
    "\n",
    "_COLUMNS_TO_CLEAN=['ADDRESS_LINE_1','ADDRESS_LINE_2','ADDRESS_LINE_3','SITE_NAME','STATE','CITY','POSTAL_CODE']\n",
    "_THRESHOLD_FOR_INDIVIDUAL=0.85\n",
    "_THRESHOLD_FOR_ENTIRE_COMBINED=0.55\n",
    "_THRESHOLD_FOR_ADDRESS_COMBINED=0.75\n",
    "\n",
    "_THRESHOLDS_DICT={\n",
    "    'CONCAT_ADDRESS': _THRESHOLD_FOR_ADDRESS_COMBINED,\n",
    "    'CONCAT_SRC': _THRESHOLD_FOR_ENTIRE_COMBINED, \n",
    "    'SITE_NAME': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'STATE': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'CITY': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'POSTAL_CODE': _THRESHOLD_FOR_INDIVIDUAL\n",
    "    }\n",
    "\n",
    "_SCALING_FACTOR=3\n",
    "\n",
    "_TOTAL_MATCHES_THRESHOLD=3\n",
    "\n",
    "\n",
    "def write_df_to_csv(df, root_dir='', curr_country='', file_suffix='_temp.csv', index_flag=False):\n",
    "    try:\n",
    "        abs_path=os.path.join(root_dir, curr_country+file_suffix)\n",
    "        df.to_csv(abs_path, index=index_flag)\n",
    "        print(f'\\nSuccessfully created \\{abs_path}!')\n",
    "    except:\n",
    "        print(f'\\nSomething went wrong while writing the file. Please check if it is currently in use.')\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df.replace(np.nan, '', inplace=True)\n",
    "    for colname in df.columns.values:\n",
    "        df[colname]=df[colname].astype(str).apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    special_chars=re.escape(string.punctuation)\n",
    "    print('\\nSpecial Character that will be replaced are:  ', special_chars) #!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\{\\\\|\\\\}\\\\~\n",
    "    for colname in df.columns.values:\n",
    "        if colname in _COLUMNS_TO_CLEAN:\n",
    "            df[colname]=df[colname].replace(r'['+special_chars+']', '', regex=True).str.lower()\n",
    "\n",
    "    for colname, cols_to_concat in _FIELDS_TO_CONCAT.items():\n",
    "        df[colname]=df[cols_to_concat].apply(lambda single_row: ''.join(single_row.values), axis=1)\n",
    "    \n",
    "    df.drop(labels=_FIELDS_TO_CONCAT['CONCAT_ADDRESS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nColumns:  ['SOURCE_IDENTIFIER' 'DATA_SOURCE_NAME' 'PROTOCOL_NUMBER' 'SITE_NUM'\n 'UNIQUE_SITE_ID' 'COUNTRY' 'SITE_NAME' 'STATE' 'CITY' 'ADDRESS_LINE_1'\n 'ADDRESS_LINE_2' 'ADDRESS_LINE_3' 'POSTAL_CODE' 'SITE_STATUS'] \n\n\nUnique Countries:  ['Italy']\n"
     ]
    }
   ],
   "source": [
    "site_master_df=pd.read_excel(_STATIC_FILE_NAME, index_col=0)\n",
    "print('\\nColumns: ', site_master_df.columns.values,'\\n')\n",
    "countries=site_master_df['COUNTRY'].unique()\n",
    "print('\\nUnique Countries: ',countries)"
   ]
  },
  {
   "source": [
    "## 2. Process data in batches: getting scores of all the candidate-pairs is highly computation intensive. RAM crashes for incoming batch-size>4000\n",
    "> a. Partition the entire input dataset based on country-name. For-loop to do the entire process for all countries.\n",
    "\n",
    "> b. Create a concatenated (address) field and drop the individual 3 address fields\n",
    "\n",
    "> c. Create a concatenated (name, address) field\n",
    "\n",
    "> c. Initialize the candidate-pairs (indexes) for comparison"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Special Character that will be replaced are:   !\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n",
      "\n",
      "Italy has 1371 records\n",
      "\n",
      "Number of candidate-pairs for match consideration= 939135\n"
     ]
    }
   ],
   "source": [
    "# todo: Research on multithreading to speed up country-wise batches. RAM might crash for incoming batch-size>4000.\n",
    "\n",
    "i=0\n",
    "curr_country=countries[i]\n",
    "country_df=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "preprocess_dataframe(country_df)\n",
    "clean_dataframe(country_df)\n",
    "\n",
    "# todo: Imputation of address values where Site-name is exactly the same, otherwise it'll result in 2 separate master-records\n",
    "\n",
    "print(f'\\n{curr_country} has {country_df.shape[0]} records')\n",
    "indexer=recordlinkage.Index()\n",
    "indexer.block(left_on='COUNTRY')\n",
    "candidates=indexer.index(country_df)\n",
    "print('\\nNumber of candidate-pairs for match consideration=',len(candidates))\n",
    "\n",
    "# todo: Read source code to figure out how is number of pairs reduced? Possibly uses only unique combinations rather than permutations"
   ]
  },
  {
   "source": [
    "## 3. Initialize Comparer() object with set of fields to compare amongst the candidate pairs\n",
    "\n",
    "### Currently configured such that: \n",
    "\n",
    "> if individual fields' match-score > 85% , then set the *_col_*_COMPARISON_SCORE column to 1, else 0\n",
    "\n",
    "> if combined fields' match-score > 55% , then set the CONCAT_SRC_COMPARISON_SCORE column to 1, else 0\n",
    "\n",
    "### Comparer algorithm can be: 'jaro', 'jarowinkler', 'levenshtein', 'damerau_levenshtein', 'qgram', 'cosine', 'smith_waterman', 'lcs'\n",
    "\n",
    "### You can also set n_job=-1 to use up all cores available for parallel-computation of scores for the candidate-pairs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Italy 191 vs 99"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Comparer created with individual-fields' threshold= 0.85  , combined-address-field threshold= 0.75  , and combined-entire-field threshold= 0.55\n\n\n Starting computation for match-scores... \n\n"
     ]
    }
   ],
   "source": [
    "# todo: check if non-latin scripts can be handled. It seems to be able to handle UTF-8 encoding data\n",
    "def add_field_to_compare(comparer_obj, field_name, threshold, method='levenshtein'):\n",
    "    comparer_obj.string( field_name, field_name, method=method, threshold=threshold, label=field_name+'_COMPARISON_SCORE' )\n",
    "\n",
    "comparer=recordlinkage.Compare(n_jobs=-1)\n",
    "\n",
    "for column, threshold in _THRESHOLDS_DICT.items():\n",
    "    add_field_to_compare(comparer, column, threshold)\n",
    "\n",
    "print('Comparer created with individual-fields\\' threshold=', _THRESHOLD_FOR_INDIVIDUAL, ' , combined-address-field threshold=', _THRESHOLD_FOR_ADDRESS_COMBINED, ' , and combined-entire-field threshold=', _THRESHOLD_FOR_ENTIRE_COMBINED)\n",
    "\n",
    "start=time.time()\n",
    "print('\\n\\n Starting computation for match-scores... \\n')\n",
    "score_features=comparer.compute(candidates, country_df)\n",
    "print(time.time()-start,' seconds needed for ',country_df.shape[0],' records.')\n",
    "print('\\n\\nScore_features generated: ', score_features.shape)"
   ]
  },
  {
   "source": [
    "## 4. Get the set of potential duplicates where TOTAL_SCORE > THRESHOLD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_COLS_FOR_TOTAL_MATCH_CALC=[colname+'_COMPARISON_SCORE' for colname in _THRESHOLDS_DICT]\n",
    "\n",
    "def scale_up_comparison_score(df, colname='SITE_NAME_COMPARISON_SCORE', scaling_factor=_SCALING_FACTOR):\n",
    "    print(f'\\nScaling up {colname} by {scaling_factor}')\n",
    "    df[colname]=df[colname].apply(lambda x: x*scaling_factor)\n",
    "\n",
    "scale_up_comparison_score(score_features,'SITE_NAME_COMPARISON_SCORE', _SCALING_FACTOR)\n",
    "scale_up_comparison_score(score_features,'CONCAT_ADDRESS_COMPARISON_SCORE', _SCALING_FACTOR)\n",
    "\n",
    "write_df_to_csv(df=score_features, root_dir=_RAW_SCORES_DIRECTORY, curr_country=curr_country, file_suffix='_Raw_Scores.csv', index_flag=True)\n",
    "\n",
    "duplicates=score_features[score_features.sum(axis=1) > _TOTAL_MATCHES_THRESHOLD].reset_index()\n",
    "duplicates['NUM_OF_MATCHES_FOUND']=duplicates[_COLS_FOR_TOTAL_MATCH_CALC].sum(axis=1)\n",
    "\n",
    "duplicates.head(30)"
   ]
  },
  {
   "source": [
    "## 5. Choose the best match for incoming child records based on highest total-score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_match(df, child_column, score_key_column):\n",
    "    normalized_duplicates=df.sort_values(by=[child_column]).sort_values(by=[score_key_column],ascending=False)\n",
    "    normalized_duplicates=normalized_duplicates.groupby(child_column).head(1).sort_values(by=[child_column])\n",
    "    return normalized_duplicates\n",
    "\n",
    "normalized_duplicates=return_top_match(df=duplicates, child_column='SR_NUM_1', score_key_column='NUM_OF_MATCHES_FOUND')\n",
    "normalized_duplicates.head(30)"
   ]
  },
  {
   "source": [
    "## 5. Reusable function to replace the cyclic matches\n",
    "### For example:\n",
    "\n",
    ">   Record45 matches with Record44\n",
    "\n",
    ">   Record67 matches with Record45\n",
    "\n",
    "### In this case we should maintain:\n",
    ">   Record67 matches with Record44"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def replace_cyclic_dependencies(df, child_indicator, master_indicator):\n",
    "    arr=set(df[child_indicator].array)\n",
    "    for val in df[master_indicator]:\n",
    "        if val in arr:\n",
    "            replace_val=df[df[child_indicator]==val][master_indicator].values[0]\n",
    "            print(val,' found in normalized_duplicates[',child_indicator,']. Replacement: ', replace_val)\n",
    "            df[master_indicator].replace(val, replace_val, inplace=True)\n",
    "    return df\n",
    "\n",
    "normalized_duplicates=replace_cyclic_dependencies(df=normalized_duplicates, child_indicator='SR_NUM_1', master_indicator='SR_NUM_2')\n",
    "normalized_duplicates"
   ]
  },
  {
   "source": [
    "## 6. CSV for static-analysis of matches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df_to_csv(df=normalized_duplicates, root_dir=_CLEANED_SCORES_DIRECTORY, curr_country=curr_country, file_suffix='_Cleaned_Feature_Scores.csv')\n",
    "print('\\n\"SR_NUM_2\" will be the master record')"
   ]
  },
  {
   "source": [
    "## 7. Get the unique set of Master-Records and create a master CSV file for each country\n",
    "\n",
    "> a. Think of 'SR_NUM_1' as the list of incoming Primary-keys, and 'SR_NUM_2' as the value to which it should be mapped based on match-score\n",
    "\n",
    "> b. Hence, union of 'SR_NUM_1' & 'SR_NUM_2' will be entire set of duplicates\n",
    "\n",
    "> c. Stand-alone records in the current country_batch_dataframe will not fall in this entire set of duplicates\n",
    "\n",
    "> d. Master-records set wil be the sets of 'SR_NUM_2' & #c above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=set(normalized_duplicates['SR_NUM_1'].values.tolist())\n",
    "a2=set(normalized_duplicates['SR_NUM_2'].values.tolist())\n",
    "country_set=set(country_df.index.values.tolist())\n",
    "entire_duplicates_set=a1.union(a2)\n",
    "no_match_set=country_set.difference(entire_duplicates_set)\n",
    "master_record_ids=no_match_set.union(a2)\n",
    "\n",
    "print(f'{site_master_df.shape[0]} records get merged into {len(master_record_ids)}')\n",
    "country_master_df=country_df.loc[master_record_ids].drop('CONCAT_SRC',axis=1)\n",
    "write_df_to_csv(df=country_master_df, root_dir=_MASTER_DATA_DIRECTORY, curr_country=curr_country, index_flag=True, file_suffix='_Master.csv')"
   ]
  },
  {
   "source": [
    "## 8. Get the normalized-duplicates into a CSV to show translation of incoming record into single golden record\n",
    "\n",
    "> a. Create a master_cross_reference_df for the master_record_ids with relevant scores scaled up by _SCALING_FACTOR, and other comparison scores set to 1\n",
    "\n",
    "> b. concat it with the normalized_duplicates dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_record_score_array=[1.0]*len(master_record_ids)\n",
    "master_record_df_dict={'SR_NUM_1': list(master_record_ids), 'SR_NUM_2': list(master_record_ids), 'SITE_NAME_COMPARISON_SCORE': master_record_score_array, 'STATE_COMPARISON_SCORE': master_record_score_array, 'CITY_COMPARISON_SCORE': master_record_score_array, 'CONCAT_ADDRESS_COMPARISON_SCORE': master_record_score_array, 'POSTAL_CODE_COMPARISON_SCORE': master_record_score_array, 'CONCAT_SRC_COMPARISON_SCORE': master_record_score_array}\n",
    "\n",
    "cross_ref_df=pd.DataFrame(master_record_df_dict)\n",
    "\n",
    "scale_up_comparison_score(cross_ref_df,'SITE_NAME_COMPARISON_SCORE',_SCALING_FACTOR)\n",
    "scale_up_comparison_score(cross_ref_df,'CONCAT_ADDRESS_COMPARISON_SCORE',_SCALING_FACTOR)\n",
    "cross_ref_df['NUM_OF_MATCHES_FOUND']=cross_ref_df[_COLS_FOR_TOTAL_MATCH_CALC].sum(axis=1)\n",
    "\n",
    "\n",
    "cross_ref_df=cross_ref_df.append(normalized_duplicates)\n",
    "cross_ref_df.sort_values(by=['SR_NUM_1'], axis=0, inplace=True)\n",
    "\n",
    "write_df_to_csv(df=cross_ref_df, root_dir=_MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix='_Raw_Cross_Ref.csv')"
   ]
  },
  {
   "source": [
    "## 9. Generate the report to display name & address fields of match-and-merge combinations\n",
    "\n",
    "> a. Merge the master_cross_reference_df with the country_batch_dataframe as a left-outer-join on Primary-key='SR_NUM_1'\n",
    "\n",
    "> b. Merge this master_cross_reference_df with the country_batch_dataframe as a left-outer-join on Primary-key='SR_NUM_2'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df_copy=site_master_df[site_master_df['COUNTRY']==curr_country][_FIELDS_TO_CONCAT['CONCAT_SRC']].copy(deep=True).reset_index()\n",
    "country_df_colnames=country_df_copy.columns.values\n",
    "\n",
    "country_df_copy.columns=[colname+'_1' for colname in country_df_colnames]\n",
    "cross_ref_df=cross_ref_df.merge(country_df_copy, how='left', on='SR_NUM_1')\n",
    "\n",
    "country_df_copy.columns=[colname+'_2' for colname in country_df_colnames]\n",
    "cross_ref_df=cross_ref_df.merge(country_df_copy, how='left', on='SR_NUM_2')\n",
    "cross_ref_df=cross_ref_df[['SR_NUM_1', 'SR_NUM_2', 'SITE_NAME_1','SITE_NAME_2','SITE_NAME_COMPARISON_SCORE','STATE_1','STATE_2','STATE_COMPARISON_SCORE', 'CITY_1','CITY_2','CITY_COMPARISON_SCORE','CONCAT_ADDRESS_1','CONCAT_ADDRESS_2','CONCAT_ADDRESS_COMPARISON_SCORE', 'POSTAL_CODE_1','POSTAL_CODE_2','POSTAL_CODE_COMPARISON_SCORE','NUM_OF_MATCHES_FOUND']]\n",
    "\n",
    "write_df_to_csv(df=cross_ref_df, root_dir=_MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix='_Cross_Ref_Full_Report.csv')"
   ]
  }
 ]
}
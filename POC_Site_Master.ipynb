{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "df07b04948fcdefd07d686a15c20f5c13147995460fc283da7a7d27998f2a407"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. Initialization\n",
    "\n",
    "> a. Read the excel/csv containing the entire query-output\n",
    "\n",
    "> b. Initialize the parameters of thresholds, directories, and fields to concat for generating individual as well as combined match-score\n",
    "\n",
    "> c. Get the unique list of countries in present dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nColumns:  ['SOURCE_IDENTIFIER' 'DATA_SOURCE_NAME' 'PROTOCOL_NUMBER' 'SITE_NUM'\n 'UNIQUE_SITE_ID' 'COUNTRY' 'SITE_NAME' 'STATE' 'CITY' 'ADDRESS_LINE_1'\n 'ADDRESS_LINE_2' 'ADDRESS_LINE_3' 'POSTAL_CODE' 'SITE_STATUS'] \n\n\nUnique Countries:  ['Italy']\n"
     ]
    }
   ],
   "source": [
    "import time, numpy as np, pandas as pd, recordlinkage, re, string\n",
    "\n",
    "_STATIC_FILE_NAME=\"SM_Temp_Shortlist.xlsx\"\n",
    "_RAW_SCORES_DIRECTORY='Raw_Scores'\n",
    "_CLEANED_SCORES_DIRECTORY='Cleaned_Scores'\n",
    "_MASTER_DATA_DIRECTORY='Master_Data'\n",
    "_FIELDS_TO_CONCAT={\n",
    "    'CONCAT_ADDRESS':   ['ADDRESS_LINE_1','ADDRESS_LINE_2','ADDRESS_LINE_3']#, 'CONCAT_SRC':       ['SITE_NAME','STATE','CITY','CONCAT_ADDRESS','POSTAL_CODE']\n",
    "                }\n",
    "\n",
    "_COLUMNS_TO_CLEAN=['ADDRESS_LINE_1','ADDRESS_LINE_2','ADDRESS_LINE_3','SITE_NAME','STATE','CITY','POSTAL_CODE']\n",
    "_THRESHOLD_FOR_INDIVIDUAL=0.85\n",
    "#_THRESHOLD_FOR_ENTIRE_COMBINED=0.55\n",
    "_THRESHOLD_FOR_ADDRESS_COMBINED=0.75\n",
    "\n",
    "_THRESHOLDS_DICT={\n",
    "    'CONCAT_ADDRESS': _THRESHOLD_FOR_ADDRESS_COMBINED,\n",
    "    #'CONCAT_SRC': _THRESHOLD_FOR_ENTIRE_COMBINED, \n",
    "    'SITE_NAME': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'STATE': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'CITY': _THRESHOLD_FOR_INDIVIDUAL,\n",
    "    'POSTAL_CODE': _THRESHOLD_FOR_INDIVIDUAL\n",
    "    }\n",
    "_COLS_FOR_TOTAL_MATCH_CALC=[colname+'_COMPARISON_SCORE' for colname in _THRESHOLDS_DICT]\n",
    "\n",
    "_SCALING_FACTOR=3\n",
    "\n",
    "_TOTAL_MATCHES_THRESHOLD=3\n",
    "\n",
    "\n",
    "def write_df_to_csv(df, root_dir='', curr_country='', file_suffix='_temp.csv', index_flag=False):\n",
    "    try:\n",
    "        abs_path=os.path.join(root_dir, curr_country+file_suffix)\n",
    "        df.to_csv(abs_path, index=index_flag)\n",
    "        print(f'\\nSuccessfully created \\{abs_path}!')\n",
    "    except:\n",
    "        print(f'\\nSomething went wrong while writing the file. Please check if it is currently in use.')\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df.replace(np.nan, '', inplace=True)\n",
    "    for colname in df.columns.values:\n",
    "        df[colname]=df[colname].astype(str).apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "def clean_dataframe(df, replace_punctuations=True):\n",
    "    if replace_punctuations:\n",
    "        special_chars=re.escape(string.punctuation)\n",
    "        print('\\nSpecial Character that will be replaced are:  ', special_chars) #!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\{\\\\|\\\\}\\\\~\n",
    "    for colname in df.columns.values:\n",
    "        if colname in _COLUMNS_TO_CLEAN and replace_punctuations:\n",
    "            df[colname]=df[colname].replace(r'['+special_chars+']', '', regex=True).str.lower()\n",
    "    for colname, cols_to_concat in _FIELDS_TO_CONCAT.items():\n",
    "        df[colname]=df[cols_to_concat].apply(lambda single_row: ''.join(single_row.values), axis=1)\n",
    "    df.drop(labels=_FIELDS_TO_CONCAT['CONCAT_ADDRESS'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def scale_up_comparison_score(df, colname='SITE_NAME_COMPARISON_SCORE', scaling_factor=_SCALING_FACTOR):\n",
    "    print(f'\\nScaling up {colname} by {scaling_factor}')\n",
    "    df[colname]=df[colname].apply(lambda x: x*scaling_factor)\n",
    "\n",
    "def replace_cyclic_dependencies(df, child_indicator, master_indicator):\n",
    "    arr=set(df[child_indicator].array)\n",
    "    for val in df[master_indicator]:\n",
    "        if val in arr:\n",
    "            replace_val=df[df[child_indicator]==val][master_indicator].values[0]\n",
    "            print(val,' found in normalized_duplicates[',child_indicator,']. Replacement: ', replace_val)\n",
    "            df[master_indicator].replace(val, replace_val, inplace=True)\n",
    "    return df\n",
    "    \n",
    "def return_top_match(df, child_column, score_key_column):\n",
    "    normalized_duplicates=df.sort_values(by=[child_column]).sort_values(by=[score_key_column],ascending=False)\n",
    "    normalized_duplicates=normalized_duplicates.groupby(child_column).head(1).sort_values(by=[child_column])\n",
    "    return normalized_duplicates\n",
    "\n",
    "\n",
    "site_master_df=pd.read_excel(_STATIC_FILE_NAME, index_col=0)\n",
    "print('\\nColumns: ', site_master_df.columns.values,'\\n')\n",
    "countries=site_master_df['COUNTRY'].unique()\n",
    "print('\\nUnique Countries: ',countries)"
   ]
  },
  {
   "source": [
    "## 2. Process data in batches: getting scores of all the candidate-pairs is highly computation intensive. RAM crashes for incoming batch-size>4000\n",
    "> a. Partition the entire input dataset based on country-name. For-loop to do the entire process for all countries.\n",
    "\n",
    "> b. Create a concatenated (address) field and drop the individual 3 address fields\n",
    "\n",
    "> c. Create a concatenated (name, address) field"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Special Character that will be replaced are:   !\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n",
      "\n",
      "Successfully created \\Italy_country_df.csv!\n",
      "\n",
      "Italy has 1371 records\n",
      "\n",
      "Number of candidate-pairs for match consideration= 939135\n"
     ]
    }
   ],
   "source": [
    "# todo: Research on multithreading to speed up country-wise batches. RAM might crash for incoming batch-size>4000.\n",
    "\n",
    "i=0\n",
    "curr_country=countries[i]\n",
    "country_df=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "preprocess_dataframe(country_df)\n",
    "clean_dataframe(country_df)\n",
    "write_df_to_csv(df=country_df, curr_country=curr_country, file_suffix='_country_df.csv', index_flag=True)\n",
    "\n",
    "# todo: Imputation of address values where Site-name is exactly the same, otherwise it'll result in 2 separate master-records\n",
    "\n",
    "print(f'\\n{curr_country} has {country_df.shape[0]} records')"
   ]
  },
  {
   "source": [
    "## 3. Initialize the candidate-pairs for comparison via the Index() object, and a Comparer() object with set of fields to compare amongst the candidate pairs\n",
    "\n",
    "### Currently configured to run this match-score generation in R, such that:\n",
    "\n",
    "> if individual fields' match-score > 85% , then set the *_col_*_COMPARISON_SCORE column to 1, else 0\n",
    "\n",
    "> if combined address-fields' match-score > 75% , then set the CONCAT_ADDRESS_COMPARISON_SCORE column to 1, else 0\n",
    "\n",
    "### Comparer algorithm can be: 'jaro', 'jarowinkler', 'levenshtein', 'damerau_levenshtein', 'qgram', 'cosine', 'smith_waterman', 'lcs'\n",
    "\n",
    "### You can also set n_job=-1 to use up all cores available for parallel-computation of scores for the candidate-pairs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\ndef add_field_to_compare(comparer_obj, field_name, threshold, method='levenshtein'):\\n    comparer_obj.string( field_name, field_name, method=method, threshold=threshold, label=field_name+'_COMPARISON_SCORE' )\\n\\ncomparer=recordlinkage.Compare(n_jobs=-1)\\n\\nfor column, threshold in _THRESHOLDS_DICT.items():\\n    add_field_to_compare(comparer, column, threshold)\\n\\nprint('Comparer created with individual-fields' threshold=', _THRESHOLD_FOR_INDIVIDUAL, ' , combined-address-field threshold=', _THRESHOLD_FOR_ADDRESS_COMBINED, ' , and combined-entire-field threshold=', _THRESHOLD_FOR_ENTIRE_COMBINED)\\n\\nstart=time.time()\\nprint('\\n\\n Starting computation for match-scores... \\n')\\nscore_features=comparer.compute(candidates, country_df)\\nprint(time.time()-start,' seconds needed for ',country_df.shape[0],' records.')\\nprint('\\n\\nScore_features generated: ', score_features.shape)\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# todo: check if non-latin scripts can be handled. It seems to be able to handle UTF-8 encoding data\n",
    "# todo: invoke the R-code from Python\n",
    "\"\"\"\n",
    "def add_field_to_compare(comparer_obj, field_name, threshold, method='levenshtein'):\n",
    "    comparer_obj.string( field_name, field_name, method=method, threshold=threshold, label=field_name+'_COMPARISON_SCORE' )\n",
    "\n",
    "\n",
    "indexer=recordlinkage.Index()\n",
    "indexer.block(left_on='COUNTRY')\n",
    "candidates=indexer.index(country_df)\n",
    "print('\\nNumber of candidate-pairs for match consideration=',len(candidates))\n",
    "\n",
    "# todo: Read source code to figure out how is number of pairs reduced? Possibly uses only unique combinations rather than permutations\n",
    "\n",
    "comparer=recordlinkage.Compare(n_jobs=-1)\n",
    "\n",
    "for column, threshold in _THRESHOLDS_DICT.items():\n",
    "    add_field_to_compare(comparer, column, threshold)\n",
    "\n",
    "print('Comparer created with individual-fields\\' threshold=', _THRESHOLD_FOR_INDIVIDUAL, ' , combined-address-field threshold=', _THRESHOLD_FOR_ADDRESS_COMBINED, ' , and combined-entire-field threshold=', _THRESHOLD_FOR_ENTIRE_COMBINED)\n",
    "\n",
    "start=time.time()\n",
    "print('\\n\\n Starting computation for match-scores... \\n')\n",
    "score_features=comparer.compute(candidates, country_df)\n",
    "print(time.time()-start,' seconds needed for ',country_df.shape[0],' records.')\n",
    "print('\\n\\nScore_features generated: ', score_features.shape)\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "## 4. Get the set of potential duplicates where TOTAL_SCORE > THRESHOLD\n",
    "\n",
    "### Currently configured to perform this in R itself.\n",
    "\n",
    "> R code will finally generate the *Country*_Score_Features.csv in the /Raw_Scores/ directory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\"\"\"\n",
    "scale_up_comparison_score(score_features,'CONCAT_ADDRESS_COMPARISON_SCORE', _SCALING_FACTOR)\n",
    "\n",
    "write_df_to_csv(df=score_features, root_dir=_RAW_SCORES_DIRECTORY, curr_country=curr_country, file_suffix='_Raw_Scores.csv', index_flag=True)\n",
    "\n",
    "duplicates=score_features[score_features.sum(axis=1) > _TOTAL_MATCHES_THRESHOLD].reset_index()\n",
    "duplicates['NUM_OF_MATCHES_FOUND']=duplicates[_COLS_FOR_TOTAL_MATCH_CALC].sum(axis=1)\n",
    "\"\"\"\n",
    "duplicates=pd.read_csv(os.path.join(_RAW_SCORES_DIRECTORY, curr_country+'_Score_Features.csv'))\n",
    "duplicates.head(30)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## 5. Choose the best match for incoming child records based on highest total-score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     SR_NUM_1  SR_NUM_2  SITE_NAME_COMPARISON_SCORE  STATE_COMPARISON_SCORE  \\\n",
       "0           6         1                           0                       0   \n",
       "10         10         4                           1                       1   \n",
       "60         13        12                           0                       0   \n",
       "65         16        15                           0                       0   \n",
       "70         17        16                           0                       0   \n",
       "86         20        19                           0                       0   \n",
       "1          22         1                           1                       0   \n",
       "108        26        25                           0                       1   \n",
       "109        37        25                           0                       0   \n",
       "188        38        37                           0                       0   \n",
       "71         39        16                           1                       0   \n",
       "72         40        16                           1                       0   \n",
       "130        41        29                           0                       0   \n",
       "206        49        48                           0                       0   \n",
       "30         50         6                           0                       0   \n",
       "134        51        31                           1                       1   \n",
       "155        60        33                           0                       0   \n",
       "189        62        37                           0                       0   \n",
       "73         63        16                           1                       0   \n",
       "93         66        20                           0                       0   \n",
       "198        67        45                           0                       0   \n",
       "199        68        45                           1                       0   \n",
       "207        69        48                           0                       0   \n",
       "233        70        50                           0                       0   \n",
       "330        75        73                           1                       1   \n",
       "241        77        53                           1                       1   \n",
       "38         87         7                           0                       0   \n",
       "40         88         8                           0                       0   \n",
       "221        91        49                           0                       0   \n",
       "135        93        32                           0                       1   \n",
       "\n",
       "     CITY_COMPARISON_SCORE  POSTAL_CODE_COMPARISON_SCORE  \\\n",
       "0                        1                             1   \n",
       "10                       1                             1   \n",
       "60                       0                             1   \n",
       "65                       1                             1   \n",
       "70                       1                             1   \n",
       "86                       0                             1   \n",
       "1                        1                             1   \n",
       "108                      1                             1   \n",
       "109                      1                             1   \n",
       "188                      1                             1   \n",
       "71                       1                             1   \n",
       "72                       1                             1   \n",
       "130                      1                             1   \n",
       "206                      1                             1   \n",
       "30                       1                             1   \n",
       "134                      1                             1   \n",
       "155                      1                             1   \n",
       "189                      1                             1   \n",
       "73                       1                             1   \n",
       "93                       1                             1   \n",
       "198                      1                             1   \n",
       "199                      1                             1   \n",
       "207                      1                             1   \n",
       "233                      1                             1   \n",
       "330                      1                             1   \n",
       "241                      1                             1   \n",
       "38                       1                             1   \n",
       "40                       1                             1   \n",
       "221                      0                             1   \n",
       "135                      1                             1   \n",
       "\n",
       "     CONCAT_ADDRESS_COMPARISON_SCORE  NUM_OF_MATCHES_FOUND  \n",
       "0                                  3                     5  \n",
       "10                                 3                     7  \n",
       "60                                 3                     4  \n",
       "65                                 3                     5  \n",
       "70                                 3                     5  \n",
       "86                                 3                     4  \n",
       "1                                  3                     6  \n",
       "108                                3                     6  \n",
       "109                                3                     5  \n",
       "188                                3                     5  \n",
       "71                                 3                     6  \n",
       "72                                 3                     6  \n",
       "130                                3                     5  \n",
       "206                                3                     5  \n",
       "30                                 3                     5  \n",
       "134                                3                     7  \n",
       "155                                3                     5  \n",
       "189                                3                     5  \n",
       "73                                 3                     6  \n",
       "93                                 3                     5  \n",
       "198                                3                     5  \n",
       "199                                3                     6  \n",
       "207                                3                     5  \n",
       "233                                3                     5  \n",
       "330                                0                     4  \n",
       "241                                0                     4  \n",
       "38                                 3                     5  \n",
       "40                                 3                     5  \n",
       "221                                3                     4  \n",
       "135                                3                     6  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SR_NUM_1</th>\n      <th>SR_NUM_2</th>\n      <th>SITE_NAME_COMPARISON_SCORE</th>\n      <th>STATE_COMPARISON_SCORE</th>\n      <th>CITY_COMPARISON_SCORE</th>\n      <th>POSTAL_CODE_COMPARISON_SCORE</th>\n      <th>CONCAT_ADDRESS_COMPARISON_SCORE</th>\n      <th>NUM_OF_MATCHES_FOUND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>13</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>16</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>17</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>20</td>\n      <td>19</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>26</td>\n      <td>25</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>37</td>\n      <td>25</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>38</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>39</td>\n      <td>16</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>40</td>\n      <td>16</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>41</td>\n      <td>29</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>49</td>\n      <td>48</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>50</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>51</td>\n      <td>31</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>60</td>\n      <td>33</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>189</th>\n      <td>62</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>63</td>\n      <td>16</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>66</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>67</td>\n      <td>45</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>68</td>\n      <td>45</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>69</td>\n      <td>48</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>233</th>\n      <td>70</td>\n      <td>50</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>330</th>\n      <td>75</td>\n      <td>73</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>241</th>\n      <td>77</td>\n      <td>53</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>87</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>88</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>221</th>\n      <td>91</td>\n      <td>49</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>93</td>\n      <td>32</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "normalized_duplicates=return_top_match(df=duplicates, child_column='SR_NUM_1', score_key_column='NUM_OF_MATCHES_FOUND')\n",
    "normalized_duplicates.head(30)"
   ]
  },
  {
   "source": [
    "## 5. Reusable function to replace the cyclic matches\n",
    "### For example:\n",
    "\n",
    ">   Record45 matches with Record44\n",
    "\n",
    ">   Record67 matches with Record45\n",
    "\n",
    "### In this case we should maintain:\n",
    ">   Record67 matches with Record44"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rmalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "163  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "197  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "121  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  56\n",
      "214  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  35\n",
      "245  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  73\n",
      "111  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  73\n",
      "232  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "233  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "282  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  71\n",
      "266  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "284  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  149\n",
      "259  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "77  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  53\n",
      "297  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  149\n",
      "286  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  150\n",
      "238  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  120\n",
      "314  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  150\n",
      "269  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "264  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "208  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  18\n",
      "289  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "324  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  149\n",
      "313  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  253\n",
      "305  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  136\n",
      "334  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  56\n",
      "387  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  95\n",
      "241  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  54\n",
      "323  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "395  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  148\n",
      "406  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  28\n",
      "265  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  73\n",
      "404  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  400\n",
      "407  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  346\n",
      "369  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  136\n",
      "370  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  56\n",
      "304  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  53\n",
      "399  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  341\n",
      "410  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  351\n",
      "303  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "236  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  124\n",
      "242  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  155\n",
      "125  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  35\n",
      "343  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  18\n",
      "357  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "412  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  73\n",
      "366  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  253\n",
      "438  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  155\n",
      "431  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  341\n",
      "278  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "376  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  155\n",
      "384  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  154\n",
      "455  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  341\n",
      "240  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  35\n",
      "367  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  363\n",
      "397  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "460  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  154\n",
      "485  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  483\n",
      "442  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  425\n",
      "104  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  48\n",
      "174  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  45\n",
      "490  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  1\n",
      "491  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  48\n",
      "435  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "507  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "489  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  425\n",
      "504  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  450\n",
      "409  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  28\n",
      "396  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  54\n",
      "445  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  169\n",
      "432  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  33\n",
      "449  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  353\n",
      "420  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "498  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  1\n",
      "526  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  36\n",
      "512  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  450\n",
      "540  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  419\n",
      "553  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  525\n",
      "447  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  253\n",
      "476  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  95\n",
      "517  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  497\n",
      "434  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  351\n",
      "437  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  124\n",
      "499  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  48\n",
      "347  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  19\n",
      "534  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "518  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  54\n",
      "263  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  73\n",
      "548  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  464\n",
      "479  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  363\n",
      "312  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  149\n",
      "330  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  56\n",
      "573  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  536\n",
      "350  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  260\n",
      "610  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  149\n",
      "577  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  525\n",
      "441  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  18\n",
      "458  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  33\n",
      "580  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  48\n",
      "354  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  279\n",
      "551  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  419\n",
      "627  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  436\n",
      "595  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  306\n",
      "466  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  368\n",
      "522  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  169\n",
      "319  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  150\n",
      "481  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "487  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n",
      "415  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  400\n",
      "663  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n",
      "640  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  279\n",
      "637  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  327\n",
      "257  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  36\n",
      "661  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  368\n",
      "664  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  150\n",
      "554  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "617  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  149\n",
      "698  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "673  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n",
      "586  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  54\n",
      "654  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  28\n",
      "716  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n",
      "338  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "620  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  596\n",
      "427  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  136\n",
      "560  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  253\n",
      "574  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  351\n",
      "576  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  124\n",
      "737  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  544\n",
      "678  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n",
      "757  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  124\n",
      "632  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  529\n",
      "758  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  500\n",
      "331  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  73\n",
      "612  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  493\n",
      "666  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  351\n",
      "631  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  528\n",
      "509  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  503\n",
      "494  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  205\n",
      "742  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "593  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  464\n",
      "802  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  419\n",
      "766  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  535\n",
      "584  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "808  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  529\n",
      "700  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "736  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  596\n",
      "724  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  327\n",
      "634  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  18\n",
      "818  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  400\n",
      "537  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  1\n",
      "801  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  95\n",
      "732  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  28\n",
      "835  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  18\n",
      "516  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  28\n",
      "533  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  353\n",
      "781  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  235\n",
      "850  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  353\n",
      "674  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  400\n",
      "623  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  569\n",
      "824  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  405\n",
      "639  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  48\n",
      "782  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  484\n",
      "809  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  528\n",
      "789  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  124\n",
      "888  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  528\n",
      "492  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  45\n",
      "761  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  544\n",
      "884  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  48\n",
      "511  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  425\n",
      "672  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "709  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  149\n",
      "616  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  260\n",
      "846  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  381\n",
      "908  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "837  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  564\n",
      "777  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "697  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n",
      "609  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  95\n",
      "428  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  56\n",
      "866  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  849\n",
      "461  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  18\n",
      "907  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  351\n",
      "686  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  36\n",
      "854  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  169\n",
      "946  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "608  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "918  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n",
      "472  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "630  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  123\n",
      "292  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  73\n",
      "875  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "882  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  124\n",
      "935  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  95\n",
      "728  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  1\n",
      "505  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  422\n",
      "941  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  804\n",
      "662  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  169\n",
      "743  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  253\n",
      "402  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  148\n",
      "546  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  332\n",
      "645  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  54\n",
      "285  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  71\n",
      "949  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  351\n",
      "1004  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  73\n",
      "721  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n",
      "906  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  400\n",
      "869  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  1\n",
      "805  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "894  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  544\n",
      "939  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  56\n",
      "819  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  419\n",
      "909  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  363\n",
      "786  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  351\n",
      "598  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  95\n",
      "659  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  515\n",
      "722  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  155\n",
      "748  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  545\n",
      "898  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  48\n",
      "927  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "980  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  18\n",
      "459  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  155\n",
      "335  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  207\n",
      "1039  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  351\n",
      "1057  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  493\n",
      "643  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "1019  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  73\n",
      "931  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "933  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n",
      "302  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  231\n",
      "1070  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  252\n",
      "539  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  36\n",
      "206  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  114\n",
      "1042  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  725\n",
      "603  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  363\n",
      "970  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  235\n",
      "779  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  279\n",
      "735  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "1060  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  207\n",
      "344  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  279\n",
      "973  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  148\n",
      "971  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  123\n",
      "701  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "690  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  675\n",
      "744  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  514\n",
      "1098  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  32\n",
      "502  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  332\n",
      "1052  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  48\n",
      "1000  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  169\n",
      "913  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  260\n",
      "636  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  33\n",
      "826  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "1011  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  148\n",
      "550  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  547\n",
      "1099  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  493\n",
      "749  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  351\n",
      "1024  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  237\n",
      "1145  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  71\n",
      "624  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  525\n",
      "820  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  28\n",
      "299  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  201\n",
      "912  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  149\n",
      "961  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  279\n",
      "1112  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  279\n",
      "250  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  124\n",
      "1165  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  201\n",
      "1166  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n",
      "635  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  556\n",
      "1171  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  529\n",
      "1187  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  529\n",
      "1189  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  1050\n",
      "680  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  154\n",
      "1195  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  1050\n",
      "1138  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  332\n",
      "583  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  19\n",
      "273  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  235\n",
      "256  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  57\n",
      "660  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  306\n",
      "796  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  73\n",
      "196  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  154\n",
      "1199  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  531\n",
      "1150  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "1029  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  544\n",
      "1241  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "848  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  28\n",
      "1142  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  33\n",
      "873  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  405\n",
      "1221  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  306\n",
      "1253  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  1193\n",
      "1269  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  536\n",
      "1184  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  4\n",
      "1196  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  154\n",
      "791  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  85\n",
      "705  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  467\n",
      "1301  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  1204\n",
      "1279  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  536\n",
      "1226  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  529\n",
      "1246  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  1201\n",
      "1308  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  529\n",
      "1073  found in normalized_duplicates[ SR_NUM_1 ]. Replacement:  426\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      SR_NUM_1  SR_NUM_2  SITE_NAME_COMPARISON_SCORE  STATE_COMPARISON_SCORE  \\\n",
       "0            6         1                           0                       0   \n",
       "10          10         4                           1                       1   \n",
       "60          13        12                           0                       0   \n",
       "65          16        15                           0                       0   \n",
       "70          17        15                           0                       0   \n",
       "...        ...       ...                         ...                     ...   \n",
       "4009      1315      1201                           1                       1   \n",
       "4019      1316      1286                           1                       1   \n",
       "4023      1317       529                           1                       1   \n",
       "2187      1320       514                           0                       0   \n",
       "3884      1321       426                           1                       0   \n",
       "\n",
       "      CITY_COMPARISON_SCORE  POSTAL_CODE_COMPARISON_SCORE  \\\n",
       "0                         1                             1   \n",
       "10                        1                             1   \n",
       "60                        0                             1   \n",
       "65                        1                             1   \n",
       "70                        1                             1   \n",
       "...                     ...                           ...   \n",
       "4009                      1                             1   \n",
       "4019                      1                             1   \n",
       "4023                      1                             1   \n",
       "2187                      1                             1   \n",
       "3884                      1                             1   \n",
       "\n",
       "      CONCAT_ADDRESS_COMPARISON_SCORE  NUM_OF_MATCHES_FOUND  \n",
       "0                                   3                     5  \n",
       "10                                  3                     7  \n",
       "60                                  3                     4  \n",
       "65                                  3                     5  \n",
       "70                                  3                     5  \n",
       "...                               ...                   ...  \n",
       "4009                                0                     4  \n",
       "4019                                3                     7  \n",
       "4023                                3                     7  \n",
       "2187                                3                     5  \n",
       "3884                                3                     6  \n",
       "\n",
       "[792 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SR_NUM_1</th>\n      <th>SR_NUM_2</th>\n      <th>SITE_NAME_COMPARISON_SCORE</th>\n      <th>STATE_COMPARISON_SCORE</th>\n      <th>CITY_COMPARISON_SCORE</th>\n      <th>POSTAL_CODE_COMPARISON_SCORE</th>\n      <th>CONCAT_ADDRESS_COMPARISON_SCORE</th>\n      <th>NUM_OF_MATCHES_FOUND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>13</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>16</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>17</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4009</th>\n      <td>1315</td>\n      <td>1201</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4019</th>\n      <td>1316</td>\n      <td>1286</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4023</th>\n      <td>1317</td>\n      <td>529</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2187</th>\n      <td>1320</td>\n      <td>514</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3884</th>\n      <td>1321</td>\n      <td>426</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>792 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "normalized_duplicates=replace_cyclic_dependencies(df=normalized_duplicates, child_indicator='SR_NUM_1', master_indicator='SR_NUM_2')\n",
    "normalized_duplicates"
   ]
  },
  {
   "source": [
    "## 6. CSV for static-analysis of matches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nSuccessfully created \\Cleaned_Scores\\Italy_Cleaned_Feature_Scores.csv!\n\n\"SR_NUM_2\" will be the master record\n"
     ]
    }
   ],
   "source": [
    "write_df_to_csv(df=normalized_duplicates, root_dir=_CLEANED_SCORES_DIRECTORY, curr_country=curr_country, file_suffix='_Cleaned_Feature_Scores.csv')\n",
    "print('\\n\"SR_NUM_2\" will be the master record')"
   ]
  },
  {
   "source": [
    "## 7. Get the unique set of Master-Records and create a master CSV file for each country\n",
    "\n",
    "> a. Think of 'SR_NUM_1' as the list of incoming Primary-keys, and 'SR_NUM_2' as the value to which it should be mapped based on match-score\n",
    "\n",
    "> b. Hence, union of 'SR_NUM_1' & 'SR_NUM_2' will be entire set of duplicates\n",
    "\n",
    "> c. Stand-alone records in the current country_batch_dataframe will not fall in this entire set of duplicates\n",
    "\n",
    "> d. Master-records set wil be the sets of 'SR_NUM_2' & #c above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1371 records get merged into 579\n\nSuccessfully created \\Master_Data\\Italy_Master.csv!\n"
     ]
    }
   ],
   "source": [
    "a1=set(normalized_duplicates['SR_NUM_1'].values.tolist())\n",
    "a2=set(normalized_duplicates['SR_NUM_2'].values.tolist())\n",
    "country_set=set(country_df.index.values.tolist())\n",
    "entire_duplicates_set=a1.union(a2)\n",
    "no_match_set=country_set.difference(entire_duplicates_set)\n",
    "master_record_ids=no_match_set.union(a2)\n",
    "\n",
    "\n",
    "\n",
    "country_df_copy=site_master_df[site_master_df['COUNTRY']==curr_country]\n",
    "preprocess_dataframe(df=country_df_copy)\n",
    "clean_dataframe(df=country_df_copy, replace_punctuations=False)\n",
    "\n",
    "print(f'{site_master_df.shape[0]} records get merged into {len(master_record_ids)}')\n",
    "#country_master_df=country_df_copy.loc[master_record_ids].drop('CONCAT_SRC',axis=1)\n",
    "country_master_df=country_df_copy.loc[master_record_ids]\n",
    "write_df_to_csv(df=country_master_df, root_dir=_MASTER_DATA_DIRECTORY, curr_country=curr_country, index_flag=True, file_suffix='_Master.csv')"
   ]
  },
  {
   "source": [
    "## 8. Get the normalized-duplicates into a CSV to show translation of incoming record into single golden record\n",
    "\n",
    "> a. Create a master_cross_reference_df for the master_record_ids with relevant scores scaled up by _SCALING_FACTOR, and other comparison scores set to 1\n",
    "\n",
    "> b. concat it with the normalized_duplicates dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nScaling up CONCAT_ADDRESS_COMPARISON_SCORE by 3\n\nSuccessfully created \\Master_Data\\Italy_Raw_Cross_Ref.csv!\n"
     ]
    }
   ],
   "source": [
    "master_record_score_array=[1.0]*len(master_record_ids)\n",
    "master_record_df_dict={'SR_NUM_1': list(master_record_ids), 'SR_NUM_2': list(master_record_ids), 'SITE_NAME_COMPARISON_SCORE': master_record_score_array, 'STATE_COMPARISON_SCORE': master_record_score_array, 'CITY_COMPARISON_SCORE': master_record_score_array, 'CONCAT_ADDRESS_COMPARISON_SCORE': master_record_score_array, 'POSTAL_CODE_COMPARISON_SCORE': master_record_score_array}\n",
    "\n",
    "cross_ref_df=pd.DataFrame(master_record_df_dict)\n",
    "\n",
    "scale_up_comparison_score(cross_ref_df,'CONCAT_ADDRESS_COMPARISON_SCORE',_SCALING_FACTOR)\n",
    "cross_ref_df['NUM_OF_MATCHES_FOUND']=cross_ref_df[_COLS_FOR_TOTAL_MATCH_CALC].sum(axis=1)\n",
    "\n",
    "\n",
    "cross_ref_df=cross_ref_df.append(normalized_duplicates)\n",
    "cross_ref_df.sort_values(by=['SR_NUM_1'], axis=0, inplace=True)\n",
    "\n",
    "write_df_to_csv(df=cross_ref_df, root_dir=_MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix='_Raw_Cross_Ref.csv')"
   ]
  },
  {
   "source": [
    "## 9. Generate the report to display name & address fields of match-and-merge combinations\n",
    "\n",
    "> a. Merge the master_cross_reference_df with the country_batch_dataframe as a left-outer-join on Primary-key='SR_NUM_1'\n",
    "\n",
    "> b. Merge this master_cross_reference_df with the country_batch_dataframe as a left-outer-join on Primary-key='SR_NUM_2'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nSuccessfully created \\Master_Data\\Italy_Cross_Ref_Full_Report.csv!\n"
     ]
    }
   ],
   "source": [
    "country_df_copy.reset_index(inplace=True)\n",
    "country_df_colnames=country_df_copy.columns.values\n",
    "\n",
    "country_df_copy.columns=[colname+'_1' for colname in country_df_colnames]\n",
    "cross_ref_df=cross_ref_df.merge(country_df_copy, how='left', on='SR_NUM_1')\n",
    "\n",
    "country_df_copy.columns=[colname+'_2' for colname in country_df_colnames]\n",
    "cross_ref_df=cross_ref_df.merge(country_df_copy, how='left', on='SR_NUM_2')\n",
    "cross_ref_df=cross_ref_df[['SR_NUM_1', 'SR_NUM_2', 'SITE_NAME_1','SITE_NAME_2','SITE_NAME_COMPARISON_SCORE','STATE_1','STATE_2','STATE_COMPARISON_SCORE', 'CITY_1','CITY_2','CITY_COMPARISON_SCORE','CONCAT_ADDRESS_1','CONCAT_ADDRESS_2','CONCAT_ADDRESS_COMPARISON_SCORE', 'POSTAL_CODE_1','POSTAL_CODE_2','POSTAL_CODE_COMPARISON_SCORE','NUM_OF_MATCHES_FOUND']]\n",
    "\n",
    "write_df_to_csv(df=cross_ref_df, root_dir=_MASTER_DATA_DIRECTORY, curr_country=curr_country, file_suffix='_Cross_Ref_Full_Report.csv')"
   ]
  }
 ]
}